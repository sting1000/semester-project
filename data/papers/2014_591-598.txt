The Discourse of Creative Problem Solving in ChildhoodEngineering EducationElise Deitrick, Brian Oâ€™Connell and R. Benjamin ShapiroCenter for Engineering Education & Outreach, Tufts University, Medford, MA, USAElise.Deitrick@tufts.edu, Brian.O_Connell@tufts.edu, Ben@cs.tufts.eduAbstract: Researchers and teachers are increasingly in agreement that classrooms shouldadopt more open-ended, ill-structured, creative problem solving pedagogies (Kapur, 2008).However, we lack sufficient understandings of how to assess the variegated outputs oflearning activities that afford students considerable discretion over what they will produce,and of the mechanisms through which group work can produce those outcomes. In order tounderstand how collaborative problem solving discourse shapes the creativity of collaborativeproducts (as measured by the novelty of those products), we analyzed collaborative problemsolving talk and the resulting products designed for fictional character by 9 groups of middleschool aged youth. We found that engaged responses to peersâ€™ proposed design ideas arepredictive of novel solutions.IntroductionCreativity has been gaining attention in education as an important skill for students in a variety of disciplines.Researchers are beginning to recognize the need to study the role of creativity in learning, and how creativity isrelated to other important phenomena. Creativity has been directly implicated by prominent theories ofgiftedness in young students (Renzulli, 2005; Sternberg, 2005). Creativity has been connected with improvedperformance and retention in mathematics (Van Harpen & Presmeg, 2013; Yuan & Sriraman, 2011), found to bebeneficial to science understanding and literacy (Develaki, 2010; Webb & Rule, 2012), and enhances retentionin music (Peterson & Madsen, 2010). This excitement extends to policy circles as well: The InternationalSociety for Technology in Education lists it first among their student standards and other organizations placesimilar importance to this desired virtue (ISTE, 2012; Davies et al., 2013). Most recently the Next GenerationScience Standards (NGSS) have included engineering design, among other reasons, because â€œengineering offersopportunities for â€˜innovationâ€™ and â€˜creativityâ€™ at the K-12 levelâ€ (NGSS, 2013).Collaboration among students is another goal â€“ long held by learning scientists â€“ that is receiving broadattention. NGSS (2013) exemplifies this by expecting that students learn about working in a team anddeveloping communication skills, stating, â€œthese skills are likely to be acquired when students engage inprojects based on the science and engineering practices and core content.â€ Connections between these values,creativity and collaboration, have been examined in many various studies from collaborative creativity as adesired learning outcome (Sullivan, 2011) to the resources and obstacles found in teacher team creativecollaboration (Kurtzberg & Amabile, 2010) to more in depth and wider examinations of the topic (EtelÃ¤pelto &Lahti, 2008).TheoryResearchers have proposed many definitions for creativity (GlÃ¼ck, Ernst, & Unger, 2002; Taylor, 1988). Somany, in fact, that many have found it ineffective to narrow the definitions to one that is universally accepted(Saunders & Gero, 2002). This lack of a communal definition made creativity a nebulous catch-all buzz wordin research. Instead of focusing on the vast number of concepts creativity could arguably cover, we have chosena clear aspect of creativity often encompassed in these definitions: novelty (Saunders & Gero, 2002; Shah,Vargas-hernandez, & Smith, 2003). Novelty is â€œa measure of how unusual or unexpected an idea is as comparedto other ideasâ€ (Shah et al., 2003).Researchers have developed two broad categories of instruments for measuring novelty: comparisonand selection. When using a selection scale, student work is related to a set scale, as in a rubric. When using acomparative scale, the differences between artifacts are examined and scaled in comparison to one another(Merrill, Charyton, & Jagacinski, 2008). While most grading is currently done on a type of selection scale, e.g.,a rubric, a problem arises when trying to do the same with creativity. In a classroom, there is a culture ofborrowing and picking up ideas from peers. This may lead to a classroom set of solutions that look remarkablysimilar and on a selection scale, would be scored similarly. However, on a comparative scale, solutions arescored based on their differences. This means that creativity is scored locally, allowing for students to beassessed on their ideas, regardless of the common features that may have come about from classroom influence.Collaboration, like creativity, does not have an accepted definition in the research world. â€œThe broadest(but unsatisfactory) definition of 'collaborative learning' is that it is a situation in which two or more peoplelearn or attempt to learn something togetherâ€ (Dillenbourg, 1999). As two people attempt to learn, they shareICLS 2014 Proceedings591Â© ISLSideas and knowledge that have the potential to be taken up by the group, often after critique or discussion(Soller & Lesgold, 2007). Due to the uncertain nature of collaborative learning, there is â€œa general concern is todevelop ways to increase the probability thatâ€¦ types of interaction [that trigger learning mechanisms] occurâ€(Dillenbourg, 1999). Dillenbourg (1999) categorizes these catalytic activities into Setup of Initial Conditions,Over-Specifying Collaboration Contract with a Scenario Based on Roles, Scaffolding Productive Interactions byEncompassing Interaction Rules in the Medium, or Monitoring and Regulating the Interactions.There has been extensive research in the Computer Supported Collaborative Learning (CSCL)community around how to enable or study collaboration. This research examines what kinds of learningenvironment designs can support changes in the social organization of learning and enable youth to worktogether to construct new knowledge (Scardamalia, Bereiter, & Lamon, 1994; White, 2006). However, much ofthis work focuses on problem solving in domains where there are a priori knowable right answers. Learningscientists know relatively little about how collaborative discourse shapes solutions to open ended problems,particularly about how student talk can support the development of creative solutions to those problems.Collaboration and creativity are two very complicated subjects, made up of a series of observable andunobservable factors whose relations the aforementioned studies have examined to varying levels of detail.What sort of discourse is associated with creative problem solving? Existing work in collaboration has shownengaged responses to peersâ€™ ideas can lead to correct answers in group problem solving, but this work has beenlimited to studying problem solving collaboration around problems that only have a single correct or incorrectsolution (Brigid Barron, 2003). In this paper, we build upon Barronâ€™s (2003) methods to study problem solvingdiscourse from a hands-on engineering summer camp to examine whether the characteristics of studentdiscourse patterns that lead to success on closed-ended problems also predict more creative solutions in openended engineering design projects. Using mixed methods, we examine correlations between key features ofstudentsâ€™ collaborative discussion and the creativity of their solutions to open-ended engineering problems.Study ContextWe present data from a middle school age summer camp that was part of Integrating Engineering and Literacyat Tufts Universityâ€™s Center for Engineering Education and Outreach. This camp challenged participating youthto brainstorm, design, build, and test inventions that could assist fictional characters in childrenâ€™s literature whoface a variety of problems. Three challenges, each based in a different book, were addressed over the span ofthree days. The camp had a morning session which used LEGO Mindstorms, and an afternoon session whichused PaperBots, a newly developed educational robotics kit that was designed to be inexpensive and makes useof paper and craft material as building components (O'Connell, 2013). Each session of the camp consisted of 15students in 4th through 6th grade, split into five groups of three.Each challenge began with the participants and their teacher together reading a book. Then, theteacher-researcher asked participants to identify engineering problems within the story, facilitated by the leadinstructor. Going back to their groups, participants then chose one of the identified problems and designed asolution to help the characters in the story using a robotics system. The data presented in this paper is takenfrom their interactions and solutions for the story Muncha Muncha Muncha by Candace Fleming, in whichbunnies sneak into a farmerâ€™s garden and eat his vegetables at night despite his attempts to stop them. Thechildren identified the problems of trying to help the farmer keep bunnies out of his garden or, alternatively, tohelp the bunnies get into the garden to eat the vegetables. After these two possible problems were identified,students returned to their groups, where they worked together to decide which problem to focus on. They thenbrainstormed possible solutions, and iteratively built and tested them. Studentsâ€™ group work lasted 2.5 hours,which was spread over two days. During this time, the teachers and researchers interacted with the groups,prompting them to talk about their ideas and what they were doing while working not to influence decisionmaking. They did emphasize that their solutions had to work for the characters. The groups presented theirsolutions to the class at the end of the session.Research MethodsWe videotaped each group of students throughout their work, collecting about 115 hours of high definitionvideo. Researchers deliberately avoided influencing solutions but intervened if significant group discord arose.Computing Solution NoveltyWe calculated novelty using a five step process: First, researchers identified attributes. Second, we assignedweights to the attributes. Third, we mapped ideas and features to the identified attributes. Fourth, we computedvalues for ideas. Finally, we calculated a novelty score for each artifact.Attributes were identified that were common between both populations and identifiable as distinct ornecessary features of their artifacts by either direct communication by the participants during their final shareout or directly observable from their artifacts. Those identified were intention, means, sensor, and body.Intention is the chosen purpose for their robot, or their initial idea. Means is denotes how the artifact fulfilledICLS 2014 Proceedings592Â© ISLStheir intention. Sensor refers to the means with which it senses the state of the world around it. Body is theoverall physical embodiment of their final solution. Researchers experienced in working with children onrobotics selected these features, and refined through discussion within the research team. Different populationsor problems may require different attributes..Attribute weights emphasize the importance of particularly difficult or design-critical features. Due tothe pilot nature of this study and the fieldâ€™s lack of understanding about which parts of robotic engineering areparticularly difficult for youth, we assigned all weights equally. Note that ğ‘“ğ‘— is the weight for the attribute ğ‘—,where ğ‘›ğ‘—!! ğ‘“ğ‘— = 1.0 (Shah et al., 2003). Since all of the weights must sum to one, we set all our weights to 0.25.Ideas and features were mapped to attributes based upon observation of the artifact itself anddiscussion during a groupâ€™s final share out. More specifically, we identified intention based on discussionduring final share out, means both from discussion during share out as well as observing the artifact itself andboth sensor and body through observing the artifact.Values for the ideas were computed using the formula given in Shah et. al (2003):ğ‘†ğ‘— = ğ‘‡ğ‘— âˆ’ ğ¶ğ‘— ğ‘‡ğ‘— Ã—10 where ğ‘‡ğ‘— is the total number of ideas for attribute ğ‘— and ğ¶ğ‘— is the number of instancesfor a specific idea in that attribute.Finally, Novelty scores are calculated using a summation of those values, computed from ğ‘€ =ğ‘›ğ‘“ğ‘†ğ‘—!! ğ‘— ğ‘— (Shah et al., 2003). Resulting values range from 0 to 8.0 that were then translated to a percent of thepossible value to get a Novelty Score out of 100.In this case, the solutions for the LEGO group and the PaperBots group were scored as separatepopulations since although they were participating in the same activities; they were using different technologieswith unknown difference in breadth of solution possibilities or impact on studentsâ€™ conversations.Analyzing Collaborative DiscourseA coding scheme described by Barron (2003) was used to classify how students responded to a peerproposed problem-solving solution. There is a two-part process for coding responses: identifying solutionproposals and coding the responses as Accept, Discuss and Non-engage. A proposed solution was defined as anysuggestion that explained how the group would help the designated character in the story. It was counted as anaccept response if a group mate indicated â€œagreement with the content of the proposal,â€ a discuss response if agroup mate acknowledged â€œthe proposal but did not accept them outright or reject them without rationaleâ€, anda reject or ignore response if a group member rejects â€œthe proposal without a rationaleâ€¦.[or] there is a lack ofrelevant verbal response.â€ The term engage refers to the both accept and discuss responses, and the term nonengage refers to the reject or ignore responses (Barron, 2003). In addition to the counts of the types ofresponses, an Engagement Score, Acceptance Score and a Discussion Score were calculated as the percent ofengage, accept and discuss out of the total responses, respectivelyOne of the researchers coded all video data available for the Muncha Muncha Muncha. A secondresearcher randomly chose a group from the LEGO session and a group from the PaperBots session andindependently coded their first hour of the activity using the same coding scheme. Agreement between the twocoders was 100%.Comparing Discourse and Solution NoveltyWe used Microsoft Excel to calculate a Pearson product-moment correlation coefficient between each of theEngagement Score, Acceptance Score, and Discussion Score against Novelty Score.ResultsThe methods described above were then applied to the data collected from the camp to yield the followingresults. Please note that all solutions were considered. The solutions in the book by the farmer were to dig amoat around his garden and after that failed to keep out the bunnies, build a large fortress wall around it. Thebunnies were still able to infiltrate it through cunning though. The absurdity of the farmerâ€™s solutions and theanthropomorphic abilities of the bunnies opened up such possibilities that the student solutions, no matter howunrealistic, were considered as long as the students could present their reasoning which they all successfully did.Solution NoveltyAn example calculation: To clarify our process, we present an instance of mapping ideas to an attribute,computing the values for ideas and calculating a groupâ€™s novelty score.Using the LEGO sessionâ€™s artifacts shown in Figure 1 as an example, the body attribute had 3 differentdesign concepts; a single body where all components were in a single package, a single-functional body whereall components were in a single package but included some functional components like a cow catcher on thefront, and a tethered system where the NXT brick was tethered to the functional portion of the robot.ICLS 2014 Proceedings593Â© ISLSFigure 1. Artifacts from Muncha Muncha Muncha activity for LEGO groupâ€™s A, B, and C in the top row andgroupâ€™s D and E across the bottom.For the single and single-functional design ideas, there were two instances (ğ¶! = 2) of each so for apopulation of 5 (ğ‘‡! = 5) giving an idea value of ğ‘†! = ğ‘‡! âˆ’ ğ¶! ğ‘‡! Ã—10 = 5 âˆ’ 2 5 Ã—10 = Â 6.0. For thetethered idea, it was unique among that population (ğ¶! = 1) so it gains a higher novelty value of ğ‘†! =ğ‘‡! âˆ’ ğ¶! ğ‘‡! Ã—10 = 5 âˆ’ 1 5 Ã—10 = Â 8.0.LEGO group Câ€™s novelty score comes from adding up all of their idea scores. They had the idea towarn the rabbits with a light triggered by a pressure sensor built into a tethered system robot. That would givethem a value of 8.0, 8.0, 6.0, and 8.0 for their ideas (values shown in Table 1) and then, with the weightscalculated in, a total novelty score of 7.2 (scores shown in Table 2).Overall: We used the methods described above to compute the novelty of each groupâ€™s design solution.Table 1 shows the results from computing the values for the ideas as described as step four. None of theattributes have all unique ideas, the intentions of the morning session being the least unique. Table 2 shows theideas for each attribute by group as mapped in step three as well as their final novelty scores as calculated instep five. Overall, we found that the artifacts in the afternoon were on average more novel then the morningartifacts with statistical significance using a 1-tailed heteroscedastic t-test, which is statistically significant usingthe conventional 5% cut off.Table 1: Attribute idea values from Muncha Muncha Muncha activity final artifacts and presentationLegoLego GroupIntentionIdeaMeansCVScare42.0Help18.0IdeaSensingCVChase3LightCatapultIdeaBodyCV4.0 User118.0 Dark18.0 PressureBaitedIdeaCV8.0 Single26.018.0 Sgl-Funct26.026.0 Tethered18.018.0CVLegoPaperBots GroupIntentionIdeaMeansSensingCVIdeaCVScare26.0Scarecrow2Trap18.0Drop CageHelp26.0IdeaBodyCVIdea6.0 None18.0 Diorama26.018.0 Baited18.0 Tethered26.0Launch18.0 Timed18.0 Multi18.0Pickaxe18.0 Dark26.0Discourse Markers for CollaborationTo illustrate the coding process, we provide two snippets of transcript and the correlated coding process. Thefirst is from Group A, which had a lower Engagement Score. This could have been due to the fact the one girl,ICLS 2014 Proceedings594Â© ISLSHelen (gender-keeping pseudonyms have been used) seemed as if she did not want to work with her groupmates so much as delegate tasks to them. Part of this is evident in the fact that George did not say anythingduring this active brainstorming session. This created a tension between Helen and Karl who seemed to want toinvolve George and work as a cohesive team. Despite this greater than normal tension, they are still productivein sharing and discussing ideas.Karl:Helen:Karl:Helen:Karl:Helen:Karl:Helen:*drawing* So we could do- this is the whole vegetable garden. And then this is all thevegetables.I have an idea. I could build a paper fence. And thenNo, because - you know how they tried that and it didn't work? We could do thisNo no no no no. I mean like a tall paper fence and then *looks at name tag* Georgewhatever your name is- you could fill the holes with something and then you *points toK* could build some kind of ceiling on it.No like, yeah, I was thinking of the ceilingYeah so youand like a door that you can open and close.Um, the bunnies would be able to go through the door.Table 2: Novelty scores from Muncha Muncha Muncha activity final artifacts and presentationLegoGroupIntentionMeansSensorBodyScoreNovelty Score(out of 100)Wt = 0.25Wt = 0.25Wt = 0.25Wt = 0.25AScareChaseUserSingle562.5BScareChaseDarkSingle-Functional562.5CWarnLightPressureTethered7.593.75DScareChaseBaitedSingle-Functional562.5EScareCatapultPressureSingle5.568.75Lego Avg. Score:5.670PaperBotsGroupIntentionMeansSensorBodyScoreNovelty Score(out of 100)Wt = 0.25Wt = 0.25Wt = 0.25Wt = 0.25RedScareScarecrowNoneDiorama6.881.25BlueTrapDrop cageBaitedTethered7.893.75GreenHelpLaunchTimedMulti-functional7.893.75YellowScareScarecrowDarkTethered675WhiteHelpPickaxeDarkDiorama6.881.257.0485PaperBots Avg. Score:The solutions in this segment include the fence and the ceiling as they both explain how the group iskeeping the bunnies out of the garden whereas the addition of the door does not explain how they are keepingthe bunnies from the vegetables. Helen first introduces the idea of a fence on the second turn. Karlâ€™s response iscategorized as discuss because even though he rejects the proposal, he explains that in the story, the characteralready tried building a fence and it did not prevent the bunnies from eating the vegetables. The idea of theceiling is presented by Helen on the fourth turn and is immediately accepted by Karl so the response is coded asaccept.The second snippet is of group B which had a disruptive group member, Andrew, whom wasconstantly off task or suggesting inhumane solutions for keeping the bunnies out of the garden. Jane and Alexiswho originally tried to include Andrew eventually started ignoring and rejecting his proposed solutions, possiblybecause they found them counter-productive or viewed them as his way of playing around. The two girlsmanaged to complete the challenge with very limited assistance from Andrew.Jane:No no no no no. Like, like a net! A net.Andrew: No, itsAlexis: But a net could hurt them.Jane:No you just put it in it, like fish.ICLS 2014 Proceedings595Â© ISLSAndrew: Yeah, well what about like um like what about like a nuclear bomb? That won't hurtthem.Alexis: NoJane:The bunny catcher....Teacher: It's just going to drive around?Alexis: It will go around- it will go aroundAndrew: How is that suppose to scare them? Wait, no, it will drive around and then theyexterminate them with like giant lasersJane:If we could we would make it high speeds of running aroundTeacher: OkAlexis: Yeah, we could scare the bunnies because a lot of animals if you get too close to themthey get scared.Teacher: They'll run away.Alexis: Yeah, so we're just going to scare them away.Andrew: These are radioactive bunniesTable 3: Student Engagements from Muncha Muncha Muncha ActivityGroupAcceptanceEngagement Score (outNonScoreof 100)Accept Discuss engage Total (out of 100)DiscussionScore(outof100)A537155333.3320B264126716.6750C36110903060D22266733.3333EN/AN/AN/AN/A N/AN/AN/ALEGO12171443672740Red2215804040Blue41021687.527.7862.5Yellow7501210057.1442Green17210801070White232771.4328.5742.8627749863254Paperbots 16The solutions proposed in this segment are a net, a nuclear bomb, and giant lasers. The idea of a net isintroduced in the first turn by Jane and Alexis responds with concern for the bunnies being hurt by the netwithout accepting or rejecting the idea which means the response is categorized as discuss. A new proposal isbrought to the group by Andrew at turn five for a nuclear bomb where he assures his group mates that it wouldnot hurt the bunnies, however he is flat out rejected by Alexis which is coded as non-engage. Later, when Janeand Alexis are explaining to the teacher their current solution of a robot that drives around and scares bunnies,Andrew proposes giant lasers. This idea is not addressed in six turns, thus it is considered being ignored andcategorized as non-engage. The results of the discourse coding are summarized in Table 3.Collaborative Discourse Predicts Solution NoveltyEach groupâ€™s Novelty Score was charted and regressed against their Engagement Score (Figure 2), AcceptanceScore and Discussion Score. Because one video file was lost, we only had enough video to code nine groups.Our regression value (R) of .90 for Engage responses is statistically significant (p < 0.00042),indicating a strong correlation. However, when Accept and Discuss scores were charted against Novelty score,there was not a significant correlation (p < .13 and .08 respectively), indicated by the low regression values of0.40 and 0.50.ICLS 2014 Proceedings596Â© ISLSEngagement	 Â vs.	 Â Novelty	 Â Novelty	 Â Score	 Â 100	 Â 90	 Â 80	 Â 70	 Â 60	 Â 50	 Â 50	 Â 55	 Â 60	 Â 65	 Â 70	 Â 75	 Â 80	 Â 85	 Â Engagement	 Â Score	 Â Figure 2. Engagement versus novelty90	 Â 95	 Â 100	 Â DiscussionOur results show that there is a strong correlation between number of engage responses and novelty in thestudentsâ€™ final artifacts. Engaged responses in student discourse predict novel solutions. There has been otherresearch that shows there are ways to teach this type of discourse to students (Demetriadis, Egerter, Hanisch, &Fischer, 2011). Further, we have demonstrated the possibility of novelty assessment being used in a classroomlike environment using only observations, share outs and pictures of final artifacts.By identifying a type of student discourse that supports novelty, we began research that we hope willultimately inform teachers how to better foster creativity in the classroom. The method utilized in this paper is aquantitative way of defining the type of student discourse but in classrooms, in the moment, discourse must beobserved and the most beneficial discourse be fostered. â€œItâ€™s clear that the classroom teacher plays a critical rolein establishing and modeling practices of productive group learning processes and conversations. Observing agroupâ€™s interactions can provide teachers with valuable insightâ€ (Barron & Darling-Hammond, 2008) but only ifthey know what they are looking for. CSCL research has explored how a facilitator can influence discourse. Oneexample is having a tutor sustain and deepen inquiry through well-timed refocusing (Lakkala, Muukkonen, &Hakkarainen, 2005). It has long been taught that group members shouldnâ€™t say â€œtypes of comments that indicatecompetition, premature judgment, or failure to listen in group discussion" (McKendall, 2000). Our researchsupports this long-held idea and extends it to teachers actively supporting engaged responses in studentdiscourse.This study also illustrates the use of a creativity assessment instrument, specifically the method ofassessing novelty described by Shah et. al (2003), in a classroom-like environment. The prescribed noveltyassessment allows teachers to grade projects after-the-fact either by looking at the physical artifacts, pictures orvideo, depending on their preferences and resources. This is a vast improvement over other instruments thatadvocate identifying the provenance of an idea, not something that can readily be applied in a classroom, thoughadvancements in learning analytics may change this (Blikstein et al., 2012). The methods used in their currentform are unrealistic for timely use in classrooms but the novelty measure and others like it in the works of Shahet al (2003) may be useful for assessment as part of an application that organizes the student works and theirfeatures and takes care of the calculations. Despite the small sample size, the implications of a quick and easyway to measure even an aspect of creativity after-the-fact are clear. Teachers who have been able to intuitivelytell that one solution is more original than others will finally have a way to measure and support that sense. Inthe future, systematic assessment creativity could even become a more effective way to measure teamworkeffectiveness than current methods of asking groups to report on the other members or scattered observations.ConclusionsOpen-ended and group problem solving have been shown to lead to more robust individual understandings(Kapur, 2008). While teachers are constantly pressured to increase test scores, they are also expected to promoteteamwork and creativity through engineering design projects (NGSS, 2013). This study suggests that it may bepossible to combine methods from the learning sciences, art, and engineering education research to betteranalyze creative problem solving, and ultimately to develop classroom-practicable techniques for assessingcreativity.ReferencesBarron, B., & Darling-Hammond, L. (2008). Teaching for meaningful learning: A review of research oninquiry-based and cooperative learning. Powerful learning: What we know about teaching forunderstanding, 11-70.ICLS 2014 Proceedings597Â© ISLSBarron, B. (2003). When smart groups fail. The journal of the learning sciences,12(3), 307-359.Davies, D., Jindal-Snape, D., Collier, C., Digby, R., Hay, P., & Howe, A. (2013). Creative learningenvironments in educationâ€”A systematic literature review. Thinking Skills and Creativity, 8, 80â€“91.Demetriadis, S., Egerter, T., Hanisch, F., & Fischer, F. (2011). Peer review-based scripted collaboration tosupport domain-specific and domain-general knowledge acquisition in computer science. ComputerScience Education, 21(1), 29â€“56.Develaki, M. (2010). Integrating Scientific Methods and Knowledge into the Teaching of Newtonâ€™s Theory ofGravitation: An Instructional Sequence for Teachersâ€™ and Studentsâ€™ Nature of Science Education.Science & Education, 21(6), 853â€“879.Dillenbourg, P. (1999). What do you mean by collaborative learning?.Collaborative-learning: Cognitive andcomputational approaches., 1-19.EtelÃ¤pelto, A., & Lahti, J. (2008). The resources and obstacles of creative collaboration in a long-term learningcommunity. Thinking Skills and Creativity, 3(3), 226â€“240.GlÃ¼ck, J., Ernst, R., & Unger, F. (2002). How Creatives Define Creativity : Definitions Reflect Different Typesof Creativity. Creativity Research Journal, 14(1), 55â€“67.Kapur, M. (2008). Productive Failure. Cognition and Instruction, 26(3), 379â€“424.Kurtzberg, T. R., & Amabile, T. M. (2001). From Guilford to creative synergy: Opening the black box of teamlevel creativity. Creativity Research Journal,13(3-4), 285-294.Lakkala, M., Muukkonen, H., & Hakkarainen, K. (2005). Patterns of scaffolding in computer mediatedcollaborative inquiry. Mentoring & Tutoring: Partnership in Learning, 13(2), 281â€“300.McKendall, M. (2000). Teaching Groups to Become Teams. Journal of Education for Business, 75(5), 277â€“282.Merrill, J. A., Charyton, C., & Jagacinski, R. J. (2008). CEDA: A research instrument for creative engineeringdesign assessment. Psychology of Aesthetics, Creativity, and the Arts.NGSS. (2013). Engineering Design in the NGSS (pp. 1â€“7).Oâ€™Connell, B. (2013). PaperBots: An Inexpensive Means for Engineering Education. In 120th ASEE AnnualConference & Exposition. American Society for Engineering Education.Peterson, C. W., & Madsen, C. K. (2010). Encouraging Cognitive Connections and Creativity in the MusicClassroom. Music Educators Journal, 97(2), 25â€“29.Renzulli, J. S. (2005). The Three-Ring Conception of Giftedness: A developmental model for promotingcreative productivity. In R. J. Sternberg & J. E. Davidson (Eds.), Conceptions of giftedness (pp. 246â€“279). Cambridge University Press.Saunders, R., & Gero, J. (2002). How to study artificial creativity. Proceedings of the 4th conference onCreativity & â€¦, 80â€“87.Scardamalia, M., Bereiter, C., & Lamon, M. (1994). The CSILE Project: Trying to Bring the Classroom intoWorld 3. In Classroom lessons Integrating cognitive theory and classroom practice (pp. 201â€“228).Shah, J. J., Vargas-hernandez, N., & Smith, S. M. (2003). Metrics for measuring ideation effectiveness. DesignStudies, 24, 111â€“134.Soller, A., & Lesgold, A. (2007). Modeling the process of collaborative learning. In The Role of Technology inCSCL (pp. 63-86). Springer US.Sternberg, R. J. (2005). WICS: A Model of Positive Educational Leadership Comprising Wisdom, Intelligence,and Creativity Synthesized. Educational Psychology Review.Sullivan, F. R. (2011). Serious and Playful Inquiry : Epistemological Aspects of Collaborative Creativity, 14,55â€“65.Taylor, C. W. (1988). Various approaches to and definitions of creativity. The nature of creativity, 99-121.Van Harpen, X. Y., & Presmeg, N. C. (2013). An investigation of relationships between studentsâ€™ mathematicalproblem-posing abilities and their mathematical content knowledge. Educational Studies inMathematics, 83(1), 117â€“132.Webb, A. N., & Rule, A. C. (2012). Developing Second Gradersâ€™ Creativity Through Literacy-ScienceIntegrated Lessons on Lifecycles. Early Childhood Education Journal, 40(6), 379â€“385.White, T. (2006). Code talk: Student discourse and participation with networked handhelds. InternationalJournal of Computer-Supported Collaborative Learning.Yuan, X., & Sriraman, B. (2011). An exploratory study of relationships between studentsâ€™ creativity andmathematical problem-posing abilities. In The elements of creativity and giftedness in mathematics (pp.5-28). SensePublishers.AcknowledgementsThis project is funded by the National Science Foundation DRK-12 program, grant # DRL-1020243, as well asby LEGO. Any opinion, findings, conclusions or recommendations expressed in this material are those of theauthor(s) and do not necessarily reflect the views of the NSF or LEGO. We thank Merredith Portsmore, ElissaMilto, Dan Wise, Joe Sanford, and Chelsea Andrews for their assistance.ICLS 2014 Proceedings598Â© ISLS