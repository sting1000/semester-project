Taking DALITE to the Next Level: What Have We Learned from aWeb-Based Peer Instruction Application?Elizabeth S. Charles and Chris Whittaker, Dawson College,echarles@dawsoncollege.qc.ca, cwhittaker@dawsoncollege.qc.caNathaniel Lasry and Michael Dugdale, John Abbot College,lasry@johnabbott.qc.ca, michael.dugdale@johnabbott.qc.caKevin Lenton, Vanier College, lentonk@vaniercollege.qc.caSameer Bhatnagar, Dawson College, sbhatnagar@dawsoncollege.qc.caJonathan Guillemette, McGill University, jonathan.guillemette@mail.mcgill.caAbstract: The Distributed Active Learning Interactive Technology Environment (DALITE)is a web-based tool designed on the principles of Peer Instruction. DALITE promotesstudent’s self explanation and asynchronous explanation to others. This design-based researchproject involved practitioners and researchers in the co-design process. In this paper wedescribe the main features of DALITE, its extended system and its implementation. We reporton its effectiveness as a tool for teaching and learning physics.IntroductionA fundamental concern in science education, and physics in particular, continues to be the difficulty studentsexperience building robust understandings of core principles and concepts along with their ability to use them indifferent settings – i.e., conceptual change and transfer of learning. Recent studies and practice-based efforts toaddress these problems tell us that science learning and teaching can benefit from pluralistic approaches (e.g.,Treagust & Duit, 2008). These include changes to the ways students engage with the content and each other, theways teachers orchestrate and use new pedagogical approaches, and the ways we design tools that support thesevarious socio-cognitive and socio-cultural processes.The Distributed Active Learning Interactive Technology Environment (DALITE) is a web-based toolthat aims to promote conceptual learning while working within an asynchronous mode of student engagement. Itinvolves the learners in a variety of tasks including writing explanations for conceptual questions, reflecting onand comparing these explanations to those of peers and experts, and taking part in the social construction of thedatabase repository by voting on the most convincing explanations. It is part of a larger study and system ofsocial constructivist pedagogical practices conceived to promote learning in physics at the postsecondary level.Its design draws on social conceptions of conceptual change, recognition of the role of context (e.g., Engle,2006) and the success of the practical approach to conceptual learning called Peer Instruction (Mazur, 1997). 	  The study and evolution of DALITE is a design-based research (DBR) experiment (Hoadley, 2002). Itsdevelopment team is made up of researchers and practitioners (college-level physics instructors) workingtogether in using a co-design approach. DALITE is currently in its third iteration. In this paper we provide abrief background of the theoretical foundations of its design and features, including the extended system it hasbeen embedded within (tagging and concept mapping activities). Additionally, we provide an evaluation of itseffectiveness, to date, and speculate on its potential as a tool to support students’ conceptual change as well asteachers’ efforts to enact an active learning curriculum. 	  BackgroundInvestigating how students learn physics has been a perennial concern not only of physics education research(PER) but also of the learning sciences (e.g., diSessa & Sherin, 1998). The body of research generated by bothcommunities confirms that conceptions of the physical world, such as force, motion, and acceleration, aredifficult to change with traditional instruction (e.g., Hestenes, 1992). However, studies of social constructivistinstruction, popularly referred to as active learning pedagogy, report findings of statistically significant gains instudents’ conceptual understanding in physics and other science disciplines (Meltzer & Thornton, 2012). Ofparticular interest are implementations that focus on promoting conceptual change by placing an emphasis onintentional reflection (Sinatra & Pintrich 2003). Adding to this are questions about the processes involved inself-explanation (Chi, de Leeuw, Chiu & LaVancher, 1994) versus forms of peer explanations such as reciprocalteaching (Palincsar & Brown, 1984), and other collaborative and discursive practices (Stahl, 2006). In fact, itmight be argued that there is value in examining the processes of what might be referred to as “interactiveexplanation” (Ploetzner, Dillenbourg, Preier & Traum, 1999). This interception between explaining to others, aswell as reflecting on one’s own explanation provides a power nexus for investigation. We propose that such anexus is found in the variation on Peer Instruction that is at the heart of our designed intervention, DALITE.	  ICLS 2014 Proceedings982© ISLSPeer Instruction Approach to LearningPeer Instruction (PI) is an example of an evidence-based pedagogical innovation popularized by Eric Mazur(Mazur, 1997). Its method of engaging students in scientific discourse focuses on acts of explanation,comparison, and reflection that lead to conceptual change. Meltzer (2013) states that, at the postsecondary level,PI is one of the most widely used active learning approaches in North America. No doubt in large part becauseof the growing body of research supporting claims of its efficacy in producing statistically significant conceptualgains (e.g., Crouch & Mazur, 2001).In PI implementations, instructors present students with multiple-choice conceptual questions that thestudents answer using wireless handheld devices, colloquially referred to as clickers. These initial pollingactivities provide instructors with real-time feedback on the status of students’ understanding. Answering thesequestions allows instructors to know whether or not concepts are known, somewhat known or unknown tostudents. If conceptual understanding falls within the “known” range (correctly answered by more than 70% ofstudents), the teacher can move forward to another concepts and questions. If it is “unknown” (correctlyanswered by less than 30% of students), the teacher is advised to revisit the ideas. The real peer-to-peerinteractions only come into play with the “somewhat known” concepts (30-70% correctly answered). Whenresponses fall within this range, students are asked to turn to their neighbor and discuss their answers andreasoning. It is arguable that these discursive practices allow students to engage in sense making and intentionalreflection on these specific concepts.Some of the most successful implementations of PI have been those found in large lecture halls withhundreds of students. In such settings, rich discussions can arise because of the larger probabilities of havinggreater diversity among students, which undoubtedly acts to amplify the cognitive dissonance. However, insmaller classrooms there is often less diversity between students’ answers and their understandings leading to apaucity of conceptual discussions. In such cases, PI has not always worked well. Adding to this, there is thequestion of what happens if we were to take PI online. How would it work if peers cannot interact in real-time?With growing interest in active learning pedagogies, which benefit from having students prepared ahead of classwork – i.e., the flipped classrooms – there is added pressure on getting design elements for digital and onlinelearning right.Digital and Online Instruction of PhysicsComputer supported learning environments to promote learning in physics is not new. A major initiative in thisarea is the Andes project, an intelligent tutoring system for a first year college-level physics course (Gertner,Conati & VanLehn 2000). It coaches students through the problem solving process step by step and provideshints should the student get stuck. Andes and similar tutoring systems are very successful and producesignificant learning gains compared to traditional instruction. However, some have criticized Andes, and othersimilar tutoring systems, for failing to get at deep learning. In particular, three weaknesses have been identifiedas the failure to get students: (1) to use the language of the discipline (i.e., talk science); (2) to reflect moredeeply on the learning; and, (3) to work on developing their conceptual knowledge (Graesser, VanLehn, Rosé,Jordan & Harter, 2001). Viewing these as challenges to be overcome when designing an online learningenvironment, we consider these as the foundation of our design features.Building Collective ArtifactsRecent studies conducted by Slotta and colleagues show an increased sense of group regulation (aka agency)and collective responsibility when students contribute to commonly shared “knowledge base” resources (Slotta& Najafi, 2010). In addition it has long been shown that providing students with a sense of contribution hasimplications for promoting epistemic agency (Scardamalia & Bereiter, 2006).Our Four Design PrinciplesIn the process we can examine the four design principles we extracted from the literature: (1) promoting the usethe language of the discipline (i.e., physics talk); (2) promoting reflections on explanations, one’s own and thatof others – i.e., the interactive explanation; (3) promoting deeper understandings of conceptual structure; and (4)promoting students’ agency and responsibility for examining their peers’ arguments and assessing theircorrectness and quality.MethodsThis project uses a design-based research (DBR) methodology. DBR allows for the design of tools or conceptualmodels that help us better understand the conditions under which the context and/or the intervention canpromote better learning outcomes, and in turn, to adapt these to support better learning (Anderson & Shattuck,2012). Data collected were for the purpose of informing us on the design principles related to promotion ofconceptual understanding and conceptual change. Mixed-methods were used that include: standardized pre-postquestionnaires (i.e., the Force Concept Inventory (FCI), Hestenes et al., 1992); and course grades. Qualitative	  ICLS 2014 Proceedings983© ISLSdata on student’s conceptual understanding is documented in their DALITE rationales. Student interviews wereconducted and include video recordings of think-aloud protocols that help to reveal how DALITE was used andhow it was perceived as a tool to promote conceptual learning. Classroom observations were also collected todocument the ways in which DALITE was used as part of the instructor’s system of active learning practices.Context and ParticipantsThe study is situated within physics classrooms in three English-speaking colleges in Quebec. Four instructorsparticipated and each of them was also a member of the research team (see Table 1). Student participants(N=168) were first year science majors, ages 17-19, enrolled in one of five sections of a 15-week introductoryphysics course – approximately equivalent to first-year university. DALITE was assigned weekly as homeworkvia the web. Approximately four to five questions were assigned weekly along with other readings and problemsolving activity. DALITE was brought into the classroom setting regularly, which included having the instructorfollow up with the correct answers and elaboration on questions that were identified as challenging.Additionally, it was made part of an extended activity that involved concept mapping and tagging activities.These will be discussed only briefly.Table 1: Number of instructors and students participating in the study.# of Instructors# studentsClassroom designCollege Xn=1; 2 sectionsn=30 and n=31Active Learning classroomCollege Yn=1; 1 sectionn=30Active Learning classroomCollege Zn=2; 1 section eachn=36; n=41Hybrid classroomsDesigning DALITEDALITE was conceived as a way to harness the benefits of PI and address some missed opportunities. Intraditional enactments of PI, student conversations disappear into the ether. Though instructors sometimesoverhear conversations, there is no trace left behind. More importantly, instructors seldom know what types ofarguments and reasoning are convincing to students. What discursive elements help students change theiranswers – whether they change towards or away from the correct answer. In addition, there is littledocumentation of the accumulated database of conceptual questions, and what can be learned from how they areresponded to. In short, how the question frames the context. Might there be different effects regarding the timingof assigning questions, might there be an issue of the context of delivery or the wording. Can we promote betterforms of intercontextualization with the sequencing and design of questions? DALITE as a solution providesstudents with a diversity of explanations for all the possible answer choices. It allows students to interact withrationales from students at different institutions or even “peers” who took the class previously.The DALITE infrastructure is made up of the following components: (1) a student registration andsoftware application management; (2) a framework for data mining and tracking of student interactions in realtime including the instructional scripts; (3) a central database or repository; and (4) data displays for instructors.The platform uses “Agile” development practices with the aim of ensuring future availability, scalability, andperformance. The database repository is composed of two parts: (1) the curriculum content – conceptualmultiple-choice questions (sometimes referred to as concept test questions); and, (2) the student-generatedanswers and rationales for these answers.To date, the curriculum database contains over 120 questions spread across the three main topicsgenerally covered in an introductory physics course – i.e., kinematics, dynamics, and energy and momentum.These questions are designed to be roughly at the first-year university level. Influenced by the Ohio Stateconcept test questions (Lee, Ding, Reay, & Bao, 2011), many questions are organized into sets of three to fourquestions on a single concept that progressively increase in difficulty. These sets of increasingly difficultmultiple-choice problems are built on similar deep structures with different surface features, or similar surfacefeatures with different deep structures. Instructors have control over the selection and assignment of questionsusing a specially designed teacher portal. Each problem set can be customized to meet the perceived knowledgelevel of the students.The second database repository, the student-generated rationales, has been developed through a“seeding” process. That is, the database asks about 20 students to answer the questions and write rationales,without working through the full DALITE script. This process enables the first participants in the system to seeother students’ rationales. However, it places constraints on the development of new questions entering thesystem. In addition, because rationales are student-generated we believe it necessary to develop a mechanism ofcleaning up and categorizing the database. This has lead to the implementation of a voting system – studentshave the option of giving a “thumbs up” to the rationale that convinced them. In doing so, these ratings are adesign element. In the future a heuristic will be designed to highlight these popular rationales. Lastly, nonsenserationales will be eliminated (e.g., unreadable text, meaningless strings of symbols).	  ICLS 2014 Proceedings984© ISLSLastly, the data display for instructors brings DALITE into the classroom. The display provides aninterface to allow the instructor to review students’ progress in real-time as well as provide a tool for in-classreview. It has proven to be more important than we had thought. We discuss this in the upcoming section.DALITE Scripts: How Does It Work?The script for DALITE mirrors much of what we imagine students do when they engage in the discursivepractices of PI. As such, the DALITE script consist of the following six steps: (S)elect multiple choice answer;(W)rite rationale; (R)econsider based on alternatives; (R)evote; (V)ote on most convincing; and (R)eviewexpert rationale. In step 1, students are presented with a multiple-choice conceptual question. They are asked toselect an answer from the multiple choices. In step 2, they then write an explanation for their choice, what wecall rationales. In step 3, they are asked to reconsider their answer based on another possible alternative, the aimis to replicate the experience of the “turn to your neighbor” phase in PI. If their answer is incorrect, they arepresented with student rationales for the correct answer as well as rationales from other students on the sameincorrect answer they chose. The aim of this comparison is to present the contrast and cognitive dissonance oftraditional PI. If their answer is correct, they are presented with rationales from other students on the samecorrect answer as well as student rationales for the most popular wrong answer; the aim of this comparison is totest for fragile understanding or lucky guessing. In step 4, the student is asked to consider whether one of therationales was particularly convincing, if yes they are asked to vote it “thumbs up”. In step 5, students are askedto re-choose an answer for the original question: either their original answer, or the other answer that was justpresented to them, based on the reading of these rationales. Lastly, step 6, they are presented with a normativerationale of an expert, but are not given “the” answer; the aim of this decision being to delay feedback andincrease self-regulation of criteria and standards.Other Features Designed to Extend the DALITE SystemIn addition to the online components, we consider DALITE to be embedded into an extended system thatincludes a Tagging and a Concept Mapping tool. The tagging tool is digital and paperbased. It is designed toprompt students’ thinking about the deep structure of the content contained in the DALITE questions. As such,this tagging tool takes students through a series of cascading concepts – from general to specific. It starts with aDALITE question, then asks the students to reflect on and identify/tag key concepts, first individually, thencollaboratively in small groups.The concept mapping tool is computer-based, and presently uses C-Map (citation). It takes the oppositeapproach to the tagging tool. It starts by asking students to work collaboratively to identify connections and staterelationships between a restricted set of concepts – in the process creating a concept map. It then asks studentsto add in the DALITE questions to the appropriate area of the map. At the end of this process, students are askedto work on the maps individually, as a reflection exercise.These tools were implemented separately. Two section worked with the tagging tool (College X).Meanwhile another two sections worked with the concept mapping tool (one section at each College Y & Z). Inall instances students were asked to write rationales for the DALITE questions after the activity.Building on DALITE’s ImplementationDALITE implementation over the five sections of 168 students produced approximately 7182 student-generatedrationales. The actual distribution by college is described below (see Table 2). These variations in the number ofDALITE questions assigned, with the respective variations in the number of rationales written, allowed us toexamine the impact of these different modes of implementation. Results also show a statistical differencebetween DALITE students (average mean Hake gain = 0.49) compared to non-DALITE comparison group(mean = 0.31). Interestingly, the FCI gains for students in these five sections are near identical (0.59). Thesegains are calculated as the number of transitions of wrong to right answers divided by those that were initiallywrong. These results suggest that using DALITE for conceptual gains may not be dependent on the quantity ofquestions but more likely the choice of questions – i.e., the difficulty and timing (pre-post instruction).Table 2: Descriptive statistics on the rationales generated by students in the five sections, across the 3 colleges.# Rationales written/ studentCollege X1College X2MeanModeMedian515856566663College Y364839College Z1College Z2405045344836DiscussionIn regards to the four design features described earlier: (1) The DALITE rationales show that students have beenmoving towards identifying what we consider the “trigger feature” when explaining their answers. The	  ICLS 2014 Proceedings985© ISLStrajectory of their rationales also shows more complete explanations over time. In doing so, these data suggestthat the design of repeatedly asking for rationales can promote improved use of physics talk. (2) Our think-aloudprotocols and interviews with students show that they have a high level of metacognitive activity when usingand discussing their use of DALITE. In fact, one surprising finding is that the young women (?) spentconsiderably more time reflecting on their explanations. One student, whose first language was not English,stated that the reading of rationales has taught her how to better read and understand physics explanations on theinternet, which are frequently in English. 3) Observations of students’ tagging activity as well as their interviewssuggest that the combined use of DALITE and tagging have promoted their understanding of the deeper,structural similarities between questions, regardless of the surface features. Subsequent assessment activities,referred to as “sorting tasks” show that these students are better able to identify similarities between questionscompared counterparts who have not used the DALITE system.The instructor display of student results has proven to be a very important feature of DALITE.Arguably, it the most practical tool for instructors. It provides immediate and detailed feedback to instructorsand greatly supports the flipped classroom method or active learning approach in general. Students are betterprepared for class and teachers can identify conceptual issues before class and tailor their class to focus on thesespecific issues. Also it allows students to focus on both the collective and the individual – what does the classknow, what do specific individuals need to know (where are they falling short), a feature we only identified instudent interviews.ReferencesAnderson, T., & Shattuck, J. (2012). Design-Based Research A Decade of Progress in EducationResearch?. Educational Researcher, 41(1), 16-25.Chi, M. T., Leeuw, N., Chiu, M. H., & LaVancher, C. (1994). Eliciting self-explanations improvesunderstanding. Cognitive science, 18(3), 439-477.Crouch, C. H., & Mazur, E. (2001). Peer instruction: Ten years of experience and results. American Journal ofPhysics, 69, 970.diSessa, A. A. & Sherin, B.L. (1998). What changes in conceptual change? International Journal of ScienceEducation, 20 (10), 1155-1191Engle, R.A. (2006). Framing interactions to foster generative learning: A situative explanation of transfer in acommunity of learners classroom. Journal of the Learning Sciences, 15(4), 451-498.Gertner, A. S., & VanLehn, K. (2000). Andes: A coached problem solving environment for physics.In Intelligent Tutoring Systems (pp. 133-142). Springer Berlin Heidelberg.Graesser, A. C., VanLehn, K., Rosé, C. P., Jordan, P. W., & Harter, D. (2001). Intelligent tutoring systems withconversational dialogue. AI magazine, 22(4), 39.Hestenes, D., Wells, M., & Swackhamer, G. (1992). Force concept inventory. The physics teacher, 30, 141.Hoadley, C. (2002). Creating context: Design-based research in creating and understanding CSCL. In G. Stahl(Ed.), Computer Support for Collaborative Learning, (pp. 453-462). Mahwah, NJ: Lawrence ErlbaumAssociates.Lee, A., Ding, L., Reay, N. W., & Bao, L. (2011). Single-concept clicker question sequences. The PhysicsTeacher, 49(6), 385-389.Mazur, E. (1997). Peer Instruction : A User's Manual. Upper Saddle River, NJ: Prentice HallMeltzer, D. E., & Thornton, R. K. (2012). Resource letter ALIP–1: active-learning instruction inphysics. American journal of physics, 80(6), 478-496.Palinscar, A. S., & Brown, A. L. (1984). Reciprocal teaching of comprehension-fostering and comprehensionmonitoring activities. Cognition and instruction,1(2), 117-175.Pfundt, H., & Duit, R. (2009). Bibliography: Students’ alternative frameworks and science education. Kiel,FGR: Institute for Science Education.Ploetzner, R., Dillenbourg, P., Preier, M., & Traum, D. (1999). Learning by explaining to oneself and toothers. Collaborative learning: Cognitive and computational approaches, 103-121.Scardamalia, M., & Bereiter, C. (2006). Knowledge building: Theory, pedagogy, and technology. TheCambridge handbook of the learning sciences, 97-115.Sinatra, G. M., & Pintrich, P. R. (2003). The role of intentions in conceptual change learning. Intentionalconceptual change, 1-18.	  Slotta, J. D., & Najafi, H. (2010). Knowledge communities in the classroom. International encyclopedia ofeducation, 8, 189-196.Stahl, G. (2006). Group cognition: Computer support for building collaborative knowledge. Cambridge, MA:MIT Press.Treagust, D. F., & Duit, R. (2008). Conceptual change: A discussion of theoretical, methodological and practicalchallenges for science education. Cultural Studies of Science Education. Published online 29 March2008: doi:10.1007/s11422-008-9090-4.	  ICLS 2014 Proceedings986© ISLS