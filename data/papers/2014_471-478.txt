Designing Critique to Improve Conceptual UnderstandingElissa Sato and Marcia C. Linn, University of Berkeley, California, 4523 Tolman Hall, Berkeley CA 94720elissa.sato@berkeley.edu, mclinn@berkeley.eduAbstract: Students become entangled in their varied scientific ideas and struggle to reconciletheir understanding with ideas encountered in instruction. This design-based study with asixth-grade technology-enhanced inquiry science unit on global climate change investigateshow critique can support students in refining their conceptual understanding. Specifically, thestudy investigates whether students’ ability to benefit from critique is impacted by thecomplexity of the critique artifact. Findings show that students can equally benefit fromcritiquing explanations of varying complexity when guided to consider a range of alternativeideas during critique The results show the value of designing critique to support students indistinguishing among their own and alternative ideas. Case studies illustrate how studentsengaged with opportunities provided by the guidance, and indicate areas where furtherresearch is necessary to refine the design of critique as a means to support conceptual learningin science.RationaleStudents become entangled in multiple, often conflicting ideas about scientific phenomena as they interact withthe natural world and struggle to distinguish new ideas from existing beliefs (e.g., Clark, 2006). Both childrenand adults resist and discount evidence that contradicts their existing beliefs (Chinn & Brewer, 1993). Yet,citizens need to develop the ability to use scientific evidence to critique ideas of others and to interpret critiquesof their own ideas. Efforts to date offer some promise for critique but also reveal the need for clarification ofhow, when, and why critiques are beneficial for conceptual learning (e.g., Shen, 2010). This study seeks toadvance our understanding of critique by comparing two approaches to designing critique.In designing critique, we draw on the constructivist knowledge integration (KI) framework (Linn &Eylon, 2006) that addresses the difficulties students have in making sense of their multiple, conflicting ideas. KIcalls for building on the repertoire of ideas students develop in their lives by designing inquiry experiences thatsupport students in considering alternatives and refining their conceptual repertoire. However, students’ abilityto distinguish among alternatives during critique may be dependent on the complexity of the critique artifact.The first approach draws on the notion of the zone of proximal development (Vygotsky, 1978), which suggeststhat students are most likely to benefit when the learning task is designed to align with their prior knowledgesuch that the task is accessible and allows students to make progress with appropriate guidance. Guidingstudents in critiquing a normative response that is incomplete yet slightly more sophisticated than their currentexplanation could support students in distinguishing among ideas without being overwhelmed by complexity.However, alternative perspectives such as desirable difficulties in psychology (Bjork, 1994) andproductive failure in mathematical problem solving (Kapur & Bielaczyc, 2012) suggest that reducing thecomplexity of cognitive tasks may have a detrimental impact on student learning by deemphasizing the need todistinguish among ideas in their conceptual repertoire. From the KI perspective, conceptual critique involvesdistinguishing among normative and non-normative ideas. Critiquing a slightly more sophisticated normativeresponse may not support students in this process because the non-normative ideas are not explicit. Studentsmay be content with addressing the more obvious flaws and neglect to reflect on the range of ideas. Thus,critiquing a complex response with a mix of normative and non-normative ideas may be more successful insupporting deep understanding by prompting students to reflect more holistically on their conceptual repertoire.Our study therefore seeks to address the following research questions:1.2.How do students benefit overall when they critique (a) an incomplete explanation with normativeideas to identify a missing idea (incomplete) or (b) an incomplete explanation combiningnormative and non-normative ideas to identify a non-normative idea (non-normative)?How do students’ ideas, as expressed in their explanations, shift in response to critique?We hypothesized that we would observe significant differences between conditions in students’learning gains if the potential benefit of critique depended upon carefully designing an accessible or desirablydifficult critique artifact that was aligned to the students’ prior knowledge. On the other hand, we hypothesizedthat students in both conditions would make comparable progress in their learning if the potential benefit ofcritique were less dependent on the complexity of the critique artifact and more dependent on whether studentswere appropriately supported in considering alternative ideas.ICLS 2014 Proceedings471© ISLSMethodsA sixth-grade technology-enhanced earth science curriculum unit, Global Climate Change (GCC, Figure 1a),was developed in the Web-based Inquiry Science Environment (WISE, Linn, Davis, & Bell, 2004) using the KIperspective. In GCC, students grapple with the complex energy mechanisms driving changes in global climatethrough a series of interactive NetLogo simulations (Svihla & Linn, 2012). Students are provided with multipleopportunities to explain causal subsets of this complex system before generating an integrated explanation of theoverall phenomenon. They investigate how factors such as greenhouse gases impact energy transformation andhow that in turn impacts global temperature trends. Student explanations were coded for the sophistication oftheir mechanisms. We focused on an explanation targeting an energy transformation process critical tounderstanding the phenomenon of global climate change. The GCC unit was completed by 68 middle schoolstudents working in pairs taught by the same teacher, who had taught previous versions of the unit.Activity sequence. Student pairs were randomly assigned to one of two promising approaches foraligning critique artifacts to students’ ideas (Fig, 2). Based on the design research paradigm, the study sought toinvestigate whether one condition is better than the other. In the activity sequence students generated an initialexplanation, critiqued and revised an assigned explanation, received conceptual guidance on their critique, thencritiqued and revised their initial explanation (Figure 2). The design focused on encouraging students to revisitevidence steps (e.g., simulations), which has been correlated with learning gains (Svihla & Linn, 2012), and todiscuss and negotiate alternative ideas presented through the critique artifact and critique choices.(a)(b)Figure 1. (a) The WISE Global Climate Change unit. (b) The guidance checkpoint step.• Students generate initial explanationGenerate• Students critique a researcher-assigned explanation• Incomplete Critique: explanation with normative ideas expressing partial understanding• Non-Normative Critique: explanation with a non-normative idea plus normative ideasCritiqueexpressing partial understanding• Students revise the assigned explanation based on their critique choice• Students receive conceptual guidance on their critique choice and revisit the evidence stepGuidanceRevise• Students critique their initial explanation• Students revise their initial explanation based on their critique choiceFigure 2. Outline of the overall activity sequence.The shaded step indicates where the curriculum design differed between the two versions.	  ICLS 2014 Proceedings472© ISLSCritique artifacts. For each condition, three critique artifacts were designed by the researcher based onthe analysis of responses collected during previous classroom implementations. The researcher assigned anexplanation that expressed partial understanding and was slightly more sophisticated than the initial explanationgenerated by the students. The incomplete group critiqued an explanation containing only normative ideas. Thenon-normative group critiqued a modified version of the incomplete explanation with a non-normative idea.During critique, students in both groups selected a science content critique from among several alternatives.This entailed distinguishing among alternatives. They then revised the critiqued explanation based on theirchoice (Table 1).Table 1: Critique step guidance.Feature1) Critique ArtifactPromptThe other team’s response:Solar radiation was absorbed by earth and released as infrared radiation.2) Critique of Surface FeaturesScore this response for spelling, grammar, and punctuation:• Very good: No spelling, grammar, or punctuation errors• Good: Few spelling, grammar, or punctuation errors• Not So Good: Many spelling, grammar, or punctuation errors3) Critique of Science ContentWhat needs to be changed in the response to improve the scientificevidence? The response can be improved by…• Explaining what kind of energy SR becomes when it is absorbed.• Explaining that IR comes from the Sun.• Explaining that SR becomes IR when SR is reflected.• Explaining that SR becomes IR when SR is absorbed.• Adding more evidence in general.4) Revision of Critique Artifact Change and improve the other team’s response based on your choice above.Note. The design of the critique guidance was adapted from a previous study (Sato & Linn, 2011). The guidancewas consistent across conditions. The conditions differed in the explanations they were assigned to critique andin the science content critique choices displayed. The same guidance was provided in the revision step.The critiques for scientific evidence were specific to the explanation type and targeted missing ideas andconnections among ideas in the incomplete group, or non-normative ideas and connections in the non-normativegroup. Critique choices were calibrated such that both conditions considered the same range of alternatives for agiven explanation.Guidance checkpoint. Both conditions had additional opportunities to consider the same alternativeswhen they received automated conceptual feedback on their critique choice at a guidance checkpoint. Theyreceived a guiding question and were prompted to revisit a critical step to reevaluate the evidence (Figure 1b).During the guidance checkpoint, students were discouraged from mindless guessing with choices that changedorder between attempts and a diminishing score structure. During revision, students were also prompted to drawon their critique experience by applying the same criteria to their explanation. They were also prevented fromreferencing the critique step so that they would not copy ideas from the critique artifact.DataStudent work formed the core data source; the unit of analysis was the dyad. Student responses were codedusing a rubric based on the KI framework, which rewards coherence of ideas as represented by the number andcomplexity of connections students make between their ideas (see Table 2). Ten pairs whose initial explanationsalready demonstrated complex understanding were removed from the analysis.Table 2: Knowledge Integration rubric used to score students’ original and revised explanations.Explanation Prompt:Where did infrared radiation (IR) come from in the model? Give as much detail as you can.ScoreDescriptionStudent Examples1No answer or irrelevantI don’t know(Irrelevant) answers2Non-normative ideas or links It came from the sun and space.(No Link)3One relevant and normativeIR came from conduction, under the earths crust. The Solar(PartialideaRadiation transforms into heat energy that bounces off earthsLink)crust and is trapped by the greenhouse gases and unable toescape earths atmosphere.ICLS 2014 Proceedings473© ISLS4(Full Link)5(ComplexLink)Scientifically valid and fullyelaborated link between tworelevant and normative ideasAt least two links amongthree or more relevant andnormative ideasIt comes from heat energy when heat energy is released it goesinto the Infrared radiation, so it becomes heat energy.Some solar radiation is reflected back into space, and some isabsorbed. The SR that is absorbed becomes heat energy, andheats up the Earth. It is in there for a while, and is eventually isreleased back into the atmosphere as infrared radiation.Note. Examples are actual unedited responses by students.Impact of the GCC Unit on Overall Learning GainsStudents made significant pretest to posttest gains across conditions (Table 3). There was no significant effect ofcondition after controlling for pretest scores (F(1,27)=0.45, p>.05). Thus all students benefitted from the unit,including the critique activities.Table 3: Means and standard deviations for pre and posttest by condition.PretestPosttestNMSDMSDtEffect SizedpAll29 pairs2.970.194.000.935.681.54<.001Incomplete11 pairs3.000.003.810.873.111.32<.05Non-Normative18 pairs2.940.244.110.964.751.67<.01Weighing Alternatives Effective for Supporting Revision in Both ConditionsOn the embedded assessments, there was significant improvement from the students’ original to revisedexplanation across groups (Table 4). The critique guidance helped students in both conditions revise theirexplanations, with medium effect sizes. There was a slight trend for the non-normative condition to make largergains but no significant differences between conditions after controlling for pretest scores (F(1,27)=0.05, p>.05).Table 4: Means and standard deviations for original and revised explanation scores by condition.OriginalRevisedtEffect SizeNMSDMSDdpAll29 pairs2.720.753.241.023.550.58<.005Incomplete11 pairs2.910.833.691.122.890.46<.05Non-Normative18 pairs2.610.703.170.992.560.66<.05Shifts in Students’ IdeasNumber	  of	  Ideas	  in	  Responses	  (Cumula5ve)	  We analyzed students’ initial and revised explanations for a shift in use of science ideas. Students’ explanationswere coded for scientifically valid ideas that were targeted by the explanation prompt, as well as non-normativeand partially normative ideas used to assign students to specific critique artifacts (Figure 3). Ideas were coded aspartially normative when their mechanistic depth was missing details that were targeted by the explanationprompt, but were not non-normative per se.40	  35	  30	  25	  20	  Original	  15	  10	  5	  0	  Revised	  Non-­‐Norma/ve	   Par/al	  Norma/ve	  Norma/ve	  Students'	  Ideas	  Figure 3. Shifts in students’ ideas expressed in original and revised explanations across conditions.Although not an exhaustive list, the ideas were selected for coding based on their prevalence in studentresponses collected during previous implementations of the unit. There was a significant gain across conditionsfor normative ideas, t(29)=3.09, p<.01, d=.53; the decrease in non-normative ideas approached significanceICLS 2014 Proceedings474© ISLSt(29)=-1.80, p=.083, d=.30; and the increase in partial-normative ideas was not significant. There were nosignificant differences between conditions for each category of ideas. These results provide support for ourhypothesis that critique supports students’ conceptual learning of scientific phenomena by guiding then toconsider a range of alternatives. Results were not influenced by the complexity of the critiqued artifact.Value of Multiple Opportunities to Reconsider AlternativesTo investigate the general impact of the activity sequence on students’ success in critique and revision ofexplanations, we analyzed students’ science content critique and revision during the critique step prior to theguidance checkpoint and during the revision step after guidance checkpoint (Figure 4). Students’ critiques werecoded as a success if they selected the correct science content critique. Revisions were coded as successful ifthey led to a gain in KI scores relative to the initial score.Successful	  Revision	  /	  Correct	  Cri/que	  Critique	  Step	  (pre-­‐guidance	  checkpoint)	  checkpoint)	  Successful	  Revision	  Unsuccessful	  Revision	  Unsuccessful	  Revision	  /	  Correct	  Cri/que	  Revision	  Step	  (post-­‐guidance	  checkpoint)	  0%	  Successful	  Revision	  /	  Incorrect	  Cri/que	  Unsuccessful	  Revision	  /	  Incorrect	  Cri/que	  10%	   20%	   30%	   40%	   50%	   60%	   70%	   80%	   90%	   100%	  Figure 4. Frequency of Successful and Unsuccessful Revision and Critique of Assigned Explanation andStudents’ Explanation by Step across ConditionsOverall, the number of students who successfully revised either the assigned or their own explanation increasedfrom the critique to the revision step. This is an encouraging finding, given that conceptual revisions areespecially difficult for students, even if they receive direct feedback on the written artifact to be revised (Cho &MacArthur, 2010). In this study, students only received conceptual guidance on their critique choice. During thecritique step, only 27% of all students made a successful revision of the sample explanation based on theirchoice. Critique was challenging for students such that 72% of them selected an incorrect critique. However,14% of students who selected an incorrect critique were still able to improve the critiqued explanation.Grappling with critique which involves considering alternative ideas, even when unsuccessful, may still supportstudents in making productive revisions. Following the guidance checkpoint, more students (45%) made asuccessful revision of their own explanation. Although 62% of students still struggled with critique, a greaterpercentage of those students (44%) made a successful revision of their own explanation. These positive shiftsindicate that, at least for some, the guidance checkpoint was a valuable opportunity to reconsider their ideas inlight of alternatives and make further progress after the critique step.Preliminary 2x2 Chi square tests suggest that there may be a significant association between critiquingand revising the target artifact (the assigned explanation) during the critique step (χ2(1)=6.74, p<.05).Essentially, students were 10.35 times more likely to make a successful revision of the critique artifact if theyselected the correct critique during the critique step. However, there was no significant association betweensuccessful critique and successful revision of the students’ own explanation during the revision step (χ2(1)=.00,p>.05). The decoupling of critique and revision following the guidance checkpoint supports the idea thatreceiving conceptual guidance and an opportunity to revisit a key visualization allowed students to makesuccessful revisions of their own explanation despite their struggles with critique.Classroom observations. To further examine how students engaged with the various steps comprisingthe activity sequence, we used classroom observations and video records. In this paper, we characterize studentengagement as the types of discussions students had with each other and their interactions with the activityscaffolds. During critique, we observed that some students seemingly guessed when initially selecting theirscience content critique and did not discuss alternatives until prompted to revise the critiqued explanation, whileothers discussed the critique choices during selection. During the guidance checkpoint, some were frustrated bythe complexity of selecting among plausible alternatives and engaged in guessing behavior, whereas othersleveraged the additional opportunity provided by feedback to reassess their understanding or to request helpfrom the instructor or researcher.To illustrate the kinds of engagement observed in the overall data corpus, we present descriptions andtranscribed excerpts of video records. The video data suggest that the design can provoke opportunities forstudents who may not otherwise engage in negotiation and reconsideration of ideas, as well as for students whoare already doing so. However, how to ensure that such opportunities are leveraged by students remains an openquestion, as we discuss below. In this paper, we focus on the guidance checkpoint, because it was intended toICLS 2014 Proceedings475© ISLSserve as a pivotal opportunity for students to reconsider their own ideas and their assessment of alternative ideasduring critique.Capitalizing on Opportunities: Collaborative Sensemaking and Reflection on IdeasJanelle and Ida took turns controlling the computer and answering prompts. They were jointly engaged with theunit, discussing science content and co-constructing responses to prompts. They also asked each other forconfirmation while commenting on ideas with questions such as “We’re OK, right?” and “How’s that?” beforefinalizing their work. Their engagement pattern persisted throughout the activity sequence, with both partnerscommenting on the critique choices. Their aptitude for collaborative sensemaking and deliberation raises thequestion of whether the activity design adds value to their learning process. The transcript below suggests thattheir existing orientation allowed them to capitalize on opportunities afforded by the activity design and furtherrefine their ideas. Prior to this moment in the guidance checkpoint, they had worked their way through critique,assessing each critique choice with regard to its scientific validity, but without justifying why by referencingrelevant ideas (e.g., “That’s not true.”). They made a successful critique and revision of the critique artifact(Table 5), but during the guidance checkpoint, Janelle argues for a different critique choice (“needs moreevidence in general”).12345678Janelle: I think “adding more evidence in general” because they didn’t really explain where theenergy comes from or what it transforms into. (J chooses the choice and submits; it’swrong)J:Oh.Ida:Wait.J:Sorry.I:Oh wait we have to review it. (I goes back to simulation step and reviews text precedingthe simulation while saying, “Blah, blah, blah.”)J:OK, go back to the [guidance checkpoint] step. (I continues to and starts simulation. I andJ watch it silently for 11 seconds, then I goes back to guidance checkpoint. I readsthrough options and evaluates each with “That’s not true,” etc. with J watching)J:(Sighs) Wait, “Explaining that IR comes from the Sun” (.) But not directly. (.) It doesn’tcome, like, directly though.I:Yeah.In this example, the activity design provides an opportunity for the students to further refine their understandingbecause the unsuccessful attempt during the guidance checkpoint prompted them to revisit and review thesimulation (Line 5) and re-evaluate the choices they had previously evaluated during critique (6-7). Unlikeduring critique when they had evaluated the choices without justification, this time Janelle elaborated why sheagreed or disagreed with the choice (7, also 1 prior to receiving guidance). Similar instances were observedelsewhere in the corpus during the activity sequence where they reassessed the content more carefully after aninitial attempt to select an alternative. Although their revision of the critique artifact indicates complexunderstanding of the target ideas (Table 5), they chose to elaborate on albedo’s role in the process, an untargetedbut relevant idea, when revising their own explanation. Their revision was the only response in the data corpusthat demonstrated increased sophistication but focused on the amount of energy transformation (using albedo)rather than on the kind of energy SR becomes. Their discussions and actions during the activity sequenceprovide evidence for how the activity’s design can create opportunities for students to reconsider their ideas,which in Janelle and Ida’s case enhanced their engagement with the science content and deepened theirunderstanding.Table 5: Critique artifacts, critiques, and revisions by Janelle and IdaCritique ArtifactThe creation of infrared radiation began when the solar radiation comes from theStep(AssignedSun. Some radiation is absorbed or reflected. The ones that were absorbed goesExplanation) through Earth and eventually come back out of the Earth and becomes infraredradiation. (KI Score = 4)CritiqueThe response can be improved by…explaining what kind of energy SR becomes when it is absorbed.(Correct choice for critique)RevisionThe creation of infrared radiation begins when the solar energy radiates from theSun. Some radiation is absorbed and/or reflected. The rays that were absorbedtravel through Earth's lithosphere, transform into thermal energy, and eventuallyexits out of the Earth's surface as infrared radiation. (KI Score = 5)Revision ArtifactWhen solar radiation comes in contact with the Earth's surface, some of it isStep(Originalabsorbed by the surface. When this solar radiation is absorbed by the Earth'sICLS 2014 Proceedings476© ISLSExplanation)CritiqueRevisionsurface, it is heated by conduction. The Earth gives off this heated solar radiationas infrared radiation as heat. (KI Score = 3)The response can be improved by…explaining what kind of energy SR becomes when it is absorbed.(Correct choice for critique)When solar radiation from the sun comes in contact with the Earth's surface, someof it is absorbed by the Earth. The amount of radiation absorbed or reflecteddepends on the amount of albedo, or ability to reflect solar radiation. This meansthat an area with high albedo would reflect more solar radiation and an area withlow albedo would absorb more solar radiation. When solar radiation is absorbedby the Earth's surface, it is heated by conduction. The Earth gives off this heatedsolar radiation as infrared radiation or heat. (KI Score = 3)Missed Opportunities: Turn-Taking and GuessingHailey and Tom took turns controlling the computers and answering prompts. Although they engaged with eachother and the unit, the nature of their collaboration was primarily strategic in that they alternated responsibilityfor answering the prompt discussed, but rarely discussed the science content and their understanding of it. Theyboth remained engaged regardless of whose turn it was, but their peer monitoring rarely ventured beyondlogistics and accountability with comments such as, “It’s your turn,” “I’m not going to tell you anything,” “Youjust have to get it better,” “You can click there,” and so on. Upon encountering an impasse, both partners tendedto ask the other to try the step. They neither asked for help nor discussed alternatives. This activity elicited morefrequent turn-taking comments such as “Here, you try” that were less commonly observed in other activities.Despite their focus on turn taking, there were instances during the critique and guidance checkpoint steps thatprovoked moments of content discussion and negotiation. These opportunities were rarely pursued.The transcript below illustrates one example of a missed opportunity during the guidance checkpoint.Prior to this, they had engaged in frequent turn-taking while attempting revision of the critiqued explanation.However, they did not discuss the critique choices. Tom eventually typed the revision, which consisted ofcapitalizing one word (Table 6); although Hailey watched attentively, they engaged in an off-topic discussion.When Tom continued to the guidance checkpoint and paused, Hailey asked Tom if he needed help for the firsttime, but Tom did not take Hailey up on her offer. After multiple instances of turn-taking and failed attempts topass the checkpoint, they attend to the content of the critique choices for the first time in the activity sequence.12345Tom:OK, remember “explain that IR comes from the Sun.” (T reading previously selectedchoice; T navigates back to revisit the simulation)Hailey: It doesn’t even make sense, though. (T waits for the simulation to load)T:I know. (T begins navigating back to guidance checkpoint without watching simulation)T:This one, right? (T makes a selection)H:I guess? (T scrolls down to hit submit; choice is incorrect)We see this as an important moment because, in a departure from their usual mode of turn-taking collaboration,Tom asked Hailey to attend to the content of the critique choice (line 1), and Hailey commented on the contentto indicate her confusion (2). However, instead of leveraging this opportunity to resolve their dilemma throughdiscussion or by reviewing the simulation, Tom simply agreed with Hailey and navigated back to the checkpoint(3). There, Tom selected another choice without explicating why, but asked Hailey for confirmation (4), whoindicated she was not sure (5), which also diverged from their turn-taking mode. However, Tom proceeded tosubmit his choice without comment. Following this episode, Hailey indicated her frustration and took over thecomputer. Tom then suggested requesting help for the first time during the activity sequence, saying, “Ask [theresearcher], ‘cause that is confusing,” but neither did so. Eventually, they managed to make a correct guesswithout discussing the content and proceeded to the next step.Their written artifacts (Table 6) suggest that the activity design had no impact on Hailey and Tom’sprogression through the unit. However, by examining their video records, we see moments where the design wassuccessful in provoking opportunities for the dyad to engage in collaborative sensemaking, discussion,negotiation, and reconsideration of ideas, because the guidance disrupted their turn-taking approach tocollaboration. Yet, in contrast to Janelle and Ida’s case, they rarely capitalized on those moments, proceedingthrough the activity sequence without discussing or reconsidering their ideas. Further work is necessary to refinethe guidance to address these observed limitations so that more students can be supported in making progress.Table 6. Critique artifacts, critiques, and revisions by Hailey and TomCritique ArtifactSolar radiation was reflected by earth as infrared radiation. (KI Score = 2)Step(AssignedExplanation)ICLS 2014 Proceedings477© ISLSCritiqueRevision StepRevisionArtifact(OriginalExplanation)CritiqueRevisionThe response can be improved by…explaining that IR isn't reflected SR.(Correct choice for critique)Solar Radiation is reflected by Earth as infrared radiation. (KI Score = 2)It came from the sun and space. (KI Score = 2)The response can be improved by…adding more evidence in general.(Incorrect choice for critique)It came from the sun and space. The heat came from the sun witch is in space. Andit will heat the Earths atmosphere and the people. (KI Score = 2)Summary and Design ImplicationsOur findings illustrate the value of encouraging students to consider a range of ideas and to capitalize on critiqueactivities. Considering alternatives during critique led to progress in conceptual understanding of scientificphenomena. Although students were assigned explanations of differing complexity in the two conditions,students in both conditions benefited equally from the critique opportunity and were able to make conceptualimprovements to their initial explanations during revision, suggesting that both conditions introduced desirabledifficulties. The activity sequence designed to provide students with multiple opportunities to consider a rangeof alternatives was equally successful for critique of explanations that were incomplete and those that included anon-normative idea. The slight trend for critique of the non-normative alternative deserves further study with alarger sample. Since the critique artifacts designed for the study cover a relatively narrow range of complexity,more research is also needed to identify the generalizability of these findings to other critique artifacts, differentdomains, and to students with different levels of prior knowledge. Future work will also help clarify the specificvalue of the opportunities to reflect on alternative ideas (such as revising the critique artifact and revisitingcontent based on guidance) in supporting students to make progress.By broadening the alternatives for critique and providing multiple opportunities to reconsider ideas, thecurrent investigation showed benefit of critique activities for enhancing students’ conceptual understanding.These findings resonate with other investigations of critique activities such as providing critique guidelines(Chang & Linn, 2013). The case studies illustrate the limitations of this approach. While some students weresufficiently prompted by the guidance to reassess their understanding, others needed additional support. A nextstep is to refine the guidance design to help students such as Tom and Hailey to seriously consider the ideas oftheir partner.ReferencesBjork, R. A., & Linn, M. C. (2006). The science of learning and the learning of science: introducing desirabledifficulties. American Psychological Society Observer, 19, 29, 39.Chang, H.-Y., & Linn, M. C. (2013). Scaffolding learning from molecular visualizations. Journal of Research inScience Teaching, 50(7), 858-886.Chinn, C. A., & Brewer, W. F. (1993). The role of anomalous data in knowledge acquisition: A theoreticalframework and implications for science instruction. Review of Educational Research, 63, 1-49.	  Clark, D. B. (2006). Longitudinal conceptual change in students’ understanding of thermal equilibrium: Anexamination of the process of conceptual restructuring. Cognition and Instruction, 24(4), 467-563.Kapur, M., & Bielaczyc, K. (2012). Designing for Productive Failure. Journal of the Learning Sciences, 21(1),45–83.Linn, M. C., Davis, E. A., & Bell, P. (Eds.). (2004). Internet Environments for Science Education. Mahwah, NJ:Erlbaum.Linn, M. C. & Eylon, B.-S. (2006). Science Education: Integrating Views of Learning and Instruction. In P. A.Alexander & P. H. Winne (Eds.), Handbook of Educational Psychology, 2nd edition (pp.511-544).Mahwah, NJ: Lawrence Erlbaum Associates.Sato, E., & Linn, M.C. (2011, April). Developing Criteria for Explanations in Science: Scaffolding Peer Critiqueand Feedback in Technology-Enhanced Instruction. Poster presented at the 2011 American EducationalResearch Association Annual Meeting, New Orleans, LA.Shen, J. (2010). Nurturing students’ critical knowledge using technology-enhanced scaffolding strategies inscience education. Journal of Science Education and Technology, 19(1), 1–12.Svihla, V., & Linn, M.C. (2012). Distributing Practice: Challenges and Opportunities for Inquiry Learning. InProceedings of the 10th International Conference of the Learning Sciences (ICLS2012) (Vol. 1, pp.371-378).ICLS 2014 Proceedings478© ISLS