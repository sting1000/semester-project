Evaluating Lesson Design and Implementationwithin the ICAP FrameworkRod D. Roscoe (rod.roscoe@asu.edu) and Pedro J. Gutierrez (pgutier1@asu.edu)Arizona State University, 7271 E. Sonoran Mall, Santa Catalina Hall, Mesa AZ 85212Ruth Wylie (ruth.wylie@asu.edu) and Michelene T. H. Chi (mtchi@asu.edu)Arizona State University, Learning Sciences Institute, PO Box 872111, Tempe AZ 85287Abstract: The ICAP framework provides a theory of cognitive engagement based on overtlearning activities that may inform instructional design. In ongoing work to investigate ICAPas a theoretically-grounded instructional design system, classroom teachers participated in aworkshop to learn about the framework and design lessons at varying levels. In this paper, weconsider how the ICAP framework can be used in the evaluative stages of the instructionaldesign cycle to assess learning tasks and implementation. Using an example lesson from alanguage arts classroom, we describe the evaluation methodology, consider how analysesrevealed potential disconnects between intention and implementation, and discuss how ICAPbased evaluations may advance the design cycle by informing future lesson plans.Instructional Design within the ICAP FrameworkThe ICAP framework (Chi, 2009) was initially proposed as a theory for interpreting learning research findingsby examining students’ overt learning activities. In passive (P) tasks, learners receive information withoutacting with or upon it (e.g., listening to a lecture). Active (A) tasks focus learners’ attention or activate relevantknowledge (e.g., highlighting a text while reading). Constructive (C) tasks require generating ideas that gobeyond the presented information (e.g., self-explaining). Finally, interactive (I) tasks involve dialogues in whichideas are jointly produced by multiple participants (e.g., revising based on peer feedback). Chi (2009) found thatinteractive learning tasks tended to support deeper learning than individual constructive tasks; constructive taskstended to outperform active tasks; and active tasks outperformed passive tasks.ICAP has recently been applied to instructional design. Although the value of constructive andinteractive activities are well known, it is not easy to develop appropriate curricula that foster high levels ofengagement (e.g., Allen & Tanner, 2005; Armbruster & Patel, 2009). By focusing on overt learning behaviors,ICAP offers a tractable way for teachers to design lessons that target desired levels of cognitive engagement. Inthis paper, we consider how ICAP can be applied to the evaluative stages of instructional design to assesswhether lesson designs and implementation successfully matched pedagogical intentions. In the followingsections, we briefly summarize instructional design, describe our evaluation methodology, and discuss a casestudy using this methodology drawn from a language arts lesson on complex sentences.Stages of Instructional DesignInstructional design seeks to create lessons and experiences that build learners’ understanding of the domain.Many design models incorporate planning, conducting, and evaluating processes. For example, ADDIE (e.g.,Chan, 2010) describes a five-stage process of analysis, design, development, implementation, and evaluation. Inanalysis, designers gather data about the domain and learners’ needs. In design and development, designers setlearning objectives and create the instructional activities and learning materials. The implementation stage putsthe lesson plan into action. Teachers deliver lectures, engage students in discussion, assign worksheets or otheractivities. Finally, evaluation examines whether the instruction was delivered as planned and identifies changesneeded to improve the design. A key principle is that lesson evaluations inform the next iteration of design.For successful instruction, the design process must also be paired with a guiding theoretical framework.Such theories provide a perspective for analyzing learner needs and specifying effective learning activities. Forexample, cognitive load theories posit that learners are constrained by cognitive capacity limitations, and thusinstructional materials must be designed to balance sources of productive or harmful load (van Merrienboer,Kirschner, & Kester, 2003). In contrast, Keller’s ARCS model (Keller, 2010) emphasizes motivational factors ofattention, relevance, confidence, and satisfaction. Instructional materials must be designed to grab attention,make the value of the subject matter salient, and support learners’ confidence.ICAP offers a theoretical perspective for instructional design that focuses on overt learning behaviorsfor deeper understanding (Chi, 2009). Learners’ needs are analyzed with respect to desired levels of cognitiveengagement and then overt learning activities can be specified to suit those aims. For example, when recall ofkey terms is the goal, then passive or active tasks may be sufficient. However, if students must develop a deepand flexible understanding, then constructive or interactive tasks may be better. This paper extends work onICAP as an instructional design tool by considering how the framework can be used to evaluate lesson design.ICLS 2014 Proceedings972© ISLSParticipating teachers created and implemented ICAP-based lessons during their school year and latersubmitted their materials and examples of students’ work to the research team. If ICAP is to be viable forinstructional design, then the framework must demonstrate utility for critically reviewing lessonimplementation. The evaluation process must be able to reveal adherence to (or departures from) curriculargoals and inform ways to improve future lesson plans (i.e., complete the design cycle). In this paper, we applythe ICAP framework to consider two key questions: Did teachers’ lessons fulfill their intentions to providepassive, active, constructive, or interactive learning experiences? Did students’ overt activities and responsesmanifest the target levels of cognitive engagement?MethodTeacher Professional DevelopmentTen middle school and high school teachers participated in a workshop introducing ICAP. Teachers representeda diverse selection of domains, including history, language arts, general science, physics, chemistry, and earthscience. Participating teachers reported an average of 9.7 years of teaching experience, including new andveteran teachers. Four teachers had completed or were enrolled in a master’s degree program. The workshopbegan with an introduction to the ICAP framework, including a detailed description of each level, examples ofovert activities within each level, and hypothesized cognitive processes associated with each level. In addition,we also discussed logical and practical issues involved with implementing ICAP lessons into a classroom andgave two full days to develop two lessons based on ICAP. Teacher learning and understanding was assessedthrough pretests, posttests, surveys, and evaluating the lessons they created during the workshop.The ten teachers, with one exception, each created two lessons within their areas (19 lessons total). Forexample, an earth science teacher created and contrasted active and interactive lessons on (a) the science ofdecay and (b) earth systems. Science of Decay discussed principles of decomposition, such as the role of keymicroorganisms (e.g., bacteria and fungi). Earth Systems discussed various spheres (e.g., lithosphere) and theeffects of natural and man-made events (e.g., volcanoes and fossil fuels). Similarly, one language arts teachercreated and contrasted active, constructive, and interactive lessons for (a) analyzing sentence structure and (b)complex sentences. Analyzing Sentence Structure taught students to recognize fluency problems, such as choppysentences and repetitive syntax. Complex Sentences taught students about independent and dependent clausesand how to combine two simple sentences into a single sentence using a subordinating conjunction. Such lessonexamples demonstrate the rich and challenging concepts covered by the participating teachers. For this paper,and to demonstrate the ICAP evaluation methodology, we focus only on the Complex Sentences lesson.Evaluation MethodologyIn the first phase of evaluation, Lesson Design Analysis, we consider how various tasks within a lesson supportthe intended engagement level. This process has three steps:1.Review lesson plans and materials to determine the sequence of tasks that comprise the lesson. Eachtask within a lesson may support a different level of engagement, and thus it is desirable to analyzethem separately. Multiple tasks might be presented in one assignment (e.g., a workbook with wordmatching and diagramming tasks) but the level of analysis should be the constituent tasks.2.Categorize each task based on the level of cognitive engagement required. Each task can be evaluatedfor whether the required overt behaviors are passive, active, constructive, or interactive. For example,matching terms and definitions is “active” whereas drawing a diagram is “constructive.”3.Categorize the overall lesson based on the pattern of constituent tasks. The holistic categorization isbased on the highest level of engagement demonstrated by at least one-third of the tasks. For instance,if a lesson has 10 tasks with 3 active (30%), 5 constructive (50%), and 2 interactive (20%), then thelesson is labeled “constructive” because interactive tasks comprise less than 33% of the lesson.Even well-designed lessons cannot guarantee student compliance. In the second phase, Student Work Analysis,we consider how students’ overt responses exhibit a given engagement level. This process has three steps:1.Obtain and review available records of student work and products. Student work may include notestaken while watching a video, answers to worksheet problems and questions, video recordings ofstudents’ interactions during a lesson, assessment tests, and other materials created by students.2.For each activity or task, develop a coding scheme to assess students’ exhibited level of cognitiveengagement. The coding process should be specific to the assignment and engagement level. Forexample, active tasks might be coded based on completeness and constructive tasks might be codedICLS 2014 Proceedings973© ISLSbased on the number of elements generated. In some cases, the analysis may reveal gaps in the data(e.g., no record of students’ interactions), which may imply ways to improve future lesson activities.3.Implement the coding scheme(s). Code the data and establish expectations for the range of scores thatindicate the target engagement level. For instance, if a worksheet is intended to be “constructive,” whatrange of scores would support the interpretation that students were indeed constructive?Table 1: Complex Sentences lesson task list and ICAP categorization (italics indicate coded actions).Task Order and DescriptionCategoryActive Version (n = 20 students)1Copy definitions of “independent clause” and “dependent clause” in a notebook2Read pre-made flashcards containing example clauses3Mix and match flashcards to create complex sentences4Write the generated sentences in a notebook5Use flashcards to create groupings based on clause types6Write grouped clauses in a notebook7Glue the notes into a notebook to use as examples8Practice combining two short sentences into one complex sentenceActivePassiveConstructiveActiveConstructiveActiveActiveConstructiveConstructive Version (n = 29 students)1Generate definitions of “independent” and “dependent”2Check the definitions using the dictionary3Answer comparison questions about their definitions versus the dictionary4Write the correct definitions in a notebook5Generate definitions of “independent clause” and “dependent clause”6Check the definitions using the dictionary7Answer comparison questions about their definitions versus the dictionary8Glue the notes into a notebook to use as examples9Identify correctly punctuated complex sentences and justify the choice10 Teacher tells the students which answers are correct11 Create a general rule for using commas in complex sentences12 Practice combining two short sentences into one complex sentenceConstructiveActiveConstructiveActiveConstructiveActiveConstructiveActiveConstructivePassiveConstructiveConstructiveInteractive Version (n = 30 students)1Generate definitions of “independent” and “dependent”2With a partner, compare definitions and check them with the dictionary3Generate a final definition based on original, partner, and dictionary versions4Write the correct definitions in a notebook5With a partner, generate definitions of “independent clause” and “dependent clause”6Check the definitions using the dictionary7With a partner, compare definitions and check them with the dictionary8Glue the notes into a notebook to use as examples9With a partner, identify correctly punctuated complex sentences and justify the choice10 Teacher tells the students which answers are correct11 With a partner, create a general rule for using commas in complex sentences12 As a whole class, discuss sentence rules and definitions13 Glue the notes into a notebook to use as examples14 With a partner, practice combining two short sentences into one complex sentenceConstructiveInteractiveConstructiveActiveInteractiveActiveInteractiveActiveInteractivePassiveInteractiveInteractiveActiveInteractiveResultsLesson Design Analysis: Complex SentencesOne teacher designed a lesson to teach 79 6th-grade students about independent clauses, dependent clauses, andsentence combining. The teacher developed three lesson versions to contrast active, constructive, and interactiveinstruction. For each lesson, the teacher’s plans and materials were analyzed to determine the constituent tasks,which were then categorized according to the ICAP level (Table 1). In the Active version, students engaged in 8tasks. Although 5 tasks (62.5%) were categorized as active, 3 tasks (37.5%) offered meaningful opportunitiesfor constructive activity. Thus, the Active version was recategorized as “constructive.” In the ConstructiveICLS 2014 Proceedings974© ISLSversion, students completed 12 tasks: 7 were constructive (58.3%), 4 were active (33.3%), and 1 was passive(8.3%). Thus, the teacher’s original “constructive” label for this lesson was retained. Finally, in the Interactiveversion, students completed 14 tasks: 7 were interactive (50.5%), 2 were constructive (14.3%), 4 were active(28.6%), and 1 was passive (7.1%). Thus, the teacher’s original “interactive” label for this lesson was retained.This evaluation demonstrated two key points regarding lesson design. First, teachers’ intentions may becountered by incorporating tasks of a different level. The teacher’s active lesson contained constructive tasksinvolving generation of examples, resulting in the lesson being relabeled as constructive. Importantly, we arenot arguing that lessons should only contain tasks of one type; learning tasks at different levels each have theirrole to play. Although ICAP does not specify how activities should be sequenced, a student debate (interactive)may be more productive if debaters first articulate their arguments on their own (constructive), which mayrequire reading and note-taking (active) to collect evidence. The core idea, however, is that if the teacher’s goalis to support a particular level, then the types of activities included in the lesson must be carefully chosen.Student Work AnalysisAn analysis of all cases of student products was beyond the scope of this paper. To exemplify a student workanalysis, we consider students’ responses on one worksheet (Figure 1) in which they practiced combiningsentences (i.e., in Table 1, see Active task #8, Constructive task #12, and Interactive task #14).Figure 1. Two example worksheets from the Complex Sentences lesson. The grades provided by the teacher(e.g., “+12/12”) were not included in the ICAP evaluation process.The worksheet contained six pairs of sentences. Students could choose the conjoining word, where toplace the conjoining word (i.e., at the start of the first sentence or between the sentences), and whether tomaintain or swap the ordering of the original sentences. To code for constructive activity, we examined thevariability of students’ strategies. Students earned up to 6 points based on whether they used a single wordinsertion strategy for all pairings (0 points) versus varying the strategies equally (6 points). Likewise, studentsearned up to 6 points based on whether all sentences used one ordering strategy (0 points) or varied the orderingstrategies equally (6 points). Finally, students earned up to 6 points for each unique conjoining word (1 point perword). Higher total scores (i.e., > 12 points) showed more constructive activity by implementing more variedstrategies whereas lower scores (i.e., < 6 points) indicated less constructive activity (Table 2).Not every student completed the assigned tasks (n = 69). Across students with available data, averageconstructive activity for the worksheet was only 6.0 (SD = 3.3). Students showed the most constructive activityin terms of varying conjoining word choice rather than other strategies, but students exhibited minimalconstructive activity, overall. According to the ICAP framework, students in the interactive version should haveoutperformed the others, which appeared to be untrue in this case. However, without data on partners’ICLS 2014 Proceedings975© ISLScontributions or dialogue, it was impossible to diagnose what occurred between partners (e.g., whether theywere co-constructing knowledge or engaging in off-task conversation).The results of this student work analysis demonstrated a potential disconnect between lesson intentionsand actual implementation by students. Although the task offered several opportunities to constructively exploredifferent sentence combining techniques, students used relatively few of them. Instead, students may have beenusing only one or two rules for combining sentences by rote instead of thoughtfully constructing new sentences.Table 2: Mean (and standard deviation) scores for worksheet constructive activity.StrategyTotal ScoreWord InsertionSentence OrderingWord VariabilityActive (n = 19)5.5 (2.5)1.3 (1.5)0.4 (1.1)3.8 (1.2)Constructive (n = 26)6.8 (3.6)2.1 (1.9)1.6 (2.2)3.1 (1.1)Interactive (n = 24)5.5 (3.6)0.8 (1.5)1.1 (1.9)3.7 (1.5)F1.283.932.341.92p.284.024.104.155ConclusionThe methodology presented here suggests that ICAP is useful and viable for instructional design. In this paper,we considered how ICAP principles can be used to assess lesson design and implementation. Such evaluationscan reveal how the individual tasks that comprise a lesson may support or undermine broader curriculum goals.For example, learning tasks might support a lower level of cognitive engagement than intended or students maynot enact the task at the target level. Such failures of instructional design are not new but ICAP provides aframework for analyzing and specifying these issues in a fine-grained manner.Importantly, ICAP also informs the analysis and development of new lessons. When combined withevaluation, ICAP can guide iterative lesson design and improvements. For example, in the Complex Sentenceslesson, the teacher might revise the instructions for the sentence combining task (Figure 1) to explicitly requireconstructive use of diverse strategies. Similarly, the teacher might create more precise sequences of active,constructive, or interactive tasks that build from lower to higher levels of cognitive engagement. Although ICAPdoes not specify an ideal ordering of tasks or engagement levels, researchers could use this methodology to testhypotheses about instructional sequences. That is, one might contrast the efficacy of lessons that progress fromlow-to-high cognitive engagement, high-to-low cognitive engagement, or stagger the engagement levels.Any evaluation method is limited by the available data. For instance, no trace data were collectedregarding students’ dialogue in the interactive version of the Complex Sentences lesson. Thus, it was impossibleto evaluate whether students co-constructed ideas while combining sentences. For researchers and educatorsseeking to use ICAP for instructional design, care must be taken to create and collect diagnostic examples ofstudent work. If students are supposed to be constructive, do their materials allow them to record their generatedresponses? If students are supposed to be interactive, do their materials allow them to record their individual ormutual contributions to the final products? Addressing such questions further supports iterative instructionalgoals (e.g., asking partners to record their contributions may encourage interaction) and evaluation goals.ReferencesAllen, D., & Tanner, K. (2005). Infusing active learning into the large-enrollment biology class: Sevenstrategies, from the simple to complex. Cell Biology Education, 4, 262-268.Armbruster, P., Patel, M., Johnson, E., & Weiss, M. (2009). Active learning and student-centered pedagogyimprove student attitudes and performance in introductory biology. CBE-Life Sciences Education, 8,203-213.Chan, J. F. (2010). Designing and developing training programs: Pfeiffer essential guides to training basics.Wiley and Sons: San Francisco, CA.Chi, M. T. H. (2009). Active-Constructive-Interactive: A conceptual framework for differentiating learningactivities. Topics in Cognitive Science, 1, 73-105.Keller, J. M. (2010). Motivational design for learning and performance: The ARCS Model approach. NewYork: Springer.Merrienboer, J. J. G., Kirschner, P. A., & Kester, L. (2003). Taking the load off of a learner’s mind:Instructional design for complex learning. Educational Psychologist, 38, 5-13.AcknowledgmentsThe research reported here was supported by the Institute of Education Sciences, U.S. Department of Education,through Grant 305A110090 to Arizona State University. The opinions expressed are those of the authors and donot represent views of the Institute or the U.S. Department of Education. The authors would like to thank WanTing Huang, Seokmin Kang, Matthew Lancaster, Derek Stark, and David Yaghmourian for their assistance.ICLS 2014 Proceedings976© ISLS