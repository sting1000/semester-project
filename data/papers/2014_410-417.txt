Showing What They Know:Multimedia Artifacts to Assess Learner UnderstandingCindy E. Hmelo-Silver, Indiana University, 1900 East 10th St, Bloomington, IN, 47406 USAchmelosi@indiana.eduCarolyn A. Maher, Marjory F. Palius, Robert Sigley, Alice Alston, Rutgers University, 10 Seminary Pl.,New Brunswick, NJ 08901, USAcarolyn.maher@gse.rutgers.edu, marjory.palius@gse.rutgers.edu, robert.sigley@gse.rutgers.edu,alston@rci.rutgers.eduAbstract: Engaging learners in constructing multimedia artifacts provides rich opportunitiesfor them to make their thinking visible. In this research, we demonstrate the use of theVMCAnalytic, a multimedia artifact that builds on an extensive video collection of children’smathematical reasoning. Using reliable rubrics, we coded all VMCAnalytics created in arange of classes. These rubrics focused on the quality of the students’ arguments and depth oftheir reasoning. Analysis showed that the rubric was useful in differentiating among thedifferent groups of students. Moreover, different metrics had different degrees of correlation,suggesting that we were identifying several different dimensions of quality.Engaging students in technology-rich projects provides opportunities for both learning and making theirthinking visible (Collins & Halverson, 2009). Creating multimedia artifacts offers opportunities for learners toengage with substantive content through their designs (Kafa & Ching, 2001). Building on earlier research with avideo repository, we provide opportunities for students to engage in generative activity through construction ofmultimedia artifacts by making use of a new tool, the VMCAnalytic (Agnew, Mills, & Maher, 2010). In priorwork, we examined a range of course contexts and tasks in which learners used the VMCAnalytic (HmeloSilver et al., 2013). In current research, we extend the range of contexts in which learners’ use of the tool toconstruct multimedia artifacts enables assessment of the complex knowledge required for understanding,teaching, and researching the development of mathematical reasoning in students across several contentdomains. In particular, we examine how graduate students create arguments using videos of student reasoningby bringing together ideas from mathematics education and the learning sciences with the perceptual groundingof classroom practice to warrant claims about learning. Our research questions for this paper are as follows:(1) To what extent can we use a cyber-enabled multimedia construction tool to assess how welllearners justify their arguments about children’s reasoning?(2) To what extent do students identify relevant concepts in making their claims?(3) How, if at all, does variation in course context, with differing instructional guidelines forcompleting a task, relate to qualitative differences in the multimedia artifacts that studentsproduce?We conjecture that studying these questions will guide the further development of formative and summativeassessments. Conducting such investigations is of particular interest to the field of the learning sciences, wherepractitioners often fill the dual roles of designing activities for student learning and assessing the effects of thoseactivities in order to learn what works to enhance student learning (Schwartz & Hartman, 2007).The complex knowledge that we expect learners to construct through working with video, text, andpractical experiences, entails multiple classes of learning outcomes. Included in their learning space is seeing,saying, and engaging ideas as well as targeting discernment, explanations, and contextualization (Schwartz &Hartman, 2007). Construction of multimedia artifacts with the VMCAnalytic tool supports student assessmentby allowing us to identify the strength of arguments posed and the quality of their reasoning about mathematicseducation and the learning sciences.Our research investigates the affordances and constraints of using multimedia artifact construction as ameans of assessing the knowledge that learners have constructed about the development of mathematicalreasoning and what they view as implications for teaching, learning, and/or further research. First we discuss thetheories of learning that framed the earlier research yielding the video collection on children’s mathematicalreasoning and the recent research on teachers studying those videos to attend to students’ reasoning. This isfundamental and relevant to the theory of learning through artifact design as a context for the current research.We then describe the resources on the video repository to illustrate what learners have available for constructingtheir multimedia artifacts, and share some results demonstrating the promise of the videos for learning aboutstudents’ mathematical reasoning. This serves as a basis for exploring a technology-enhanced assessment forlearners to show us what they know.ICLS 2014 Proceedings410© ISLSTheoretical PerspectivesTeachers seeking to promote students’ competency to represent, communicate, and justify their ideas in thecontext of doing mathematics are faced with the challenge of developing their own adaptive expertise aseducators (Bransford, Derry, Berliner, Hamerness, & Darling-Hammond, 2005). High quality teaching demandsthe ability to spontaneously and flexibly identify, critically evaluate, and respond in appropriate ways toinstances of children’s learning. In mathematics it is particularly important to attend to emergent forms ofreasoning as children express justifications using their own language (Hiebert et al., 1997; Yackel & Hanna,2003). To build such capacity, teachers must know how to solve math problems, but must also come torecognize and understand the reasoning that justifies valid solutions to those problems (Maher, Landis & Palius,2010). Consistent with the view of active knowledge construction, teachers need opportunities to engage aslearners in building knowledge for teaching mathematics. There are several models for mathematics teachereducation and professional development that use video as a tool to make instructional practices available forstudy, interpretation and discussion (e.g., Borko, Jacobs, Eiteljborg, & Pittman, 2008; Zhang, Lundeberg, &Eberhardt, 2011). The VMC video collection offers particularly valuable resources for teachers to buildunderstanding of how students learn mathematics and conditions that promote development of mathematicalreasoning. Because teachers typically have learned math in ways that relied heavily on procedural knowledge,facilitation of their learning entails engaging them in problem-solving tasks and justifying their solutions tobuild a deeper conceptual understanding of the mathematics and learning to attend to ways that students engagein those activities by studying video episodes.Our goal is to advance teacher learning to the next level by engaging them in the construction ofmultimedia artifacts for sharing what they have come to understand about the development of mathematicalreasoning or about learning more broadly. We define artifacts as “digital representations created by students thatcommunicate their understanding, application, analysis, or evaluation of relevant ideas.” (Rodriguez, Frey,Dawson, Liu, & Ritzhaupt, 2012, p. 358). While teachers’ prior learning activities involved a video playbacktool, their work with VMCAnalytic utilizes a video-editing tool. As with other video editing tools, such asWebDiver (Zahn et al., 2010), the VMCAnalytic enables selection of segments of video that can be annotatedand remixed to form multimedia narratives of reflection and analysis for a variety of purposes (Hmelo-Silver etal., 2013). VMCAnalytics are shared, as they become objects of discussion, whether created individually orcollaboratively. The process of constructing the analytics allows meaning to be negotiated and potentiallyrefined for greater clarity and coherence.Using a Video RepositorySupport from four National Science Foundation grants for longitudinal and cross-sectional research studiesproduced over 4500 hours of video and related data showing students doing mathematics from elementarygrades throughout high school and beyond. The Video Mosaic Collaborative (VMC, see:www.videomosaic.org) was built as a repository that houses a unique video collection, amassed from a quarterof a century of research (Maher, 2008; Agnew et al., 2010). An important early finding from this research is thatyoung children, in justifying their solutions to problem tasks, provide arguments that take the form ofmathematical proof (e.g., analogy, cases, contradiction, induction, upper/lower bound, Maher & Davis, 1995;Maher & Martino, 1996,). From our collection, one can study videos that show the evolution of the developmentof students’ arguments over several years and follow how students’ ideas are originally represented, shared, thenexpanded, and then generalized. The videos give examples of children’s early strategies and heuristics and thedurability of their early ideas, later expressed in more elegant form. One can follow the elaboration of studentreasoning over several years and observe the richness and depth of understanding that has evolved over time asstudents make connections between and among ideas and express their solutions using formal notation andlanguage to give meaning to the symbols (Maher, Powell, & Uptegrove, 2010). Because the research wasconducted in working class, urban and suburban environments and in classrooms as well as informal, afterschool settings, the collection is rich, not only from its longitudinal nature (some students are followedthroughout schooling and beyond) but also because of the variety of contexts and content strands in which theresearch was conducted. As part of ongoing work, we continue to populate the VMC with series of short clipsand full-length videos of problem-solving sessions from the collection by cataloging them with extensivemetadata to enable a variety of search paths for discovering resources.As videos get ingested to the VMC, they become publicly available resources for use in teaching,teacher education, and research (Hmelo-Silver et al., 2013). They also become available for use in theworkspace of the VMCAnalytic tool as Figure 1 shows (Agnew et al., 2010). Following initial piloting anddesign revisions, we engaged learners in using the VMCAnalytic tool to construct multimedia artifacts for arange of purposes in a variety of graduate course and research contexts. Experiences with the VMCAnalytic toolhave been preceded by participation in an instructional intervention during which videos from the collectionwere studied.ICLS 2014 Proceedings411© ISLSWe have engaged in a program of design-based research to use the VMC resources for teachereducation studies (Bielaczyc, in press). In particular, for pre and in-service teacher interventions, we haveexamined the effect of studying videos of student reasoning on teachers’ growth in recognizing forms ofreasoning used by children in the videos. Interventions were conducted in which study participants were askedto describe the details of children’s arguments offered in justifying solutions to a specific problem-solving tasks(Palius & Maher, 2013; (Maher, Palius, Maher, Hmelo-Silver, & Sigley, 2014). In each strand, the task elicitedmultiple forms of reasoning that participants could recognize.In the counting strand, a study based on our initial interventions investigated whether teacher study ofVMC videos improved their ability to recognize a variety of forms of reasoning expressed by the children in theassessment video (Maher, Palius, Maher, Hmelo-Silver, & Sigley, 2014). Video assessment data were analyzedto measure growth from pre to post assessment for recognition of forms of reasoning. Results indicated that, onaverage, 60% of the in-service teachers and 36% of the experimental pre-service teachers improved on the postassessment in recognizing the various forms of children’s arguments, compared to the average of 5%improvement for the comparison group of pre-service teachers.Figure 1. Screenshot of the VMCAnalytic toolA study conducted in the fractions strand investigated teacher learning in an experimental online courseusing discourse analysis as well as analysis of their video-based assessment data (Palius, 2013). Teacherlearners in the experimental intervention were more likely to demonstrate growth in their ability to recognizedifferent forms of students’ mathematical reasoning compared with comparison groups (Palius & Maher, 2013).Although results from these studies using video-based assessment data are not conclusive, they offer evidenceof promise of the VMC videos as resources for learning about students’ mathematical reasoning. These findingshave led to shifting our focus to the prospects of the VMCAnalytic tool for assessment.Technology-Enhanced AssessmentLearning technologies are providing new opportunities to teach thinking and reasoning and what students can beexpected to do to show their knowledge and skills (Pellegrino & Quellmalz, 2010). These changes allow us tothink about what is assessed and new ways to provide evidence of understanding. In particular, technologyallows assessing a range of complex performances (Pellegrino, 2013) An important source of evidence forassessment can be found in the artifacts that learners create with technology (de Jong, Wilhelm, & Anjewierden,2012). In particular, technology provides a high level of expressiveness as learners can create multimediaartifacts. There is also evidence that assessment based on teacher’s analysis of video can predict studentlearning outcomes. For example, Kersting, and colleagues (Kersting, Givvin, Sotelo, & Stigler, 2010) used avideo analysis task to assess teacher knowledge. To score the teacher’s analyses, they used a rubric thatmeasured Mathematical Content, Student Thinking, Suggestions for Improvement, and Depth of Interpretationon a 3-point scale. They found that suggestions for improvement predicted student learning. Unlike theKersting et al. study, in our work, the VMCAnalytic is an embedded assessment that is both designed to supportlearning and to provide an occasion for assessment.ICLS 2014 Proceedings412© ISLSThe VMCAnalytic provides opportunity for students to make their thinking visible and provides uniqueopportunities for assessment because students must bring together conceptual knowledge and the rich videos oflearning in action (Derry, Hmelo-Silver, Nagarajan, Chernobilsky, & Beitzel, 2006). As Pellegrino & Quellmalz(2010) note, technology offers opportunities for innovative assessment of complex skills along while allowingscaffolding that promotes learning. Although the VMCAnalytic provides opportunities for eliciting complexperformance, we consider this to be part of an instructional system that includes teacher scaffolding and rubricsthat make expectations clear for both learners and instructors. The VMCAnalytic thus provides a means forinstructors to monitor what students are learning and help scaffold their progress as a formative assessment andcan provide evidence of a student’s (developing) competence. Although space precludes providing examples ofVMCAnalytics here, we refer the reader to examples of published VMCAnalytics: http://bit.ly/1hZcjoR.MethodsThe data analyzed in this paper come from 63 VMCAnalytics that were created over the last two years byparticipants in 7 courses. The VMCAnalytics were graded on an integer scale from 0 to 3 on two levels; a localindividual event level and a global level evaluating the VMCAnalytic as a whole. Each event that contributed tothe participants VMCAnalytic was rated on how well the event fit into the overall description. Examples of highscoring events included text in the description that explained how the video they chose lent support to theiroverall description. Lower scoring events tended to select video but not situate it or made faulty inferences fromthe video. The scoring rubric is shown in Table 1. Two independent coders with expertise in evaluating andcreating VMCAnalytics scored the 63 VMCAnalytics. Inter-rater reliability between the two coders was88.72%.Table 1. Scoring rubricCriteria0123Overall description of theanalytic is very explicitabout what it showsOff topicOn topic, but vagueabout what analyticwill showDiscusses topicexplicitly, but is onlyrelated peripherallyto topic and does notcapture its essenceCaptures essenceEach event contributesmeaningfully to the overallpurpose of the analyticMost events areextraneous orweakly linked topurposeClips connect to each other No easily discerniblein a meaningful waylogical sequenceMost eventscontribute well,some extraneousand have noconnectionSome events are insequence, most arenotAll events contributestrongly to thepurposeMost events areconnected, someseem unconnectedAll events are inlogical sequenceMost clams backedby evidence, a feware notAll claims made indescriptions backed byvideo evidence orresearch literatureClaims are backed withevidenceNo claims made arebacked by evidenceOverall clarity of analyticDescriptions are allhard to understand/unclearSome descriptionseasy to understand,most are difficultMost descriptionseasy to understand,some difficult, oroverall descriptionunclearEasy to understand theintent of eachdescription as well asthe intent of the overalldescriptionOverall coherenceHard to understandwhy any of theevents were includedor how theycontribute to purposeHard to understandwhy most of theevents are includedbut some are easyEasy to understandwhy most of theevents are includedbut some are hardEasy to understandwhy each eventincluded. Overalldescription describespurpose well.Mathematical/ LearningSciences depthDoes not addresslearningSuperficial use ofterminologyMid-levelBuilds on specificlearning theoryFit of TitleOff topicRelated to topicperipherally, doesnot capture itsessenceCaptures its essenceICLS 2014 ProceedingsSome claimsbacked by evidencebut most are notMost eventscontribute well,some are weaklylinkedVague, cannotpredict contentbased on title413© ISLSBecause the score a participant received may have been related to the context of the assignment, belowwe describe the classes on which the data were collected. VMCAnalytics were collected from three semesters ofIntroduction to Mathematics Education, which is a required class for students to obtain an M.Ed, Ed.D., orPh.D. degree with a specialization in mathematics education. Several participants were in other degree programstaking the class as an elective. The course used a hybrid format with a mix of in-person meetings and onlineasynchronous discussions. During in-person sessions, participants worked in groups on problem-solving tasksand shared solutions. For homework, they watched videos on the VMC of students working on the same orsimilar tasks, read related articles from research literature, and engaged in small-group discussions onlineThe means by which the VMCAnalytic project in Introduction to Mathematics Education waspresented to the students varied for each semester. The participants in the first iteration of the course (n=11)worked initially with a subset of VMC videos about the Guess My Towers task (see videomosaic.org for allvideo examples). In this task, students must determine and compare various probabilities of different eventsoccurring when building towers four-tall using Unifix cubes while selecting from two colors. The VMC has aseries of five videos with fifth graders working on the task over an eighty-minute session. The participants inthis iteration of the class worked in pairs creating VMCAnalytics around this task with the constraint that theyhave a minimum of three events and a maximum of six. The run time of their VMCAnalytic was also to bebetween four and ten minutes. In the second iteration of the class (n=19), the participants were free to create aVMCAnalytic about any topic. Individuals created their own VMCAnalytic, but groups were formed alongcommon themes. Several participants were interested in constructing VMCAnalytics for professionaldevelopment around building fraction concepts and met several times throughout the semester for sharing theirVMCAnalytics with their group and a course instructor. The group discussions were centered on helping eachother improve their individual VMCAnalytics. Similar groups were also formed for teaching algebra conceptsand constructing examples with events that demonstrate how to implement the Common Core Standards forMathematics. In the third iteration during the spring of 2013 (n=11), students were free to use any videos theywanted but were constrained to a ten-minute run time on their VMCAnalytic.The Critical Thinking and Reasoning course (n=8) is an online, mathematics education elective forgraduate students. In this class students watched videos of a group of fourth graders over several months as theyexplored fraction ideas before they were introduced formally (Palius & Maher, 2013) The participants drewfrom these videos to construct their VMCAnalytics.The Early Algebraic Learning course (n=7) is a graduate mathematics education elective. Videos ofstudents engaging in the Guess My Rule task were used extensively in the course. Many participants drew onthose and related videos to construct their VMCAnalytics. As in the spring sections of Introduction toMathematics Education, the participants in this course had a time limit of ten minutes imposed for theirVMCAnalytics.The Design-based Research (DBR) course (n=7 across two semesters) used the VMCAnalytic as partof a brief exercise in video analysis. This course consisted of doctoral students in a range of disciplines andwho were focused on research methods rather than mathematics education. We expected these students to focuson more general aspects of learning and collaborative knowledge construction. This group of students wasexpected to provide a contrast with the other classes. In the 2011 class, limited directions were provided. Thiswas addressed in the subsequent iteration when the assignment was more structured and some students workedin a group. Students were pointed to a limited set of videos rather than the whole repository and were givengreater directions as to number of events, length, and the need to make connections to learning theories.ResultsTable 2 contains mean scores and standard deviations for each metric across the different classes. Pairwisedifferences were computed using the Wilcoxon signed-rank test and found significant differences (all p < 0.05)between DBR Fall 2011 and all the other classes except Critical Thinking and Reasoning across overalldescription, the clips connecting meaningfully, claims are backed, overall clarity and coherence, and eventrelevance, suggesting that the students with the least preparation and direction had the lowest scores. Significantdifferences were also found in the relevance of the events between Introduction to Mathematics Education inSpring 2012, where a smaller subset of videos were used, compared with all of the other classes exceptIntroduction to Math Education Fall 2012. This suggests that using the smaller number of videos wasbeneficial. We conjecture that this is because it reduced the amount of search in which the course participantsneeded to engage. Moreover, the Introduction to Math Education Fall 2012 class, which was involved indeveloping the rubrics, did better than all the other classes besides Spring 2012 on events connect meaningfully,claims backed with evidence, and overall clarity/coherence. This suggests that the use of the rubric helpedfocus student efforts. Although the numbers are too small for a statistical comparison, inspection of the scoresfor DBR 2012 compared with the 2011 course suggests that the increased structuring of the assignment led tohigher quality artifacts being produced.ICLS 2014 Proceedings414© ISLSTable 2. Mean Ratings of VMCAnalytics across classes (standard deviations in parentheses)ClassEarlyAlgebraicLearningDesignBasedResearchFall 2011DesignBasedResearchFall 2012Reasoningand CriticalThinkingIntroductiontoMathematicsEducationSpring 2012IntroductiontoMathematicsEducationFall 2012IntroductiontoMathematicsEducationSpring 2013nOveralldescriptionEventsconnectmeaningfullyClaimsbackedw/evidenceOverallclarity andcoherenceMathdepthLearningSciencedepth# ofeventsEventrelevanceaverage72.57(0.53)2.29(0.48)2.14(0.69)2.14(0.38)1.86(0.69)1.71(0.76)6.86(2.03)2.06(0.50)31.00(0)1.00(0)1.00(0)1.99(0)1.00(0)1.00(0)3.00(3.00)0.78(0.69)42.00(0.82)2.00(0.82)2.00(1.15)1.75(0.96)1.75(0.5)2.25(0.50)7.00(2.45)1.94(0.92)82.00(0.76)2.06(0.68)2.13(0.64)2.13(0.64)2.00(0.76)2.00(0.53)9.38(5.78)2.27(0.55)112.55(0.82)2.64(0.67)2.46(0.69)2.73(0.61)2.19(0.61)1.64(0.50)4.09(0.70)2.71(0.86)192.52(0.61)2.42(0.61)2.47(0.70)2.42(0.61)2.21(0.71)1.95(0.78)7.26(2.58)2.35(0.53)112.46(0.52)2.00(0.89)2.00(0.89)1.91(0.83)2.37(0.81)1.27(0.65)8.27(2.69)2.12(0.63)Table 3. Correlation between measuresOveralldescriptionThe eventsconnectmeaningfullyClaims arebacked withevidenceOverall clarityand coherenceMathematicaldepthLearningSciencesdepthNumber ofeventsEventrelevanceaverageOveralldescriptionEventsconnectmeaningfullyClaimsbackedw/evidenceOverallclarity/coherenceMathdepthLearningSciencesdepthNumberof eventsEventrelevanceaverage10.600.720.660.470.380.130.670.6010.770.910.560.34-0.090.860.720.7710.80.480.470.010.750.660.910.810.550.33-0.080.870.470.560.480.5510.0920.250.610.380.340.470.330.0921-0.050.300.13-0.090.01-0.080.25-0.051-0.050.670.860.750.870.610.30-0.051ICLS 2014 Proceedings415© ISLSCorrelations were calculated across rating metrics (Table 3). High correlations were found between theclips connecting meaningfully and overall clarity and coherence (r=0.91). This suggests that students who makemeaningful connection across events are also more likely to have an overall coherent VMCAnalytic. Similarly,the correlations between clips connecting meaningfully and event relevance (r=0.86), and overall clarity andevent relevance (r=0.87) suggests that selecting relevant events is another important factor in the overall clarityof VMCAnalytics. There is a low correlation between the number of events and all of the other metricssuggesting that they are measuring different aspects of learner performance.DiscussionThe VMCAnalytic shows promise of being a useful tool in a system of formative and summative assessment.Constructing multimedia artifacts was an integral component of the instructional design for each of the coursesin which they were used, making learner thinking visible and open for discussion and revision. However,equally important is what this research reveals about the kinds of structures and scaffolds that the use of theVMCAnalytic can provide. The rubric provides clear expectations and a roadmap for student use in creating thismultimedia artifact as our results from Introduction to Mathematics Education Fall 2012 suggest. Moreover,structuring the task by reducing the amount of video that students need to search also appears to be beneficial asdemonstrated by both the Introduction to Mathematics Education Context and Design-Based Research results.For instructors, students’ evolving understanding becomes transparent and provides new insights into astudent’s intellectual journey in thinking critically about children’s mathematical thinking and reasoning.We are now studying the advantages of the opportunities that this assessment provides. As a follow upto this research, a summer research practicum course served as a context for further refinement of themultimedia artifacts made by a subset of participants from the previous semester’s Introduction to MathematicsEducation and Early Algebraic Reasoning courses. One of the researchers who scored the VMCAnalytics metwith graduate students to discuss how their VMCAnalytics were scored using the rubric, and a Senior memberof the research team worked with those students to refine their analytics by working on them as a collaborativegroup focusing on one artifact at a time. An online forum supported the summer practicum as students gaveeach other feedback based on rubric criteria. These learners shared their work with other members of thepracticum community (who worked on different projects) midway and at the end of the term. The next phase ofour research will entail detailed analysis of the revision process and how it might have been scaffolded by therubric as well as current tools and forthcoming technology affordances being designed to support such work.In our ongoing research, we are collecting additional data to provide process feedback to instructors onhow the students are using the VMC as they create VMCAnalytics. Such information offers promise to provideautomated analyses to instructors that would support targeted facilitation of student learning. These analyseswill draw from log data of student search, iterative refinement of VMCAnalytics, and other forms of learninganalytics. In addition, collaboration tools such as threaded discussions and blogs will add opportunities for peerassessment and student reflection.As we have shown here, the VMCAnalytic is part of a system of learning and assessment. It affordsopportunities for learners to make their thinking visible and available for discussion, refinement, assessment,and revision (Collins, 2006). The rubrics can be helpful for guiding students as to what the expectations as wellas for researchers in their evaluations. These rubrics help provide constraints on the task that channel thelearners in productive ways (Reiser, 2004). Our results demonstrate that providing guidelines for the assignmentto create VMCAnalytics are important. As we continue developing and refining the tool as well its use inassessment, we are enthusiastic about the promise of the VMCAnalytic tool.ReferencesAgnew, G., Mills, C. M., & Maher, C. A. (2010). VMCAnalytic: Developing a collaborative video analysis toolsfor education faculty and practicing educators. Paper presented at the 43rd Annual HawaiiInternational Conference on System Sciences, Kauai, HI.Bielaczyc, K. (in press). Informing design research: Learning from teacher's designs of social infrastructures.Journal of the Learning Sciences. doi: 10.1080/10508406.2012.691925Borko, H., Jacobs, J., Eiteljorg, E., & Pittman, M. (2008). Video as a tool for fostering productive discussions inmathematics professional development. Teaching and Teacher Education., 24, 417-436.Bransford, J. D., Derry, S. J., Berliner, D., Hamerness, K., & Darling-Hammond, L. (2005). Theories of learningand their roles in teaching. In L. Darling-Hammond & J. D. Bransford (Eds.), Preparing teachers for achanging world (pp. 40-87). San Francisco: Jossey-Bass.Collins, A. (2006). Cognitive Apprenticeship. In R. K. Sawyer (Ed.), Cambridge Handbook of the LearningSciences (pp. 47-60). New York: Cambridge University Press.Collins, A., & Halverson, R. (2009). Rethinking Education in the Age of Technology: The Digital Revolutionand Schooling in America. New York: Teachers College Press.ICLS 2014 Proceedings416© ISLSde Jong, T., Wilhelm, P., & Anjewierden, A. (2012). Inquiry and assessment; future developments from atechnological perspective. In M. Mayrath, D. Robinson & J. Clarke-Midura (Eds.), Technology-basedassessments for 21st century skills: Theoretical and practical implications from modern researc (pp.249-265). Charlotte, NC: Information Age Publishing.Derry, S. J., Hmelo-Silver, C. E., Nagarajan, A., Chernobilsky, E., & Beitzel, B. (2006). Cognitive transferrevisited: Can we exploit new media to solve old problems on a large scale? Journal of EducationalComputing Research, 35, 145-162.Hmelo-Silver, C. E., Maher, C. A., Palius, M. F., Sigley, R., Alston, A., Agnew, G., & Mills, C. (2013).Building multimedia artifacts using a cyber-enabled video repository: The VMCAnalytic. 46th HawaiiInternational Conference on System Sciences (pp. pp. 3078-3087). Hawaii: IEEE.Kafai, Y. B., & Ching, C. C. (2001). Affordances of Collaborative Software Design Planning for ElementaryStudents' Science Talk. Journal of the Learning Sciences, 10, 323-363.Kersting, N. B., Givvin, K. B., Sotelo, F. L., & Stigler, J. W. (2010). Teachers analysis of classroom videopredict student learning of mathematics: Further explorations of a novel measure of teacher knowledge.Journal of Teacher Education, 61, 172-181.Maher, C. A., Powell, A. B. & Uptegrove, E. (2010). Combinatorics and reasoning: Representing, justifying andbuilding isomorphisms. New York: Springer Publishers.Maher, C. A. (2008). Video recordings as pedagogical tools in mathematics teacher education. In D. Tirosh andT. Wood (Eds.), International Handbook of Mathematics Teacher Education: Vol. 2: Tools andProcesses in Mathematics Teacher Education (pp. 65-83). Rotterdam, The Netherlands: SensePublishers.Maher, C. A., & Davis, R. B. (1995). Children's explorations leading to proof. In C. Hoyles & L. Healy (Eds.),Justifying and proving in school mathematics (pp. 87-105). London: Mathematical Sciences Group,Institute of Education, University of London.Maher, C. A., & Martino, A. M. (1996). The development of the idea of mathematical proof: A 5-year casestudy. Journal for Research in Mathematics Education, 27 194-214.Maher, C. A., Palius, M. F., Maher, J. A., Hmelo-Silver, C. E., & Sigley, R. (2014). Teachers can learn to attendto students’ reasoning using videos as a tool. Issues in Teacher Education 23 (1), 31-47.Maher, C. A., Powell, A. B., & Uptegrove. (2010). Combinatorics and reasoning: Representing, justifying andbuilding isomorphisms. New York: Springer Publishers.Palius, M. F. (2013). Deepening teachers’ awareness of students’ mathematical reasoning through video studyin an online course. Rutgers University. Unpublished doctoral dissertation, Rutgers University.Palius, M. F., & Maher, C. A. (2013). Teachers learning about student reasoning through video study.Mediterranean Journal of Research in Mathematics Education, 12, 39-55.Pellegrino, J. W. (2013). Measuring what matters: Technology and the design of assessments that supportlearning. In R. Luckin, S. Puntambekar, P. Goodyear, B. Grabowski, J. Underwood & N. Winters(Eds.), Handbook of Design in Educational Technology (pp. 377-387). New York: Routledge.Pellegrino, J. W., & Quellmalz, E. S. (2010). Perspectives on the integration of technology and assessment.Journal of Research on Technology in Education, 43, 119-134.Reiser, B. J. (2004). Scaffolding complex learning: The mechanisms of structuring and problematizing studentwork. Journal of the Learning Sciences, 13, 273-304.Rodriguez, P. M., Frey, C., Dawson, K., Liu, F., & Ritzhaupt, A. (2012). Examing student digital artfacts duringa year-long technology integration initiative. Computers in the Schools, 29, 355-374.Schwartz, D. L., & Hartman, K. (2007). It's not television anymore: Designing digital video for learning andassessment. In R. Goldman, R. D. Pea, B. J. S. Barron & S. J. Derry (Eds.), Video research in thelearning sciences (pp. 335-348). Mahwah NJ: Erlbaum.Yackel, E. & Hanna, G. (2003). Reasoning and proof. In J. Kilpatrick, G. W. Martin, and D. Schifter, (Eds.), AResearch Companion to Principles and Standards for School Mathematics (pp. 227-236). Reston, VA:National Council of Teachers of Mathematics.Zahn, C., Pea, R., Hesse, F. W., & Rosen, J. (2010). Comparing simple and advanced video tools as supports forcomplex collaborative design tasks. Journal of the Learning Sciences, 19, 403-440.Zhang, M., Lundeberg, M., & Eberhardt, J. (2011). Strategic facilitation of problem-based discussion for teacherprofessional development. Journal of the Learning Sciences, 20, 342-394. doi:10.1080/10508406.2011.553258AcknowledgmentsThe Video Mosaic Collaborative is a research and development project sponsored by the National ScienceFoundation grants DRL 0822204 and 1217087). We gratefully acknowledge the NSF and note that the viewsexpressed here are those of the authors are not necessarily those of the NSF.ICLS 2014 Proceedings417© ISLS