Analytics for Learning and Becoming in PracticeGolnaz Arastoopour, University of Wisconsin–Madison, arastoopour@wisc.eduSimon Buckingham Shum, The Open University, s.buckingham.shum@gmail.comWesley Collier, University of Wisconsin–Madison, collier.wesley@gmail.comPaul A. Kirschner, Open University of the Netherlands, paul.kirschner@ou.nlSimon Knight, The Open University, sjgknight@gmail.comDavid Williamson Shaffer, University of Wisconsin–Madison, dws@education.wisc.eduAlyssa Friend Wise, Simon Fraser University, alyssa_wise@sfu.caAbstract: Learning Analytics sits at the intersection of the learning sciences andcomputational data capture and analysis. Analytics should be grounded in the existingliterature with a view to data ‘geology’ or ‘archeology’ over ‘mining’. This workshop exploreshow analytics may extend the common notion of activity trace data from learning processes toencompass learning practices, with a working distinction for discussion as (1) process: aseries of related actions engaged in as part of learning activities; and (2) practice: a repertoireof processes organised around particular foci recognised within a social group. The workshopintersperses attendee presentations and demonstrations with relevant theme-based discussions.OrganizersGolnaz Arastoopour is a graduate student in Learning Sciences in the Epistemic Games research group at theUniversity of Wisconsin-Madison working on engineering epistemic games. Formerly an engineering studentand a classroom teacher, she is currently researching how players solve and reflect on complex design problemsin engineering games and simulations. Golnaz has organized workshops, tutorials, and has been involved in thedevelopment of Epistemic Network Analysis, a discourse and learning network analysis tool.Wesley Collier is a graduate student in learning sciences in the Epistemic Games research group at theUniversity of Wisconsin-Madison working on the Nephrotex project (an engineering epistemic game) andEpistemic Network Analysis. He is interested in how games and simulations can be assessed using discourseanalysis.Paul A. Kirschner is professor of Educational Psychology and Programme Director of the Learning andCognition programme at the Centre for Learning Sciences and Technologies at the Open University of theNetherlands. He is past President of the International Society for the Learning Sciences, member of theScientific Technical Council of the Foundation for University Computing Facilities (SURF WTR), associateeditor of Computers in Human Behavior and chief editor of the Journal of Computer Assisted Learning.Simon Knight is a PhD candidate in Learning Analytics at the UK Open University’s Knowledge MediaInstitute and researches epistemic commitments in information seeking. He attended the highly competitiveLearning Analytics Summer Institute and received a ‘best paper’ nomination for his LAK13 paper. He hasreviewed including for the Journal of Learning Analytics and ICLS-2013.Simon Buckingham Shum is a Professor of Learning Informatics and Associate Director (Technology) at theUK Open University’s Knowledge Media Institute, where he leads the Hypermedia Discourse group. He wasProgramme Co-Chair for the 2012 Learning Analytics conference, a co-founder of the new Society for LearningAnalytics Research, and is a regular invited speaker on the topic. He brings a human-centred computingperspective to the challenge of building analytics, collective intelligence and sensemaking tools.David Williamson Shaffer is a Professor of Learning Science at the University of Wisconsin-Madison, is aPrincipal Investigator at the Wisconsin Center for Education Research. Shaffer is one of the developers ofEpistemic Network Analysis (ENA), which uses network models to quantify and visualize the connections thatstudents make between skills, knowledge, identity, values, and epistemology when solving complex problems.Alyssa Friend Wise is an Associate Professor of Education at Simon Fraser University. An integral member ofboth learning sciences and learning analytics communities, her research focuses on developing digitalenvironments and analytics that work together to promote learning. Dr. Wise serves on the ISLS CSCLCommittee, the Executive of the Society for Learning Analytics Research, as an Associate Editor of the Journalof Learning Analytics, and was the Workshop & Tutorial Chair for LAK2013.ICLS 2014 Proceedings1680© ISLSTheoretical BackgroundBridging learning sciences and analytic techniques through learning analytics which offer the ability to track anever increasing number of process and output variables is of great current interest. Indeed, this ‘middle space’was the theme of the 2013 Learning Analytics and Knowledge conference (LAK13) (Suthers and Verbert,2013):	  In summary, although individual research efforts may differ in their emphasis, we believe thatall research in Learning Analytics should address the “middle space” by including bothlearning and analytic concerns and addressing the match between technique and application.Advances in learning theory and practice are welcome, provided that they are accompaniedwith an evaluation of how existing or new analytic technologies support such advances.Advances in analytic technologies and methods are welcome, provided that they areaccompanied with an evaluation of how understanding of learning and educational practicesmay be advanced by such methods. (Suthers and Verbert, 2013, p.2)The core issue is that for learning analytics to have the most effective impact they must attend to theexisting research around learning, cognitive, social and epistemological factors (Knight, Buckingham Shum, &Littleton, 2014). Shaffer (2011) noted the problematic nature of the term ‘data mining’ suggesting we shouldinstead refer to ‘data geology’; that is, we must understand the underlying structure of the phenomenon we seekto explore with analytics prior to digging into data.The “interpretative flexibility” of new technologies – including analytics – is high: when weconsider appropriation of technology within particular social settings we should be mindful ofnot falling into the trap of technological determinism (Hamilton & Feenberg, 2005). As Crookand Lewthwaite (2010) note, our expectations for technology for transformative changeshould be mitigated by an understanding of those technologies in wider educational systems.Moreover, we should understand that technology’s influence comes about through pedagogicchange – not out of technology’s direct effects (Crook & Lewthwaite, 2010). (Knight,Buckingham Shum and Littleton, 2013, p. 6).This workshop applies these middle space issues to the ICLS conference theme, marking a sharpconceptual distinction between practice and process. The workshop begins with discussion around the status ofthese two concepts and the ways in which they can be evidenced through the analysis of data traces:Process – a series of related actions engaged in as part of learning activitiesPractice – a repertoire of processes organised around particular foci recognised within a social groupA focus on practice reflects the growing recognition in educational thinking that students from schoolage upwards should be given the opportunities to engage in authentic learning challenges, wrestling withproblems and engaging in practices increasingly close to the complexity they will confront when they graduate.In higher education the quality of the student experience is under intense scrutiny, as educational systems reflectat national and international levels on their fitness for purpose and value for money. The workforce now needsgraduates who are not just academically excellent, but have transferable skills and competencies equipping themto rapidly learn new work practices, and demonstrate the qualities needed to thrive in complexity anduncertainty. This is serving as a driver for action research into new models focused on the wholistic design oflearning, catalysing academics (Deakin Crick, 2009; Gardner, 1983; Perkins, 1993; Claxton, 2001), nationalnetworks and funding programmes (Whole Education, 2014; Hewlett Foundation, 2014). A particular focus ofthese initiatives is on ‘deeper learning’ that is more authentic in nature—to the extent that the educator may notknow the ‘right answer’ but is learning with the students. Indeed, there may be no knowable right answer, suchis the open-ended nature of truly “wicked problems” whose very definition sometimes defies consensus (Rittel,1984). While accuracy of conceptual understanding remains as important as ever, in the absence of a knowablecorrect solution, it becomes increasingly important to evidence mastery of the appropriate practices throughwhich one may tackle such open-ended problems.To give a focus, many factors of the developing area of ‘Social Learning Analytics’ (SLA)(Buckingham Shum & Ferguson, 2012) focus on properties of learning which come into being through socialactivity. In particular SLA relate to: discourse between learners and teachers, social network analysis, contentanalytics on user generated web 2.0 materials, dispositional analytics regarding the ‘mindsets’ learners bring,and context analytics regarding facets of learner environments. Yet, while tracking and supporting high qualityinstances of these facets of learning may be important, understanding why such processes are involved inlearning or the display of knowledge requires theoretical or/and empirical understanding of the practices inwhich they are constituted, and which they constitute. It may, for example, be important to explore “interactionICLS 2014 Proceedings1681© ISLStrajectories”, the ways in which processes develop and interact over time, in their social, practice-orientedcontext (Furberg, Kluge and Ludvigsen, 2013).Knowledge practices are, thus, fundamental to learning processes – e.g. metacognitive skills, epistemiccognition, self-regulation – and the particular subject domain practices to which we introduce our students, andthe degree to which assessments value authenticity. Learning in the context of a practice is an inherently socialprocess where knowledge is co-constructed in some specific social environment, or within some community.Analytically, such learning communities have been described as communities of practice—groups of people thatshare ways of working, thinking, and acting in the world (Lave & Wenger, 1991). In this view, designedlearning environments that are modeled around such communities offer a space for novices to learn processeswithin the context of the practice. For example, epistemic games are simulations of professional workplaceswhere participants role-play as interns and learn to think about and solve problems in the ways that professionalwithin that practice would solve problems (Shaffer, 2007). These simulations provide process and product datafrom learner interactions that are analysed through the lens of an epistemic frame, the ways in which membersof a community make connections between domain-specific skills, knowledge, identity, values andepistemology (Rupp, Gushta, Mislevy, & Shaffer, 2010). The analysis of process data in terms of epistemicframes can reveal something about novice learning processes, such as how novices imitate and internalize theprofessional ways of thinking that mentors model (Nash & Shaffer, 2011), develop positive associations withthe practice (Chesler, Arastoopour, D’Angelo, Bagley, & Shaffer, 2013), and develop the social identity of apractice (Arastoopour & Shaffer, 2013). In other words, designed learning environments based on knowledgepractices allow for process data from activities that the community would recognise, and can be analysed inlight of community practices.The notion of practices of learning and becoming is also relevant at the level of analytics use. We needto design and evaluate analytics with not only a view to the processes of how they are be used, but also to howthey can form part of instructors’ and students’ practices, thus shifting patterns of teaching and learning activity(Wise, Zhao & Hausknecht, 2013). For example, Social Learning Analytics might be productively used to fosterdata-assisted reflection for the support of the goal-setting, monitoring, and evaluative processes of self-regulatedlearning. Here again Crook & Lewthwaite’s (2010) concern is key. In order for our expectations for the potentialof new tools to be met, we must understand the ways in which they are embedded into educator practice, and thepolicies around those practices (Piety, 2013).In sum, the goal of the workshop is to discuss these and other distinctions between process and practiceand explore their implications for research and the design of analytic platforms, using participant submissionsand organizer examples as useful foci of discussion.ObjectivesWe anticipate that this workshop will provide a space to discuss conceptual issues around analysis of learningand becoming in practice. Short papers and demonstrations will allow for a forum to describe and discusslearning analytics used in the field; the theoretical base/learning science concepts and frameworks surroundinglearning analytics; how current work uses data traces to generate evidence of processes, practices or both; andother items/themes for future discussion surrounding learning analytics.By way of providing illustrative examples, and a potential source of discussion and papers, this workshop willbe devoted to hands-on activity. The format will thus include:1. Short introductory presentations on the emerging field of learning analytics, and practice orientedanalytic techniques.2. Discussion of conceptual issues around process and practice analytics, and their research and designimplications. We anticipate this session being resourced by short presentations of accepted papers anddemonstrations, followed by a combination of whole-group and break-out-group discussion. Thediscussion will be shaped by participants but may be focused on the suggested paper submissiontopics, or particular subject practices (e.g., STEM), etc.3. The afternoon session is devoted to hands-on demonstrations of approaches to analysis of practice.Demonstrations will be used as a focus for discussion around suitability of analytic techniques for theanalysis of learning and becoming in practice alongside discussion of emergent themes from themorning sessions. Participants will be encouraged to consider the broad application of suchtechniques to their own research agendas. A combination of submitted and invited presentations willbe used to resource this session as required. The workshop organisers have experience in and can leaddiscussion around, demonstrations of Epistemic Network Analysis (Shaffer & Arastoopour) thebuilding of teacher practices around analytics (Wise), and discourse/collective intelligence mappinganalytics (Buckingham Shum)ICLS 2014 Proceedings1682© ISLS4. Plenary discussion will provide time for considering the theoretical and empirical implications ofdefining ‘practice’ for research agendas with reference to the learning sciences broadly, and toparticipants own research programs.Because of the interactive nature of this workshop, we anticipate that a significant number ofparticipants will continue discussing and exploring topics surrounding learning analytics and practice beyondthe workshop. In addition, we expect participants will collaborate on future projects and tool developmentregarding the relationship between learning analytics, processes, and practices.ReferencesArastoopour, G., Shaffer, D. W. (2013). Measuring social identity development in epistemic games. Paperpresented at the 10th International Conference on Computer Supported Learning (CSCL), Madison,WI.Buckingham Shum, S., & Ferguson, R. (2012). Social Learning Analytics. Educational Technology & Society,15(3), 3–26.Claxton G. (2001) Wise Up: Learning to Live the Learning Life, Stafford: Network Educational Press.Chesler, N., Arastoopour, G., D’Angelo, C., Bagley, E., & Shaffer, D. W. (2013). Design of a professionalpractice simulator for educating and motivating first-year engineering students. Advances inEngineering Education, 3(3).Crook, C., & Lewthwaite, S. (2010). Technologies for formal and informal learning. In K. Littleton, C. Wood,& J. K. Staarman (Eds.), International Handbook of Psychology in Education. Emerald (pp. 435–461).Emerald Group Publishing.Deakin Crick R. (2009) Pedagogical challenges for personalisation: integrating the personal with the publicthrough context-­‐driven enquiry. Curriculum Journal 20: 185-306.Furberg, Anniken, Anders Kluge, and Sten Ludvigsen (2013). Student Sensemaking with Science Diagrams in aComputer-Based Setting. International Journal of Computer-Supported Collaborative Learning , 8(1),41–64.Gardner H. (1983) Frames of Mind: The Theory of Multiple Intelligences, New York: Basic Books.Hamilton, E., & Feenberg, A. (2005). The Technical Codes of Online Education. Techné: Research inPhilosophy and Technology, 9(1).Hewlett Foundation. (2014) Deeper Learning Initiative: William & Flora Hewlett Foundation. Available at:http://www.hewlett.org/programs/education-program/deeper-learningKnight, S., Buckingham Shum, S., & Littleton, K. (2014). Epistemology, assessment, pedagogy: where learningmeets analytics in the middle space. Journal of Learning Analytics, In-press.Lave, J., & Wenger, E. (1991). Situated learning: Legitimate peripheral participation. Cambridge, MA:Cambridge University Press.Perkins D, Jay, E. & Tishman S. (1993) Beyond Abilities: A DispositionalTheory of Thinking,. Merrill-Palmer Quarterly 39: 1-21.Nash, P., & Shaffer, D. W. (2011). Mentor modeling: The internalization of modeled professional thinking in anepistemic game. Journal of Computer Assisted Learning, 27(2).Piety, P. J. (2013). Assessing the educational data movement. Teachers College Press.Rittel HWJ. (1984) Second-generation design methods. In: Cross N (ed) Developments in Design Methodology.New York: Wiley, 317-327.Rupp, A. A., Gushta, M., Mislevy, R. J., & Shaffer, D. W. (2010). Evidence-centered design of epistemicgames: Measurement principles for complex learning environments. The Journal of Technology,Learning and Assessment, 8(4).Shaffer, D.W. (2011, December). Epistemic Network Assessment. Presentation at the National Academy ofEducation Summit, Washington, DC.Shaffer, D. W. (2007). How Computer Games Help Children Learn. New York: Palgrave.Suthers, D. D., & Verbert, K. (2013). Learning analytics as a “middle space.” In Proceedings of the ThirdInternational Conference on Learning Analytics and Knowledge (pp. 1–4). New York, NY, USA:ACM. doi:10.1145/2460296.2460298.Whole Education. (2014) Available at: http://WholeEducation.orgWise, A. F. & Chiu, M. M. (2011). Analyzing temporal patterns of knowledge construction in a role-basedonline discussion. International Journal of Computer-Supported Collaborative Learning. 6(3), 445-470.Wise, A. F., Zhao; Y. & Hausknecht, S. N. (2013). Learning analytics for online discussions: A pedagogicalmodel for intervention with embedded and extracted analytics. In D. Suthers & K. Verbert (Eds.)Proceedings of the 3rd Conference on Learning Analytics and Knowledge (pp. 48-56). Leuven,Belgium: ACM.ICLS 2014 Proceedings1683© ISLS