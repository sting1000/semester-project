The Nature of Student Thinking and Its Implications for the Use ofLearning Progressions to Inform Classroom InstructionAlicia Alonzo, Michigan State University, alonzo@msu.eduAndrew Elby, University of Maryland, elby@umd.eduAbstract: Underlying much of the work on learning progressions (LPs) is a strong thoughoften tacit assumption that student thinking is theory-like and context-independent. In thiswork-in-progress, we use both theoretical perspectives on the nature of novices’ knowledgeand empirical evidence of the context-dependent variability of students’ reasoning in physicsto question this assumption and to argue that characterizing students in terms of LP “levels”inadequately captures their understanding of force and motion. We then analyze one teacher’suse of LP-based data to reason about student thinking and instructional responses. While theteacher reasoned fluidly using LP levels, he more frequently used finer-grained knowledgeelements and contextual factors to interpret student thinking; and these finer-grainedinterpretations led to more actionable instructional implications. Thus, while recognizing LPsas models (imperfect representations) of student thinking, we argue that their assumption of“levels” of student understanding may limit their utility for classroom decision-making.IntroductionLearning progressions (LPs)—“descriptions of the successively more sophisticated ways of thinking about atopic” (National Research Council [NRC], 2007, p. 219)—have been touted as having the potential to influenceeducational policies and practices including standards, curriculum, instruction, and assessment (e.g., Black,Wilson, & Yao, 2011; National Assessment Governing Board, 2008; NRC, 2007, 2012). Although the NRC(2007, 2012) has tended to emphasize large-scale applications and LPs that span many years of instruction, LPshave also been promoted as tools to support teachers’ instructional decision-making (Alonzo, 2011; Black et al.,2011; Furtak, 2012). Finer-grained LPs may be needed for this purpose (Alonzo & Gearhart, 2006; Gotwals,2012). In this paper, we focus on the use of these finer-grained LPs to inform teachers’ classroom work.Underlying much of the work on LPs is the strong assumption that a student’s thinking is reasonablyconsistent with a particular LP level (Alonzo, 2012). By providing generalized descriptions of students’ thinkingat each level and categorizing students as being “at” a level, LPs infer that, while some contexts may be moredifficult than others, students apply the same thinking somewhat consistently. This assumption mirrors the claimthat children’s thinking is internally consistent and theory-like (e.g., Ionnides & Vosniadou, 2001). However,some researchers argue that students’ prior knowledge consists partly of misconceptions, each with its owninternal coherence, but that a student’s misconceptions might not cohere into a theory-like structure (e.g., Carey,1986; Clement, 1982). Still others argue that novices’ intuitive knowledge is even more fragmented, consistingof abstractions from experience, such as closer is stronger, with different networks of these knowledge elementsactivated in different contexts (e.g., diSessa, 1993). Indeed, evidence suggests that student thinking in the“messy middle” (Gotwals & Songer, 2010, p. 277)—the conceptual territory between students’ initial ideas andscientific concepts—may be particularly fragmented and context-dependent (Steedle & Shavelson, 2009).Although some work on learning trajectories in mathematics has explored underlying assumptionsabout the nature of student thinking and learning (e.g., Battista, 2011), LP work in science has rarely questionedthe assumption that students’ thinking displays level-based consistency. However, this assumption is crucial forteachers’ use of LPs. If teachers do not find levels-based information about students’ understanding useful forclassroom-level decision-making, LPs will not have their anticipated impact on instruction.In this work-in-progress, we question the assumption that students’ thinking displays theory-likeconsistency, and we explore implications of this assumption for teachers’ decision-making. Relying on boththeoretical work on the nature of novices’ knowledge and empirical evidence of the context-dependentvariability of students’ reasoning, we argue that characterizing students in terms of LP “levels” inadequatelycaptures their understanding of force and motion (FM). While recognizing LPs as models (imperfectrepresentations) of student thinking, we use one teacher’s interactions with LP-based score reports to argue thatthe LP model does not provide him with instructionally actionable information. Although the teacher was able toreason fluidly about LP-based information, he tended to use finer-grained knowledge elements and contextualfactors to reason about students’ responses to assessment items and to formulate instructional responses.The Nature of Students’ ThinkingIn this section, we briefly review different models of novices’ knowledge about the physical world. We thenargue that most LP researchers at least tacitly assume theory-like consistency in student thinking at each “level,”although this assumption is problematic in light of recent empirical work.ICLS 2014 Proceedings1037© ISLSTheoretical Perspectives on the Form of Students’ Intuitive Knowledge in PhysicsSome researchers assume that novices’ knowledge consists of alterative or intuitive/naïve theories, similar incognitive structure but different in substance from the theories held by experts (e.g., Ionnides & Vosniadou,2001). For example, McCloskey (1983a, 1983b) has argued that many students initially hold a theory of motionsimilar to the one held by natural philosophers in the Middle Ages. Alternative theories about a topic such asFM, once ranked in order of increasing sophistication, map neatly onto the levels of a LP.Other accounts of students’ knowledge map less well onto LP levels. Some researchers characterizestudents’ prior knowledge as consisting partly of misconceptions, such as force is required for motion, evenmotion at constant velocity (e.g., Carey, 1986; Clement, 1982). Each misconception is assumed to drive astudent’s reasoning about all or most situations in which the misconception is relevant, but a student’s variousmisconceptions are not assumed to cohere into a theory-like structure. By this account, since a student’sconceptions at a given moment could consist of many combinations of correct conceptions and misconceptions,it may be empirically inadequate to describe students’ knowledge in terms of a small number of LP “levels.”Advocates of the knowledge-in-pieces (KiP) or “resources” perspective go a step farther from theorylike structures, by assuming that students’ intuitive knowledge of physics consists largely of abstractions fromexperience, such as closer is stronger or balancing (Elby, 2000; diSessa, 1993; Hammer, 2000; Sherin, 2006).By this account, students’ intuitive knowledge elements are neither correct nor incorrect, but are activated incontext-dependent ways, sometime productively and sometimes not. For instance, students’ intuitive sense ofbalancing can lead to correct conclusions about the forces at play in a tug-of-war between equally-matchedteams, while leading to incorrect conclusions about the forces exerted upon a ball at its peak after being thrownstraight up. By this account, the “misconceptions” documented in the literature often consist of networks ofintuitive knowledge elements, which can be unstable in response to changes in context.Empirical Evidence of Context-Dependent Variability in Students’ ReasoningEmpirical evidence is consistent with a context-dependent view of student thinking. Finegold and Gorsky (1991)found that, while more than two-thirds of college and advanced high school students used one of 11 frameworksto describe forces acting on objects in motion, for objects at rest, “specific rules exist for specific situations: aforce law for objects at rest on surfaces, for objects suspended from strings, etc.” (p. 103). When they tried toidentify frameworks that could account for both conditions, there were “almost as many models as there werestudents” (p. 109). Halloun and Hestenes (1995) found a smaller number of theories of motion (only three);however almost none of the college students they studied applied the same theory across different problemsituations. Researchers working from a KiP perspective have also provided convincing evidence to support theiraccounts of student thinking (e.g., Clark, D’Angelo, & Schleigh, 2011). Thus, there is reason to question theextent to which student thinking can be considered to be theory-like or context-independent.Learning Progression Perspective: “Levels” Assumes Theory-Like ConsistencyMost LPs assume that students will use knowledge at a particular level to reason about phenomena acrosscontexts. For example, a LP for carbon cycling (Mohan, Chen, & Anderson, 2009) expects students to reasonsimilarly about carbon generation (plant growth), carbon transformation (e.g., human growth), and carbonoxidation (e.g., burning). Indeed, the assumption of consistency across contexts is required to diagnose a studentas being at a particular level (e.g., Stevens, Delgado, & Krajcik, 2010). This assumption is present even in workthat builds from a misconceptions perspective. Alonzo and Steedle (2009) describe how a LP for FM wasdeveloped in part by “grouping similar sets of ideas together into a single level” (p. 393) such that studentthinking is treated as a predictable web of context-independent misconceptions.The iterative design of LPs and associated assessments often entails evaluating the consistency ofstudents’ responses to assessment tasks. Thus, even when researchers do not explicitly describe students’thinking as context-independent and consistent, they nonetheless use a theory-like perspective as they seekreliable means of sorting students into context-independent, somewhat consistent “levels” of thinking (e.g.,Alonzo & Steedle, 2009; Duncan, Rogat, & Yarden, 2009). Indeed, the consistency of students’ reasoning ateach level is central to the “conceptual coherence” criterion (Anderson, 2008, p.4) for validation of LPs.Although researchers acknowledge that particular contexts may be more or less difficult, most LP work assumesthat student thinking is sufficiently context-independent and consistent to be characterized in terms of levels.One Teacher’s Reasoning about LP-Based Evidence of Student ThinkingData Collection ContextOur work-in-progress uses data collected as part of a study of teachers’ interpretations and use of LP-basedformative assessment information. Seven physics teachers, all recommended as employing consistent and highquality formative assessment practices, participated in a series of three interviews. Before the second interview,teachers read a brief description of the LP construct in general and a FM LP (Alonzo & Steedle, 2009) inICLS 2014 Proceedings1038© ISLSparticular. During the second and third interviews, teachers interacted with score reports based on the FM LPand a set of 16 associated ordered multiple-choice (OMC; Briggs, Alonzo, Schwab, & Wilson, 2006) items. Thescore reports provided 1) LP diagnoses for each student and 2) item text and information about students’responses to each item. Teachers were asked to “think aloud” (Ericsson, 1993) as if the score reports describedtheir own students. Afterwards, the interviewer asked follow-up questions, both for clarification and to probeteachers’ reactions to particular features of the score reports.We have begun analyzing “Tim’s” second and third interviews. We chose Tim for this analysisbecause, from our prior three years of work with him, we knew him to be thoughtful and familiar with the ideaof “learning progressions.” Indeed, some of our work with him involved the FM LP. Although Tim represents a“best case” (strong formative assessment practices and familiarity with the LP construct), his interactions withthe score reports displayed similar patterns to those of the other six teachers in the larger study.AnalysisIn this work-in-progress, we drew upon the theoretical perspectives described above to explore two approachesto characterizing Tim’s interactions with the LP-based score reports. One researcher started by looking forevidence (and coded the transcripts in terms) of Tim’s use of a theory-like, a misconceptions, and a KiP view ofstudent thinking. The other researcher started with a narrative analysis of how Tim was reasoning about bothstudent thinking and instructional responses. After this initial pass through the data, both researchersindependently coded the transcripts using the two approaches. These codes were used as the basis for consensusconversations about the nature of Tim’s diagnoses of students’ thinking and associated instructional responses.ResultsRather than holding one view of the nature of students’ thinking, Tim expressed all three perspectives describedabove. In both interviews, he switched back and forth between thinking about students’ ideas in terms of a) theprovided LP levels and b) (mis)conceptions and even finer-grained knowledge elements and contextual factorsthat he thought affected students’ reasoning about the assessment items.When working with summary data that aggregated a student’s or the class’s responses to all items, Timsometimes offered interpretations consistent with a LP perspective. For example, when interpreting summarydata for an individual student, Tim used a LP level description to characterize the student’s thinking:It seems like he’s fighting between [level] 2 or [level] 3, so I’d go down to [the description oflevel] two and double check that. [Reading the LP document] “Motion implies a force in thedirection, non-motion implies no force. Conversely, student believes that force implies motionin the direction.” So it seems like he’s caught up with this idea between force being necessaryfor motion… you know, we can justify having… no net force when the object is standing stillbut not necessarily when the object is moving.However, in both interviews, Tim focused primarily upon item-level data, using the item text andstudent response data to make finer-grained interpretations of student thinking. Consistent with themisconceptions perspective (that students’ thinking is predictable and context-independent but not theory-like),he decomposed the LP levels into descriptions of more specific student ideas. For example, he questioned thescoring scheme for one of the items because two different misconceptions were mapped onto the same LP level:Those two level 3 [option]s address different… issues. One, the rocket will move at a constantspeed… the force is equal to the motion. The other one is saying… it will move until itreaches the maximum speed. There was another question about maximum speed…Similarly, he differentiated between students’ ideas about “maintaining motion” versus “speeding up motion,”even though the LP levels do not differentiate between these two types of scenarios.He also offered even finer-grained interpretations, looking to contextual features to provide him withimportant information about students’ thinking. At times, he was explicit about the information provided byitems set in different physical contexts. For example, after comparing two items, Tim commented:So it’s interesting that when [the object] is on a table, they… don’t recognize that gravity isacting on it. But when it’s in the air, they recognize that gravity’s acting on it. So maybe justassociating gravity with falling and only with falling, not as a force that’s always there. So thatgives me some information there.The idea that gravity is a special force, eliciting different student reasoning than is elicited by similar itemsforegrounding other forces, was a prevalent theme across both interviews. Rather than considering students’ICLS 2014 Proceedings1039© ISLSresponses in terms of LP levels, Tim found the specific context of gravity to be a more relevant lens throughwhich to view the assessment items and diagnose the students’ difficulties: “I mean, it’s a [Newton’s] third lawquestion. But it’s got that gravity thing in it, and they struggle with gravity.”At the end of the third interview, Tim seemed to question the utility of the LP levels. After noticing thata student who provided level 3 responses to most of the questions had struggled with items involving gravity, heobserved that “Kids have issues with gravity, and I’m noticing that there aren’t any specific forces [in the LP]”;he wondered “how gravity fits into” the LP and “if it’s a special case.” While he highlighted the student’s ideasabout gravity as being central to the student’s understanding of FM, the LP did not specifically mention gravityand, thus, did not allow Tim to identify the student’s conceptual difficulties using LP levels.LP researchers (e.g., Alonzo, 2011) have argued for the use of differences between levels, along withdiagnoses of students’ LP levels, to make instructional decisions. Tim demonstrated this use of LPs whenreasoning about a summary of students’ LP levels. After concluding that he would “focus on getting them out ofthe level 3 area,” Tim read the descriptions of levels 3 and 4 of the FM LP and concluded that “this idea thatforce causes motion as opposed to force changing motion… would be where the focus of the class would be.”However, when reasoning about specific, context-dependent student ideas, Tim provided a much moredetailed discussion of instructional implications. For example:This idea that gravity is this holding force that doesn’t let something move… just keepspopping up. So we would definitely have to talk about gravity… When we start talking aboutthe normal force of interaction… they have a good intuitive sense of heavier things areaffected more by friction. So I would try and base off of that idea…They’re telling me theythink that gravity, heavier things affect… friction, which is a good place to start.In addition, in this example, Tim treated student ideas as sensible foundations on which to build. In contrast,when reasoning from the LP levels, Tim focused on the differences between students’ ideas and correctscientific ideas but not on how students’ ideas could play a role in bridging the gap.ConclusionsLike Tim, we question the utility of LP levels for informing classroom practice. Theoretically and empirically,we have evidence that student thinking does not follow the neat patterns codified by LP levels. Whilerecognizing LPs as models (imperfect representations) of student thinking, we argue that their assumption ofcontext-independent, quite consistent “levels” of student thinking may limit their utility for classroom decisionmaking. We echo and empirically support Shavelson and Karpius’ (2012) concern about the danger of cubbyholing student ideas into a LP model.While not questioning the utility of multi-year LPs to inform standards and curriculum development,we argue that the finer-grained LPs intended to guide teachers’ decision-making may still be too coarse-grainedfor this purpose. Previous work suggests that teachers do not always use LPs as intended. For instance, Furtak(2012) found that some teachers used a LP unproductively, to identify student misconceptions that need to besuppressed. This paper makes a different point. We found that Tim, at times, used the LP-based score reportsexactly as the LP designers intended, attending to students’ “levels” of thinking and how to transition students tothe next higher level. We argue, however, that Tim’s analysis of the LP-based score reports provided moreactionable interpretations when he did not focus on the LP levels. Specifically, his finer-grained interpretationsof student thinking, consistent with misconceptions and KiP perspectives, led to more specific, detailedinstructional ideas. We present this work-in-progress as an existence proof, advancing the possibility that, whileLPs may be useful in supporting view of student thinking that is more nuanced than “gets it”/”doesn’t get it,”they may not be well-suited for day-to-day instructional decision-making.ReferencesAlonzo, A. C. (2011). Learning progressions that support formative assessment practices. Measurement:Interdisciplinary Research and Perspectives, 9, 124–129.Alonzo, A. C. (2012). Eliciting student responses relative to a learning progression: Assessment challenges. InA. C. Alonzo & A. W. Gotwals (Eds.), Learning progressions in science: Current challenges andfuture directions (pp. 241–254). Rotterdam, The Netherlands: Sense Publishers.Alonzo, A. C., & Gearhart, M. (2006). Considering learning progressions from a classroom assessmentperspective. Measurement: Interdisciplinary Research and Perspectives, 14, 99–104.Alonzo, A. C., & Steedle, J. T. (2009). Developing and assessing a force and motion learning progression.Science Education, 93, 389–421.Anderson, C. W. (2008, February). Conceptual and empirical validation of learning progressions: Response to“Learning progressions: Supporting instruction and formative assessment.” Retrieved from:http://www.cpre.org/ccii/images/stories/ccii_pdfs/learning%20progressions%20anderson.pdfICLS 2014 Proceedings1040© ISLSBattista, M. T. (2011). Conceptualizations and issues related to learning progressions, learning trajectories, andlevels of sophistication. The Mathematics Enthusiast, 8, 507-569.Black, P., Wilson, M., & Yao, S.-Y. (2011). Roadmaps for learning: A guide to the navigation of learningprogressions. Measurement: Interdisciplinary Research and Perspectives, 9, 71-123.Briggs, D. C., Alonzo, A. C., Schwab, C., & Wilson, M. (2006). Diagnostic assessment with ordered multiplechoice items. Educational Assessment, 11, 33-63.Carey, S. (1986). Cognitive science and science education. American Psychologist, 41, 1123-1130.Clark, D. B., D'Angelo, C. M., & Schleigh, S. P. (2011). Comparison of students' knowledge structurecoherence and understanding of force in the Philippines, Turkey, China, Mexico, and the United States.The Journal of the Learning Sciences, 20, 207-261.Clement, J. (1982). Student preconceptions in introductory mechanics. American Journal of Physics, 50, 66.diSessa, A. A. (1993). Towards an epistemology of physics. Cognition and Instruction, 10(2-3), 105-225.Duncan, R. G., Rogat, A. D., & Yarden, A. (2009). A learning progression for deepening students’understandings of modern genetics across the 5th-10th grades. Journal of Research in ScienceTeaching, 46, 655-674.Elby, A. (2000). What students' learning of representations tells us about constructivism. Journal ofmathematical behavior, 19, 481-502.Ericsson, K. A., & Simon, H. A. (1993). Protocol analysis: Verbal reports as data. Cambridge, MA: MIT Press.Finegold, M., & Gorksy, P. (1991). Students’ concepts of force as applied to related physical systems: A searchfor consistency. International Journal of Science Education, 13, 97-113.Furtak, E. M. (2012). Linking a learning progression for natural selection to teachers’ enactment of formativeassessment. Journal of Research in Science Teaching, 49, 1181-1210.Gotwals, A. W. (2012). Learning progressions for multiple purposes: Challenges in using learning progressions.In A. C. Alonzo & A. W. Gotwals (Eds.), Learning progressions in science: Current challenges andfuture directions (pp. 461-472). Rotterdam, The Netherlands: Sense Publishers.Gotwals, A. W., & Songer, N. B. (2010). Reasoning up and down a food chain: Using an assessment frameworkto investigate students’ middle knowledge. Science Education, 94, 259-281.Halloun, I. A., & Hestenes, D. (1985). Common sense concepts about motion. American Journal of Physics, 53,1056-1065.Hammer, D. (2000). Student resources for learning introductory physics. American Journal of Physics, PhysicsEducation Research Supplement, 68(S1), S52-59.Ioannides, C., & Vosniadou, S. (2001). The changing meanings of force: From coherence to fragmentation.Cognitive Science Quarterly, 2(1), 5-62. Retrieved from the University of Athens website:http://www.cs.phs.uoa.gr/el/staff/vosniadou/force.pdf.McCloskey, M. (1983a). Intuitive physics. Scientific American, 249, 122-130.McCloskey, M. (1983b). Naive theories of motion. In D. Gentner & A. Stevens (Eds.), Mental Models (pp. 299324). Hillsdale, NJ: Lawrence Erlbaum.Mohan, L., Chen, J., Anderson, C. W. (2009). Developing a multi-year learning progression for carbon cyclingin socio-ecological systems. Journal of Research in Science Teaching, 46, 675-698.National Assessment Governing Board. (2008). Science framework for the 2009 National Assessment ofEducational Progress. Retrieved from: http://www.nagb.org/publications/frameworks/science-09.pdf.National Research Council. (2007). Taking science to school: Learning and teaching science in grades K-8.Washington, DC: The National Academies Press.National Research Council. (2012). A framework for K-12 science education: Practices, crosscutting conceptsand core ideas. Washington, DC: The National Academies Press.Shavelson, R. J., & Kurpius, A. (2012). Reflections on learning progressions. In A. C. Alonzo & A. W. Gotwals(Eds.), Learning progressions in science: Current challenges and future directions (pp. 13-26).Rotterdam, The Netherlands: Sense Publishers.Steedle, J. T., & Shavelson, R. J. (2009). Supporting valid interpretations of learning progression leveldiagnoses. Journal of Research in Science Teaching, 46, 699-715.Sherin, B. (2006). Common sense clarified: The role of intuitive knowledge in physics problem solving. Journalof Research in Science Teaching, 43, 535-555.Stevens, S. Y., Delgado, C., & Krajcik, J. S. (2010). Developing a hypothetical multi-dimensional learningprogression for the nature of matter. Journal of Research in Science Teaching, 47, 687-715.AcknowledgmentsFunding for this project was provided by NCS Pearson, Inc. We acknowledge the assistance of Elizabeth Xengde los Santos, who conducted the interviews reported here.ICLS 2014 Proceedings1041© ISLS