The Interplay of Domain-Specific and Domain-General Factors inScientific Reasoning and ArgumentationFrank Fischer, Christof Wecker, Andreas Hetmanek, Ludwig-Maximilians-Universität München,Leopoldstr. 13, 80802 Munich, Germanyfrank.fischer@psy.lmu.de, christof.wecker@psy.lmu.de, andreas.hetmanek@psy.lmu.deJonathan Osborne, Stanford University, 485 Lasuen Mall, Stanford, CA, osbornej@stanford.eduClark A. Chinn, Ravit Golan Duncan, Ronald W. Rinehart, Rutgers University, 10 Seminary Place,New Brunswick, NJclark.chinn@gse.rutgers.edu, ravit.duncan@gse.rutgers.edu, ron.rinehart@gse.rutgers.eduStephanie A. Siler, David Klahr, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PAsiler@cmu.edu, klahr@cmu.eduDiscussant: William A. Sandoval, University of California at Los Angeles, 405 Hilgard Avenue, Los Angeles,CA, sandoval@gseis.ucla.eduAbstract: This symposium raises the issue of the role of domain-general factors in scientificreasoning and argumentation tasks. The contributions cover conceptual problems concerningthe interplay of general and specific elements in scientific reasoning, methodologicalrequirements and existing research paradigms for investigating the role of cross-domainfactors in scientific reasoning and argumentation tasks, limitations of domain-generalepistemic criteria in the context of evaluating competing explanations and evidence, andexperimental evidence exploring the limits of domain-generality in the context ofinterventions designed to foster scientific reasoning. The symposium includes a technologysupported interactive discussion with the audience.IntroductionArgumentation and scientific reasoning are often included among the crucial skills that students need in order tomaster the challenges of a knowledge society in the 21st century. These skills are typically conceived of asbroadly applicable and largely domain-independent.However, the idea of cross-domain skills has been challenged more generally from several directions.First, research in personality psychology has showcased so-called “general cognitive abilities” (i. e. intelligence)as the single domain-independent factor in the explanation of performance variation among persons in cognitivetasks across fields, including scientific reasoning and argumentation. Second, research originating fromcognitive psychology has emphasized the amount and quality of highly domain-specific experiences as a majorexplanatory variable for skilled performance. In particular, expertise research has provided strong evidence forthe dominating role of years of deliberate practice for developing highly domain-specific excellence (Ericsson,2006). Third, the situated cognition approach has advanced the idea that knowledge is tied to activities inspecific contexts and cannot easily be transferred to different activities (e. g., Greeno, 1998). This symposiumtakes on the issue of the existence and the role of domain-general factors for performance in scientific reasoningand argumentation tasks. A particular focus will be on the relationship and the interplay of domain-general anddomain-specific factors in these tasks.In the first contribution, Jonathan Osborn questions the tenability of scientific reasoning as domaingeneral. As further problems he identifies the failure to take the role of knowledge in scientific reasoning intoaccount, a lack of reconciliation of partly complementary perspectives of researchers from different disciplines,as well as a lack of focus on critique as an important component of scientific reasoning.Second, Christof Wecker, Andreas Hetmanek, and Frank Fischer first discuss the methodologicalrequirements for empirical research that purports to demonstrate or disprove the existence of cross-domain skillsof scientific reasoning and argumentation. Then they identify three research paradigms and review exemplarystudies from each of them, including research of their own, which all suggest that cross-domain skills mayactually play a role in scientific reasoning and argumentation.Third, Clark Chinn, Ravit Golan Duncan, and Ronald Rinehart set out to theoretically explore theinterplay of domain-general and domain-specific factors, and particularly, of the ways in which domain-generalknowledge can and cannot be used across domains or topics depending on the domain-specific knowledgeavailable. Drawing on findings from the PRACCIS (Promoting Reasoning and Conceptual Change in Science)project, they show how students both are able to develop sets of epistemic criteria that they attempt to applyICLS 2014 Proceedings1189© ISLSacross domains, and struggle actually applying these criteria across domains. Failure to identify referents for thecriteria, referent misidentification, and narrow knowledge are identified as the major obstacles.In the fourth contribution, Stephanie Siler and David Klahr turn to the issue of how domain-generalskills are learned, and to the role of domain-specific examples in this process. In an experimental study theycould show that, across the board, students made the least progress if they first evaluated experiments that wererepresented in an abstract fashion (no specific values for independent variables mentioned) and thenexperiments that were represented in a concrete fashion (specific values of independent variables mentioned).Younger and lower-ability students showed better transfer in an all-concrete condition than in a concrete-fadingcondition. Domain-specific information appears to be critical for acquiring domain-general skills.Finally, the discussant William Sandoval will identify specific and common issues in the differentcontributions, bring in his own perspective on the topic, and start the discussion with the audience. To facilitateinteraction with the audience, we will use Web-based audience response technology and ask the audience towrite comments and reactions via their smart phones or computers during the individual presentations. One ofthe co-authors of this symposium will monitor contributions from the audience and provide a summary of thereactions directly after the presentation. In addition, one or two issues that seem specifically thought-provokingmay be highlighted and the presenter may be asked to respond to it. In addition, the discussant will raise moregeneral, overarching issues, to which every presenter can briefly respond. The audience will again be invited tocontribute further, more overarching questions and arguments.Teaching Scientific Reasoning: The Ineluctable in Search of the Ineffable?Jonathan Osborne, Graduate School of Education, Stanford UniversityEver since its inception, one justification for science education has been that it might educate the neophytestudent in the highly-valued, disciplinary habits of mind of the scientist (DeBoer, 1991; Dewey, 1916; Layton,1973; Turner, 1927). In contemporary society where information is readily available, such arguments havebecome more pre-eminent. Hill (2008), for instance, has argued that the societies that sustain their competitiveedge in the coming decades will be ‘post-scientific’ societies. In such a society, highly valued skills will be theability to draw on a range of disciplinary knowledge, to think creatively and to evaluate new ideas in a critical,reflective and rational manner. Employers will require individuals who, while having a core understanding ofscientific and technical principles, have the ability to communicate and synthesize knowledge in an originalmanner. Similar arguments can be found in the recent NRC (2012) report which argues that it is important todevelop three domains of competence – the cognitive, the intrapersonal, and the interpersonal. Gilbert puts iteven more straightforwardly arguing that ‘in a world where there is an oversupply of information, the ability tomake sense of information is now the scarce resource’ (Gilbert, 2005). Central to developing students’ ability toundertake the cognitive process of complex reasoning is the ability to think critically and analytically, tacklenon-routine problems, and construct and evaluate evidence-based arguments. In the case of science education,then, it is important to ‘be certain that we emphasize what we want, for we shall surely get what we emphasize’(Hill, 2008). However, science education has suffered from a lack of clarity about ‘what we want’ and how tobuild student competency with scientific reasoning. Drawing on a historical analysis this paper identifies fourproblems have confounded the field each of which is elaborated beneath. The paper then offers a model ofscientific reasoning which addresses each of these concerns.Problem 1: The Disjuncture between Domain-Specific and Domain-GeneralConceptions of Scientific ReasoningThe first problem is a product of the failure to identify the domain specific and disciplinary nature of scientificreasoning. Whilst this may seem self-evident to science educators, many attempts to define scientific reasoninghave emphasized scientific reasoning as a domain general concept. One version of this is seen amongpsychologists who commonly take a ‘nothing-special view’ (Simon, 1966) arguing that general reasoningabilities can account for the main characteristics of scientific reasoning, and that these are more important asthey are transferable. Klahr and Simon (1999), for example, acknowledge that domain-specific aspects are thebasis for ‘strong methods’ within the sciences, but claim that these only have limited value because they applyto well-defined contexts and problems. Instead, they contend that, when scientists move into new areas and meetill-defined problems, general strategies or ‘weak methods’ are the most important forms of reasoning.The contrary argument, however, has been made both within psychology and science education.Perkins and Salomon (1989), for instance, made a seminal argument that ‘general heuristics that fail to makecontact with a rich domain-specific knowledge base are weak’ and that ‘a domain-specific knowledge basewithout general heuristics…is brittle’ (p. 24). Within science education, Passmore and Stewart (2002), forinstance, have argued that ‘scientific practice is discipline specific’ (p. 187). Likewise, Sandoval and Morrison(2003) conclude from their analysis of student explanations of Galapagos data ‘that epistemic and conceptualunderstanding are tightly interrelated.’ (p. 48)ICLS 2014 Proceedings1190© ISLSDefining scientific reasoning either as domain general cognitive skill is both flawed and unsustainable.It is flawed because it misrepresents the nature of scientific reasoning as, while individuals in everydaysituations may use reasoning akin to that of scientists, such forms of reasoning can not be used to define what isdistinct about scientific reasoning. Similarly, arguments for the value of teaching domain-general reasoningwithin science are unsustainable because general reasoning does not have to be taught in science. A logicalconsequence of this view is that science education is in need of a conception for scientific reasoning thatidentifies its domain specific characteristics more uniquely.Problem 2: The Failure to Identify the Role of Knowledge in Scientific ReasoningThis problem has challenged cognitive psychology since the late 1950s, when researchers made provocativecomments about reasoning skills being knowledge-independent, or, as expressed by Inhelder and Piaget (1958)as a facility which was to be seen as ‘liberated from particular contents’. While most psychologists today admitthat reasoning is knowledge-dependent (Zimmerman, 2007), many have continued to study domain-generalreasoning processes in knowledge-lean tasks (problem 1).From a disciplinary perspective, such a position is not satisfactory as domain-specific knowledgeshould be at the heart of scientific reasoning. What, for instance, are the domain-specific entities used by theindividual when reasoning within the domain? Undoubtedly, a factor that has changed over the past six decadesis the development in our understanding of what we mean by knowledge – a development that can be illustratedby the changes that have occurred in Bloom’s (1956) taxonomy of cognitive processes when compared to therevised version offered by Anderson & Krathwohl (2001). These authors split reasoning into two dimensions, aknowledge dimension and a cognitive process dimension, and use not one but four categories to describe theirknowledge dimension: factual knowledge, conceptual knowledge, procedural knowledge and metacognitiveknowledge. Likewise, Li and Shavelson (2001) have used a similar framework splitting knowledge intodeclarative knowledge (knowing what), procedural knowledge (knowing how), schematic knowledge (knowingwhy), and strategic knowledge (knowing when, where, how knowledge applies) – a perspective that wasinfluential in developing the NAEP 2009 framework for assessment in science. The common point in these andother examples is that they alter the way we conceptualize the relationship between knowledge and reasoning.That is, what used to be explained as a generic ‘reasoning skill’ linked only weakly to a body of undifferentiatedknowledge is now dependent on a set of distinctive aspects of domain-specific knowledge of ‘what we know’,‘how we know’ and ‘why it happens’.Problem 3: The Differing Perspectives of Psychologists and PhilosophersIn academic research, the most conspicuous difference in accounts of scientific reasoning is betweenpsychological and philosophical definitions. Psychologists approach the study of reasoning in a descriptive waywith a focus on cognitive processes and abilities. Sociologists, likewise, are equally descriptive but their focus ison the social practices that enable the constitution of new knowledge. Philosophers in contrast take a normativeperspective focusing more on the epistemological principles and values intrinsic to the reasoning. In one sensepsychologists are more concerned with how someone reasons, while philosophers and sociologists seek tounderstand why they reason as they do and how they justify scientific belief, albeit from very differentperspectives (Bailin & Siegel, 2003). While philosophers, sociologists and psychologists may find theirdifferences tolerable, educators cannot. As Millar and Driver (1987) have shown, any educational ‘definition’ ofscientific reasoning has to give a ‘correct’ picture of science in that it: first adequately represents the commonreasoning practices of science; second, aligns with current thinking in the learning sciences; and third, ‘works’in the sense that it does not place unreasonable expectations on either students or teachers. The need for a‘correct picture’ is justified because teaching of scientific reasoning is also teaching about science, and it ‘seemsunwise, to say the least, to develop a rationale for school science upon a view of science which is seriously atodds with current thinking’ (p. 45). However, in the history of science education rarely have the insights offeredby philosophy, sociology and psychology been brought together. Consequently, ‘merging’ these perspectiveson scientific reasoning to create a rationale that informs educational practice has been a goal of recent work –the aspiration being to develop a conception that is more workable in the classroom.Problem 4: The Absence of CritiqueThe fourth problem is the failure to recognize the role and value of critique in the construction of knowledge.The practice of science requires a dialectic between construction and critique (Ford, 2008). The two activitiesare thus mutually dependent and any account of scientific reasoning to be offered in the classroom mustincorporate a means of acknowledging the centrality of critique. The argument here is that ideas are rarelyconsidered in isolation and that a Bayesian account (Howson & Urbach, 2006) of reasoning offers a bettermodel of scientific reasoning. For instance, persuading a student of the validity of the scientific account is asmuch a task of demonstrating why alternate ideas are flawed as one of demonstrating why the scientific ideamight be right and adjusting the balance of their beliefs. Developing any understanding of the centrality toICLS 2014 Proceedings1191© ISLScritique to scientific reasoning requires it to be a common feature of the science classroom. However, thedominance of construction means that it is almost an absent feature of common pedagogic practice. Oneconsequence, as empirical evidence from our work on developing a learning progression for argumentationshows which will be presented, is that students find engaging in critique harder than engaging in construction(Osborne et al., 2013).To address all four of these problems, a model for scientific reasoning is proposed. This seeks tocombine both the philosophical model of reasoning presented by Giere et al. (2006), and the psychologicalmodel of science as a process dependent on working in two search spaces developed by Klahr and Dunbar(1988). This model emphasizes three fundamental spheres of activity which require scientific reasoning: thedevelopment of theoretical models and hypotheses (developing explanations and solutions); the testing ofhypotheses against using data gathered from the ‘real world’ (investigation); and the coordination andevaluation of the outcomes from these two domains of activity (evaluating).The most coherent account of the nature of scientific reasoning emerges from those historians whooffer a cognitive history of science (Crombie, 1994, Netz, 1999) that identifies 6 distinctive forms of reasoningwithin science . Crombie argues that these styles of reasoning have no foundation – they are just how we reasonin science and are an emergent feature of the socio-cultural context where they were first developed. Each ofthese forms of reasoning brings into being a set of ontic, procedural and epistemic entities that are required toperform the reasoning – each of which are domain specific. In this presentation, it is argued that the recognitionof the forms of reasoning intrinsic to science, and the spheres of activity in which they are conducted, providesnot only a better argument for what should be taught but also how it should be taught.Do Cross-Domain Skills of Scientific Reasoning and Argumentation Exist?Christof Wecker, Andreas Hetmanek, Frank Fischer, Ludwig-Maximilians-Universität MünchenIn this symposium, the present contribution has the function of (1) specifying the requirements that need to beimposed on empirical evidence in favour of the existence of cross-domain skills such as the skills ofargumentation or scientific reasoning, (2) discussing research, including our own, from three differentparadigms that may provide evidence in favour of the existence of cross-domain skills, and (3) formulatingconsequences and directions for future research.Requirements for Empirical Evidence in Favour of Cross-Domain SkillsTo specify the requirements that need to be imposed on empirical evidence in favour of the existence of crossdomain skills, we need to consider the “logic of skills”: “Skill” (the terms “competence”, “ability” could be usedsynonymously) is a dispositional concept (Heider, 1958, ch. 4), i. e. the property of a person, which it describes,is manifested in a specific kind of performance in specific situations. For instance, to state that somebody hasthe skill to perform a hypothesis test means that he or she will manage to conduct an adequate hypothesis test ifhe or she is motivated and has the opportunity (including that appropriate tools and materials are available) to doso (cf. Heider, 1958, ch. 4). Thus, skill is an explanatory factor of performance beyond opportunity andmotivation.However, the skill component involved in the explanation of a certain kind of performance is not amonolithic entity that is tied to the particular kind of action in a one-to-one fashion; the quality of hypothesistesting performed by different persons may differ if one person (a) has more knowledge about the topic than theother person; (b) excels on almost any cognitive tasks while the other struggles with most cognitive challenges;(c) has better skills in operations that are conducted during hypothesis testing. In other words, given the samemotivation and opportunity, performance varies as a function of domain-specific knowledge, intelligence, andskills that are not tied to a particular domain, i. e. cross-domain skills.Ironically, each of the explanatory factors of performance except one has its advocates in psychologicalresearch: Domain-specific knowledge (expertise research) and intelligence (differential psychology), but alsomotivation (motivational psychology) and opportunity (situated cognition, socio-cultural approach, activitytheory), are all well respected factors in explaining performance. The only factor that has remained elusive andthe existence of which has been questioned repeatedly are cross-domain skills. A reason for this may be that sofar research has failed to provide a compelling account for explaining this factor itself, as compared to theaccounts that have been suggested for domain-specific knowledge (e. g. as individual chunks or schemata withina simulation framework for human cognitive architecture, see, e. g., Anderson & Lebiere, 1998) and intelligence(e. g. as certain aspects of working memory capacity, see, e. g., Kyllonen & Christal, 1990). At the same time,the criteria for empirically demonstrating the existence of cross-domain skills are completely parallel to thosefor each of the other factors. Hence, the question whether cross-domain skills of scientific reasoning andargumentation exist can be operationalized – with increasing specificity – in the following ways:Are there any data showing thatICLS 2014 Proceedings1192© ISLS(i) some people systematically outperform other people with identical opportunity, motivation,intelligence, and domain-specific knowledge in scientific reasoning or argumentation tasks?(ii) some people systematically outperform other people with identical opportunity, motivation,intelligence, and domain-specific knowledge in scientific reasoning or argumentation tasks from atleast two different domains?(iii) the degree to which some people systematically outperform other people with identicalopportunity, motivation, intelligence, and domain-specific knowledge on a scientific reasoning orargumentation task from one specific domain is correlated with certain “strategic” knowledge that doesnot contain any reference to specific knowledge from the domain of the task?A positive answer to the first question would imply that performance is not explained by intelligenceand domain-specific knowledge alone. A positive answer to the second question would imply that theexplanatory factor beyond intelligence and domain-specific-knowledge is in fact not tied to a particular domain.A positive answer to the third question would involve insights about the nature of the strategic knowledgeunderlying a particular cross-domain skill.Research Evidence from Three Paradigms in Favour of the Existence of CrossDomain Skills of Argumentation and Scientific ReasoningWe can conceive of three research paradigms that can provide evidence pertinent to the question of whetherthere are any cross-domain skills, in particular with respect to argumentation and scientific reasoning:(i) research on expert performance in a domain that falls outside of their particular area of expertise butstill has some similarities to it,(ii) well-controlled experimental studies about interventions demonstrating effects on transfer tasksfrom a different domain than the one used during the interventions themselves, and(iii) correlational studies on predictors of performance controlling for competing explanatory factorsand trying to explain variance in performance by means of cross-domain dispositions or, morespecifically, skills.In the following, we briefly discuss some exemplary findings from these three paradigms before presenting thefindings from our own research that can be subsumed under the correlational approach.Expert StudiesSchunn and Anderson (1999) found that in scientific reasoning tasks such as experimental design and analysis,psychologists from an area unrelated to the specific tasks by far outperformed undergraduates despite similarlevels of domain-specific expertise. This finding is incompatible with the view that performance differences inscientific reasoning can be explained by highly domain-specific expertise.Transfer StudiesKlahr and colleagues have repeatedly shown in strong experimental designs that certain aspects of scientificreasoning – i. e., the control of variables strategy – can be efficiently trained using an explicit teachingapproach, and that typically learners from the training conditions by far outperform learners from control groupson control of variable tasks from topical areas that were not covered in the training phase (e. g., Klahr & Nigam,2004). These findings are incompatible with the position that domain-specific knowledge and intelligence alonecan account for performance differences in scientific reasoning.Correlational StudiesStanovich and West (1997) showed that individual differences in college students’ performance in evaluatingarguments concerning real-life situations are reliably linked not only to individual differences in intelligence,but also to a habit of actively open-minded thinking: Actively open-minded thinking remained a significantpredictor even after individual differences in intelligence had been partialled out (Stanovich & West, 1997,p. 342). These kinds of analyses show that cross-domain dispositions can be a relevant explanatory factor.However, the authors characterized this further explanatory factor as a habit rather than as a skill or some kindof strategic knowledge underlying a cross-domain skill.To generate evidence demonstrating that strategic knowledge can be an important factor (see (3)above), we conducted a study with a similar design, but with an additional cross-domain strategic knowledgefactor to bind unexplained variance. In particular, we wanted to provide evidence for the role of argumentationstrategy knowledge as a further explanatory factor for performance in argumentation tasks while controlling forgeneral cognitive abilities and domain-specific knowledge. The participants in this study were 123 universitystudents who completed online tests for the three predictor variables and produced written arguments in favourICLS 2014 Proceedings1193© ISLSof their opinion concerning energy supply. Using structural equation modeling, we were able to show that aftercontrolling for general cognitive abilities and domain-specific knowledge, argumentation strategy knowledgestill significantly contributes to the explanation of performance variation in argumentation tasks. Furthermore,multiple-group analysis revealed that this relation between argumentation strategy knowledge and performancein argumentation tasks holds only for learners with domain-specific knowledge above a certain minimalthreshold, which is in line with the idea that cross-domain skills such as argumentation strategies require acertain minimal amount of domain-specific knowledge upon which they can operate (cf. Alexander & Judy,1988, p. 384). So far, the available evidence applies primarily to novices. It is not unlikely that the explanatorypower of cross-domain skills may be marginalized with increasing expertise in a specific domain.Nevertheless, these findings are inconsistent with the view that performance differences inargumentation can be accounted for by domain-specific knowledge and intelligence alone and that cross-domainstrategic knowledge does not contribute to their explanation. Hence, this study provides evidence for theexistence of cross-domain scientific reasoning and argumentation skills.ConclusionsThe question of whether cross-domain skills such as those of scientific reasoning and argumentation exist hasbeen highly contested and answered differently across time. It appears that sometimes researchers tend tooveremphasize the factor on which they focus in their own research as the main or even sole explanatory factor.Not as much in published work as in less formal venues, such as when acting as a reviewer for a journal or as adiscussant at a conference, this view may be manifested as outright denial of the existence cross-of domainskills. Therefore, in this contribution we have thoroughly analyzed the methodological criteria that need to beimposed on research pertaining to the existence or non-existence of such cross-domain skills of scientificreasoning and argumentation, and we have identified three appropriate research paradigms for studying thisquestion. We have presented results from these three paradigms as well as research of our own speaking infavour of the existence of cross-domain skills of scientific reasoning and argumentation. There is someindication that quasi-formal argumentative strategy knowledge may underlie these cross-domain skills.Future research on cross-domain skills of scientific reasoning and argumentation should be based on acomprehensive theoretical framework of explanatory factors for performance such as the one sketched in thiscontribution, and exhibit the methodological sophistication that is necessary to rule out alternative explanatoryfactors. The operationalization of the general research question and the typology of research paradigmssuggested in this contribution may be used as a starting point in this respect. Thus, it will be possible to focusmore deeply on the interplay of cross-domain skills with domain-specific knowledge in explaining performancein scientific reasoning and argumentation tasks.Epistemic Criteria: How Far Does General Knowledge Get You?Clark A. Chinn, Ravit Golan Duncan, Ronald W. Rinehart, Rutgers UniversityThe ProblemA core premise of much work in the learning sciences is that domain and topic knowledge are intimatelyinvolved in learning and reasoning processes. This poses a serious challenge to the assumption of manyeducational researchers that it is possible to teach students general skills or strategies that can be widely usedacross different topics and disciplines (e.g. Sadler & Donelley, 2006; von Aufschnaiter et al., 2008). Forexample, there continues to be broad interest in teaching skills (critical thinking, collaboration, control ofvariables) that are generally applicable. Similarly, science education research on promoting understanding of thenature of science (NOS) seems to posit that there is general knowledge about science that can be profitablytaught. In contrast, proponents of the domain specificity of thinking argue that thinking differs so much fromone domain to another that there is little point in attempting to teach anything general (e.g., Willingham, 2007).Our goal is to work toward a theoretical analysis of ways in which general knowledge can and cannotbe used across different domains or topics. We focus on epistemic practices given their importance to humanknowledge development and their tight connection with the theme of this conference. We think that the debateon whether general epistemic practices exist, and can be productively taught, would profit from a more detailedtheoretical analysis of what a person might gain through general knowledge of reasoning practices.We draw on our experience in the PRACCIS project (Chinn et al., 2008; Duncan et al., 2011) workingwith middle school students engaged in extensive model-based inquiry instruction (several months to a full yearin four separate implementations). One feature of PRACCIS is the development of epistemic criteria by classes.We have found that students readily develop class lists of epistemic criteria for evaluating models such as “goodmodels fit all the good evidence” and “good models show all the steps in the process,” and “good models areeasy to understand.” They can also develop criteria for good evidence such as “good evidence helps you decidewhich model is better” and “good evidence uses control groups.” These criteria are used in conjunction withICLS 2014 Proceedings1194© ISLSlearning the content of a domain (e.g. genetics, natural selection). Here we consider some potential limitationsof epistemic criteria in evaluating models and evidence.The Limitations of General Knowledge of Epistemic CriteriaWhat are the limits of general knowledge of epistemic criteria? To answer this we first consider the situation inwhich a student tries to apply knowledge of such criteria to new domain with which they are fairly familiar.Two major difficulties that can arise in this situation: (1) overgeneralization and (2) undergeneralization. Theseproblems are discussed in the literature on transfer (e.g., Schwartz et al, 2012). In the domain of epistemiccriteria, we propose what we believe to be new solutions to these problems.As an example of overgeneralization, a student who believes that good models show all the steps in theprocess might prefer a model of photosynthesis with 5 steps over one with 3 steps on the grounds that the 5-stepmodel shows more steps – even though the two additional steps are not actually supported by any evidence.What this student fails to grasp is that some criteria take priority over other criteria. A prime solution to theproblem of overgeneralization is to help students refine their general understanding of the conditions underwhich different criteria are applicable. In PRACCIS classes, teachers are encouraged to invite students toarticulate conditions under which one might prefer a model with fewer steps over one with more steps.Consequently, students learn how to weigh competing criteria, and some of the conditions that govern whichcriteria take precedence in different situations. Thus, the problems of overgeneralization can be addressed in partby enabling students to develop an understanding of (still relatively general) conditions under which differentcriteria are to be applied.Undergeneralization occurs when a student fails to apply a criterion when it is relevant. For example, astudent may decide to adopt a diet based on a friend’s anecdotal success while ignoring evidence about thehealth risks associated with this diet. Among several possible sources of undergeneralization, we would like tofocus on one: Students may simply not believe that the epistemic criteria in question are valid. To facilitatestudents’ acceptance of the criteria as valuable, we encourage teachers to hold meta-criteria discussionsincorporating justifications for why different criteria should be adopted (or not adopted). For example, bydiscussing why one should prefer models that fit the evidence, students can come to appreciate the justificationsfor the use of fit-with-evidence criterion.In short, the limitations of general knowledge of epistemic criteria can be addressed in part by helpingstudents develop more nuanced (conditionalized and justified) but still relatively general epistemic criteria. Butstill, we argue below, there are limitations even to these more nuanced criteria.The Limitations of Even Conditionalized and Justified Epistemic CriteriaUsing the case of epistemic criteria, we have argued that general knowledge can be made more useful if it isconditionalized and justified. But, even if students learn more conditionalized and justified criteria, we arguethat they will still be limited in their ability to apply these criteria to new domains. Our analysis identifies threespecific limitations. First is the problem of failure to identify referents. A student with a nuanced understandingof evidential criteria may nonetheless find it impossible to identify what the evidence is in an unfamiliardomain--that is, to pick out the referents of good evidence in the real world. For example, a student readingabout whether vaccines cause autism may be committed to choosing the theory best supported by the goodevidence, yet be simply unable to identify good evidence due to poor understanding of medical research design.Second, and a more serious difficulty, arises from referent misidentification. A person applying evidentiarycriteria would erroneously identify something as evidence when it is not. For example, a student studying globalwarming could fastidiously examine evidence of local weather conditions while failing to appreciate that thisdoes not, by itself, count as evidence for long-term climate patterns. Third, and a particularly insidious problem,is narrow knowledge. Even if a student were competent in evaluating a piece of scientific evidence about thesafety of vaccinations, the student lacks the experts’ much more extensive knowledge of the boarder evidencebase on the topic. Experts in scientific fields are aware of a vast array of evidence that laypeople are not awareof, and this gap cannot be redressed without becoming an expert oneself.Unlike the under- and overgeneralization problems there is no solution to these three problems exceptdeeper domain knowledge. Even a person with extensive general knowledge will suffer from narrow knowledgeunless she herself gains true expertise in the field. As many have pointed out in recent years, ultimately in areasof non-expertise, one must trust experts (e.g., Bromme et al., 2009). All of this is not to say, however, that thegeneral knowledge is not useful. A student who has a nuanced appreciation of evidentiary fit as an epistemiccriterion will know at least two things that the student without this general knowledge will not know. First, thestudent with general knowledge of epistemic criteria will know a useful range of questions to ask experts in thefield. The student could ask about types of evidence available as well as sample size and control groups. Further,the person who has learned epistemic criteria will have command of some of the discourse practices of theexpert community and will have a greater capacity to seek out information.ICLS 2014 Proceedings1195© ISLSEvidence against Using Domain-General Examples to Teach a Domain-GeneralConcept/SkillStephanie A. Siler, David Klahr, Carnegie Mellon UniversityThe long term goal of instruction is to have students acquire knowledge and skills that are widely applicable,rather than tied to the specific context in which they were acquired. However, the question about whether theinitial learning materials and contexts should be domain-general or domain-specific, remains controversial. Asubstantial body of research has found that embedding domain-specific information in instructional exampleshelps learners understand these examples (e.g., Goldstone & Son, 2005; Goldstone & Sakamoto, 2003;Koedinger & Nathan, 2004; Kaminski, Sloutsky, & Heckler, 2006; Witzel, Mercer, & Miller, 2003). However,some research suggests that using domain-specific materials inhibits transfer to novel contexts (Kaminski et al.,2006; 2008; 2013; Bassok & Holyoak, 1989; Goldstone & Sakamoto, 2003). Consistent with these findings,which suggest that domain-specific information supports initial learning and abstracted representations bettersupport transfer to new domains, Goldstone and Son (2005) found that transfer performance among collegeundergraduates was greatest when the domain-specific information was “faded” during the training task (orpresent initially in the training session and “removed” later). We describe a study that investigated the effect offading domain-specific information in instructional examples on transfer outcomes.MethodThe domain-general concept/skill taught was simple experimental design. In the sections of instructiondiscussing domain-specific examples of experimental set-ups, specific variables and values were used inexamples and in instructional explanations (e.g., “to determine whether the slope of the ramp affects how farballs roll, one should make one ramp steep and the other not steep and make the surface, starting position, andtype of ball the same for the two ramps”). In the sections of instruction discussing domain-general examples,experimental variables and their respective values were described generically as “Variable A/B/C” and theirlevels only given as “the same” or “different” across conditions. Instructional explanations were more abstract(e.g., “to determine whether Variable X affects the result, one should make Variable X different acrossconditions and all other variables the same across conditions”).6th- and 7th-grade students at a suburban middle school worked individually on computers learning howto design unconfounded experiments in one of three conditions. In all conditions, students evaluated a series ofthree experiments. After each evaluation, students received feedback on their evaluations followed byexplanations for why the experiments were or were not “good experiments.” All instructional experiments wererepresented in tabular/text form (i.e., no pictures of the set-ups were present). In the “all-concrete” condition,students evaluated three experiments using ramps; each experiment included specific variables (e.g., slope) andcorresponding values (e.g., steep/not steep). In the “concrete fading” condition, students first evaluated aconcretely-represented ramps experiment (as in the first condition), then an intermediately concrete experiment(with specific ramps variables but unspecified values represented only as “the same” or “different” acrossconditions). In the final, “abstract fading” condition, the experimental presentation order was reversed from theconcrete-fading condition. The next day, all students completed a test assessing their ability to transfer theirknowledge to domains other than ramps.ResultsResults suggest that the benefits of incorporating this additional information (i.e., the specific experimentalvariables and their values) throughout instruction were greater for younger and lower-ability students: Thetransfer performance of younger students (6th-graders) and lower-ability 7th-graders was generally best in the allconcrete condition. However, higher-ability 7th-graders performed equally in the all-concrete and concretefading conditions and better than students in the abstract-fading condition. Across the board, students performedpoorly in the abstract-fading condition, suggesting that supporting domain-specific information is in generalcritical in the early stages of learning. A post-hoc analysis suggested that the additional concrete featuressupported students’ ability to identify experimental confounds, or variables that—because they were contrastedacross conditions – may have affected the outcome.DiscussionOur results are counter to Goldstone and Son’s (2005) finding that fading domain-specific information duringtraining led to better performance of college undergraduates on novel transfer problems. Instead, we found thatthe transfer of domain-general experimental design skills was best when domain-specific examples wereincluded throughout instruction. However, among the older students, the concrete-fading and all-concreteconditions performed similarly on the transfer task. One possible reason for these differences is that thecontinued exposure to domain-specific examples was necessary to support younger and lower-ability students’understanding of the impact of confounded variables, a concept that was not as effectively conveyed in theICLS 2014 Proceedings1196© ISLSabstracted form of the instruction. These results suggest the possibility that—at least when additional domainspecific information directly supports learners’ understanding of the instructional topic—instructional materialsand contexts that are not domain general may be most effective for teaching younger and lower-ability studentsa domain-general concept/skill. However, because this study included only minimal additional domain-specificinformation, further research is necessary to determine the optimal amount of information to support transfer aswell as how it interacts with student characteristics such as prior knowledge and general ability.ReferencesAlexander, P. A., & Judy, J. E. (1988). The Interaction of Domain-Specific and Strategic Knowledge inAcademic Performance. Review of Educational Research, 58(4), 375–404.Anderson, J. R., & Lebiere, C. (1998). The atomic components of thought. Mahwah, NJ: Erlbaum.Anderson, L.W., & Krathwohl, D. R. (2001). A Taxonomy for Learning, teaching and Assessing: A revision ofBloom's Taxonomy of Educational Objectives. London: Longman.Bailin, S., & Siegel, H. (2003). Critical thinking. The Blackwell guide to the philosophy of education, 181-193.Bassok, M., & Holyoak, K. J. (1989). Interdomain transfer between isomorphic topics in algebra and physics.Journal of Experimental Psychology: Learning, Memory, and Cognition, 15(1), 153.Bromme, R., Kienhues, D., & Porsch, T. (2009). Who knows what and who can we believe? Epistemologicalbeliefs are beliefs about knowledge (mostly) to be attained from others. In L. A. Bendixen & F. C.Feucht (Eds.), Personal epistemology in the classroom (pp. 163-193). Cambridge: CUP.Chinn, C. A., Duschl, R. A., & Duncan, R. G., Buckland, L. A., Pluta, W. P. (2008). A microgenetic classroomstudy of learning to reason scientifically through modeling and argumentation. In ICLS 2008:Proceedings of International Society of the Learning Sciences. Raleigh, NC: Lulu.Crombie, A. C. (1994). Styles of scientific thinking in the European tradition: The history of argument andexplanation especially in the mathematical and biomedical sciences and arts (Vol. 1). London:Duckworth.DeBoer, G. E. (1991). A history of ideas in science education : implications for practice. New York: TeachersCollege Press.Dewey, J. (1916). Democracy and Education. New York: The MacMillan Company.Duncan, R. G., Freidenreich, H. B., Chinn, C. A., & Bausch, A. (2011). Promoting middle-school students’understanding of molecular genetics. Research in Science Education, 41, 147-167.Ericsson, K. A. (2006). The influence of experience and deliberate practice on the development of superiorexpert performance. The Cambridge handbook of expertise and expert performance, 683-703.Cambridge, MA: Cambridge University Press.Ford, M. J. (2008). Disciplinary authority and accountability in scientific practice and learning. ScienceEducation, 92(3), 404-423.Giere, R., Bickle, J., & Maudlin, R. F. (2006). Understanding Scientific Reasoning (5th ed.). Belmont, CA:Thomson Wadsworth.Gilbert, J. (2005). Catching the Knowledge Wave? The Knowledge Society and the Future of Education.Wellington, New Zealand: NZCER Press.Goldstone, R. & Son, J. (2005). The transfer of scientific principles using concrete and idealized simulations.The Journal of the Learning Sciences, 14(1), 69-110.Goldstone, R., & Sakamoto, Y. (2003). The transfer of abstract principles governing complex adaptive systems.Cognitive Psychology, 46, 414-466.Greeno, J. G. (1998). The situativity of knowing, learning, and research. American psychologist, 53(1), 5-26.doi:10.1037/0003-066X.53.1.5.Heider, F. (1958). The Psychology of Interpersonal Relations. New York, NY: Erlbaum.Hill, C. (2008). The Post-Scientific Society. Issues in Science and Technology on Line, 24(1), 78-84.Howson, C., & Urbach, P. (2006). Scientific Reasoning: A Bayesian Approach (3rd ed.). Chicago: Open Court.Inhelder, B., & Piaget, J. (1958). The Growth of Logical Thinking from Childhood to Adoloscence. London:Routledge and Kegan Paul.Kaminski, J. A., Sloutsky, V. M. & Heckler, A. F. (2006). Do children need concrete instantiations to learn anabstract concept? Proceedings of the XXVIII Annual Conference of the Cognitive Science Society,1167-1172. Mahwah, NJ: Erlbaum.Kaminski, J. A., Sloutsky, V. M. & Heckler, A. F. (2008). The advantage of abstract examples in learning math.Science, 25, 454-455.Kaminski, J. A., Sloutsky, V. M., & Heckler, A. F. (2013). The cost of concreteness: The effect of nonessentialinformation on analogical transfer. Journal of Experimental Psychology: Applied, 19(1), 14.Klahr, D., & Dunbar, K. (1988). Dual space search during scientific reasoning. Cognitive science: Amultidisciplinary journal, 12(1), 1-48.ICLS 2014 Proceedings1197© ISLSKlahr, D., & Nigam, M. (2004). The equivalence of learning paths in early science instruction: effects of directinstruction and discovery learning. Psychological Science, 15(10).Klahr, D., & Simon H. A. (1999). Studies of Scientific Discovery: Complementary Approaches and ConvergentFindings. Psychological Bulletin, 125, 524-543.Koedinger, K. R., & Nathan, M. J. (2004). The real story behind story problems: Effects of representations onquantitative reasoning. The Journal of the Learning Sciences, 13(2), 129-164.Kyllonen, P. C., & Christal, R. E. (1990). Reasoning ability is (little more than) working-memory capacity?!Intelligence, 14(4), 389–433.Layton, D. (1973). Science for the People: The Origins of the School Science Curriculum in England. London:Allen and Unwin.Li, M., & Shavelson, R. J. (2001). Examining the links between science achievement and assessment. Paperpresented at the Annual Meeting of the American Educational Research Association, Seattle, WA.Millar, R., & Driver, R. (1987). Beyond Processes. Studies in Science Education, 14, 33-62.National Research Council. (2012). Education for Life and Work: Developing Transferable Knowledge andSkills in the 21st Century. Committee on Defining Deeper Learning and 21st Century Skills. In J. W.Pellegrino & M. Hilton (Eds.). Washington, DC: Board on Testing and Assessment and Board onScience Education, Division of Behavioral and Social Sciences and Education.Passmore, C., & Stewart, J. (2002). A modeling approach to teaching evolutionary biology in high schools*.Journal of Research in Science Teaching, 39(3), 185-204.Perkins, D. N., & Salomon, G. (1989). Are Cognitive Skills Context-Bound? Educational Researcher, 18(1),16-25.Sadler, T. D., & Donnelly, L. A. (2006). Socioscientific argumentation: The effects of content knowledge andmorality. International Journal of Science Education, 28(12), 1463 – 1488.Sandoval, W. A., & Morrison, Kathryn. (2003). High School Students' Ideas about Theories and Theory Changeafter a Biological Inquiry Unit. Journal of Research in Science Teaching, 40(4), 369-392.Schunn, C. D., & Anderson, J. R. (1999). The Generality/Specificity of Expertise in Scientific Reasoning.Cognitive Science, 23(3), 337–370.Schwartz, D. L., Chase, C. C., & Bransford, J. D. (2012). Resisting overzealous transfer: Coordinatingpreviously successful routines with needs for new learning. Educational Psychologist, 47, 204-214.Simon, H. A. (1966). Scientific discovery and psychology of problem solving. In R. Colony (Ed.), Mind andCosmos (pp. 22-40). Pittsburgh: University of Pittsburgh Press.Stanovich, K. E., & West, R. F. (1997). Reasoning independently of prior belief and individual differences inactively open-minded thinking. Journal of Educational Psychology, 89(2), 342–357.Turner, D. M. (1927). History of science teaching in England. London: Chapman and Hall.von Aufschnaiter, C., Erduran, S., Osborne, J., Simon, S. (2008) Arguing to learn and learning to argue: Casestudies of how students' argumentation relates to their scientific knowledge. JRST, 45, 101-131.Willingham, D. T. (2007, Summer). Critical thinking: Why is it so hard to teach? American Educator, 8-19.Witzel, B. S., Mercer, C. D., & Miller, M. D. (2003). Teaching algebra to students with learning difficulties: Aninvestigation of an explicit instruction model. Learning Disabilities Research & Practice, 18(2), 121131.Zimmerman, C. (2007). The development of scientific thinking skills in elementary and middle school.Developmental Review, 27(2), 172-223.AcknowledgmentsContribution by Chinn, et al.: This material is based upon work supported by the National Science Foundationunder Grant No. 9875485. Any opinions, findings, and conclusions or recommendations expressed in thismaterial are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.Contribution by Wecker et al.: This contribution was supported in the IPID program of the German AcademicExchange Service (DAAD).ICLS 2014 Proceedings1198© ISLS