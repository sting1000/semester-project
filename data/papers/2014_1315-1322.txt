Cyberinfrastructure for Design-Based Research: Toward aCommunity of Practice for Learning ScientistsChair/Author: Sharon J. Derry, University of North Carolina Chapel Hill, derry@unc.eduAuthors:Alan J. Hackbarth, Sadhana Puntambekar, University of Wisconsin Madison,ajhackbarth@wisc.edu, puntambekar@education.wisc.eduCarlos Gonzalez, Pontifical Catholic University of Chile, cgonzalu@uc.clWilliam A. Sandoval, UCLA, sandoval@gseis.ucla.eduKaterine Bielaczyc, Clark University, kateb369@gmail.comRichard Lehrer, Vanderbilt University, rich.lehrer@vanderbilt.eduDiscussant: Allan Collins, Northwestern University, acollins1937@gmail.comAbstract: This symposium is part of an NSF project to help The Learning Sciencescommunity collaboratively envision a cyberinfrastructure for design-based research (DBR).Like numerous others before us, we argue that the educational research community would bewell served by a mutually created cyberinfrastructure that would encourage and supportengagement by multiple design-based researchers in sharing knowledge and working towardanswers to important theory-driven research questions, moving our field toward a “biggerscience” approach. The vision is one in which junior and senior scholars set collaborativeresearch agendas and establish common protocols and systems to support systematiccollaborative archiving, sharing and analysis of multi-modal data. This symposium offerscritique, reflection, and concrete proposals for design, helping demonstrate the possibility ofimplementing this idea and moving it toward becoming reality.Chair’s IntroductionSharon J. DerryThe ProblemDesign-based research (DBR) is a mixed methods approach that involves the iterative and systematic design,development, and study of theoretically guided educational innovations in their implementation contexts (Barab& Squire, 2004; Collins, Joseph & Bielaczyc, 2004; The Design-Based Research Collective, 2003). AlthoughDBR is in early stages of development as a methodology and paradigm, its popularity among researchers,funding agencies, and journals is growing and spawning lively academic debates (e.g., Anderson & Shattuck.2012; e.g., Dede, 2004; McKenney & Reeves, 2013). In a penetrating commentary on the special issues devotedto DBR in Educational Researcher (2003) and Journal of Learning Sciences (2004), Dede (2004) worried thatdiscussions had pushed the boundaries of DBR too far, rendering a sort of “Swiss Army Knife” for scholarstrying to find a tool for too many purposes. He encouraged instead a bounded conceptualization (e.g., Collins,Joseph & Bielaczyc, 2004), giving focus to the kinds of questions DBR studies can reasonably address: When isan initial implementation of an educational innovation successful enough to merit further investment inperfecting the innovation? When is it successful enough to warrant implementation and testing on a large scale?What generalizable knowledge about the conditions and reasons for success can be gleaned from a DBR study,and how can we use this knowledge to support effective translation of a successful innovation into a range ofnew contexts? Our symposium addresses whether the DBR research enterprise can be improved and leveragedto help discover generalizable scientific knowledge of the type needed to develop successful educationalinnovations and understand them well enough to support their full-scale implementation.Whether general scientific knowledge can be harvested from DBR is, however, a subject for debate.DBR crafts innovations by working within educational contexts that represent a complex mesh of goals, content,technology use, principles of learning, teaching methods, and accountability systems that must be woventogether and shaped to a particular environment. Because any measure of success reflects the whole of theinteracting variables that comprise the complex system, it is very difficult to isolate stable causal mechanismswithin it or to translate lessons learned into other grade levels, students, schools, subject matter, types oftechnologies, etc. (Fadel and Lemke, 2006). For this reason, Anderson and Shattuck (2012) speculate that DBRmay be more suitable for making and sustaining improvements in small-scale systems rather than contributing tolarge-scale and far-reaching systemic reform.ICLS 2014 Proceedings1315© ISLSYet DBR is increasingly called upon to fill an important niche in the array of methods now used toimprove educational practice and create generalizable principles about it (Collins et al., 2004). For example, TheInstitute for Educational Studies (IES) currently funds ideas for educational innovation at several stages ofincreasingly larger funding, with DBR often the preferred approach for early and middle phases of aninnovation’s history. DBR in these phases helps perfect and test a theoretically designed innovation, providingevidence pertaining to its local success as well as implementation experience to support an informed translation,if warranted, into increasingly larger-scale contexts. Eventually, randomized experimental trials may producegeneralizable judgments about an innovation’s effectiveness and, consequently, about the theory of learningupon which the implementation project was based. In this model, DBR, hypothetically, contributes to theoryfrom a pragmatist’s perspective: Theories about learning in context are proposed and evolve through theirvarious instantiations within a DBR research program. Experimental trials then produce confirmatory evidence,offering generalizable judgments about an innovation’s effect as well as evidence supporting the pragmaticvalue of the situated learning theory upon which the implementation was based.However, this passage to educational research Nirvana through DBR is a slow, methodic, expensivepilgrimage and, as recent discussions of DBR acknowledge, fraught with challenges. In fact, reviewers of DBRprogress have been hard-pressed to identify any such completed journeys. But before considering thosechallenges and how to address them, it may be worth a brief digression to consider an alternative case, that is,the result of a lengthy educational research enterprise that did not build on a meticulous build-up of theory andimplementation knowledge that DBR research strives to achieve. Consider, for example, the hundreds of studiesof the effects of technology that have been conducted across a wide variety of educational settings over the lastseveral decades. Many studies have been aggregated in several influential reports—e.g., ACT Policy Report:Evaluating the Effectiveness of Technology in Our Schools, 2004; Cisco Systems/Metiri Group Technology inSchools: What the Research Says, 2006; Milken Exchange on Education Technology, 1999; and the NationalMiddle School Association Research Summary: Technology and Learning, 2007 (Fadel & Lemke, 2006; Noeth& Volkov, 2004; Schacter, 1999). These reports consistently acknowledge that using technology provides asmall but significant increase in learning across all uses in all content areas, but they offer little guidance forimplementation and include substantial caveats. Has a torrent of research on educational technology producedonly a trickle of knowledge?One problem relates to the inadequacy of the research methodologies employed to reach conclusions.“Most studies on the effect of educational technology on learning are correlational studies,” wrote Fadel andLemke (2006). “Although such studies suggest what is working, they do not control for confounds that mayprovide alternate explanations for results.” Also, perhaps we have been mired too long in fruitless atheoreticalresearch that engages experimental and statistical control to focus on effects of single features, such as “usestechnology,” that are largely meaningless when considered separately from the complex, interacting factorscomprising an entire ecology of teaching and learning. “Results and conclusions must be considered in thecontext of the interdependent set of variables in which the use of technology is embedded” (Fadel & Lemke,2006). The unfolding DBR narrative is largely a research community’s response to this realization, and the storyof its aspiration to carry out credible, theoretical scientific research of pragmatic significance in complex naturalsettings.Unlike the experimental psychologist, who controls all aspects of an environmental recipe including thespecific added ingredients associated with causal hypotheses about cognitive mechanisms, the DBR researchereschews introducing artificial controls into educational environments. What DBR teams do is infuse anestablished educational ecology with new ideas and creations (an ‘innovation’), having in mind sometheoretically derived hypotheses about how such infusions will alter the landscape to effect positive learningoutcomes. Then begins the business of systematically studying that dynamic landscape in action, perhaps beforebut certainly during and after the innovation is introduced, searching for clues that will lead to an understandingof how the innovation and the ecology coalesce to effect learning. Researchers crisscross the landscape tocollect data for analyses based on their hypotheses and a priori research questions, but they also attempt tocapture other interactions that emerge as interesting, to increase the likelihood of fruitful unanticipateddiscoveries (e.g., Derry et al., 2010). In this effort to leave no potentially important stone unturned, DBRresearchers typically document an innovations’ development and implementation history with detaileddescriptions of situational contexts, rationales for designs and design changes at different phases, and learningand process outcomes. The overwhelming result is that every DBR project produces a substantial datacollection. A widespread criticism and controversial issue dogging DBR is that it typically generates volumes ofdata that are never used.One reason for this current state of affairs is that there are few agreed-upon standards, goals orstructures to guide DBR scholars on what data to collect and archive. In addition to being unwieldy in size, DBRdatasets are shaped in content and structure by whatever unique research questions and systems for datacollection and archiving that an individual project or research group devises. Uniquely structured, unpublisheddatasets cannot be accessed or utilized, without great difficulty, by researchers and educators from outside theICLS 2014 Proceedings1316© ISLSproject. It is even less likely that such datasets will be combined and aggregated for analytic purposes. In fact,the DBR research enterprise, in all respects ranging from the formulation of research questions to the archivingof data, is so fragmented that sharing and advanced data mining, which could address important theory-basedquestions across many projects, are difficult to impossible.In discussing both promises and threats to the impact of “design-based implementation research,”Penuel, Fishman, Cheng and Sabelli (2011) argued that it is time to develop DBR as a more systematic form ofinquiry and practice. They believe the DBR community must develop better norms and practices for theorydevelopment and for specification and testing of claims. They also called for developing standards regardinghow evidence is used to guide design refinements, and practices for incorporating into studies multiple points ofview and conflicting interpretations of data. Finally they suggested standardized use of design rationales in themanner that professionals in fields such as architecture, urban planning, and software engineering articulate suchrationales to clarify the purposes and history of the design process, and to help them reflect on and modifydesigns. Design rationales could serve to make public the ways that educational teams employ evidence toresolve conflicts, weigh competing approaches to improvement, and identify new areas of focus for their work.Penuel et al. (2011) echo similar appeals by other analysts of DBR research. Collins, Joseph and Bielaczyc(2004), recently cited in Anderson & Shattuck (2012), previously advanced the idea of a DBR infrastructure tosupport data archiving, sharing and collaboration. Noting that design experiments produce large amounts of datathat go unanalyzed, they called for an infrastructure to allow researchers from outside the original design teamto access and analyze the data collected in large studies. Not surprisingly, Anderson & Shattuck’s recent review(2012) failed to uncover any evidence of such sharing currently taking place. Such sharing would require theDBR community to have standard protocols and systems for archiving of data in a format that supports multipleforms and levels of access, sharing and analysis, while adequately protecting the privacy and identities of humansubjects.Purpose of SymposiumThe organizers of this symposium believe the learning sciences community would be well served by mutuallycreating an infrastructure to encourage and support collective engagement by multiple design-based researchersin working toward answers to important theory-driven research questions, moving our field toward a “biggerscience” approach. It is in this spirit that we reach out to members of this community to join with members ofother relevant fields, such as computer science, in an effort to design, build, support and participate in amutually established cyberinfrastructure and user community for DBR. This symposium would continue theorganizers’ NSF-sponsored effort, begun at a recent CSCL 2013 workshop, in which we vetted one concreteproposal to organize a group of participating researchers around designing and using a common suite ofInternet-based workflow visualization tools that would impose flexible but agreed-upon standards for datacollection and ways of working (Hackbarth, 2011; Hackbarth, Derry, Eagan & Gressick, 2010). In this ICLSsymposium we call for additional proposals as well as reflections and critiques on proposals put forth and theenterprise in general. One question that emerges from our work so far is whether a primary goal of this endeavorshould be to support the work of junior scholars. These are the issues that the collective presentations in thissymposium, which was designed to include scholars at various career stages, will address. This goal fits wellwith the stated conference theme to “focus on practices that pertain to how we organize our own work aslearning scientists: the practices for analyzing and modeling learning across settings and time and the practicesfor designing for scale and sustainability."The presentations that follow each bring a unique perspective in discussing how a well-developed lineof inquiry might inform the design of a DBR cyberinfrastructure. Hackbarth’s presentation will supply an“object to think with” in describing how developing data analytic tools based on workflow visualizationprovides one feasible way to proceed. Richard Lehrer will contribute years of experience conducting designbased research in schools by sharing his team’s practice of creating “design documents.” In discussing thestrengths and limitations of design documents, he suggests that the next phase of DBR must focus oninstrumentation to permit tracking of design trajectories at a larger scale, so that theory development can follow.Katerine Bielaczyc will draw from cases of distributed DBR with Knowledge Building Communities to examinethe usefulness of the Social Infrastructure Framework as a basis for defining data categories and metadatatagging within a cyberinfrastructure. She will also consider how a cyberinfrastructure enterprise could supportearly and low-resource researchers. Gonzalez and Sandoval will describe a needs analysis for graduate trainingof design-based researchers, examining what features of a cyberinfrastructure project are needed as support thiseffort. Discussant Allan Collins will offer an integrative commentary on these proposals and the enterprise ingeneral.In 2004 Chris Dede challenged the DBR community to engage in an evolutionary dialogue about thepurpose and processes of DBR that would define DBR in a realistic way. He was not optimistic that thecommunity would be able to do so at that time. But perhaps the time is now.ICLS 2014 Proceedings1317© ISLSWorkflow, Information Visualization, and the Potential of Cyberinfrastructure toManage Design Based ResearchAlan J. Hackbarth, Sharon Derry and Sadhana PuntambekarAccording to the Design-Based Research Collective (2003), the critical characteristic of design based research isthat the central goals of designing learning environments and developing theories or “proto-theories” of learningare intertwined. Development and research takes place through continuous cycles of design, enactment, analysis,and redesign, and research must account for how designs function in complex authentic settings – not only interms of success or failure, but also in terms of interactions that refine understanding of the learning issuesinvolved. One needs a well-developed profile of an implementation in order to analyze a design in terms of itskey elements and their interactions, but this is challenging because of the number of variables to account for andthe diverse people involved at different levels of design implementation. Given this complexity, designresearchers usually end up collecting large amounts of data, more data than they have time or resources toanalyze.Addressing these issues, Hackbarth will discuss the NSF-sponsored work our research team isundertaking to co-opt and develop data-analytic tools into a cyberinfrastructure that gives researchers leverageto organize and access layers of complex data in a way that facilitates analysis and deeper understanding oflearning in authentic settings. This work began with development of the Workflow Visualization System (WVS)(http://vmc.wceruw.org/workflow/workflow.html), a prototype that utilizes workflow concepts from businessand science, information design principles (i.e., Tufte), and web interactive tools to document the design andimplementation of units of instruction. The WVS was vetted at a workshop of DBR researchers at CSCL 2013in order to stimulate discussion of the pragmatic challenges of doing DBR and suggestions/specifications for acyberinfrastructure tool that will address the challenges. Issues raised at the workshop included the need for aninterface that provides: access to and makes clear the rationale and goals of a design to all parties (e.g.,researchers and teachers); flexibility to document a wide range of elements and structures of a design and theirinterrelations, and to document design changes that occur during implementation; reduction in time-consumingprocesses of data collection, archiving, and retrieval; and ways of connecting designs and data toquestions/propositions/theories.The emerging tool is a synthesis of course management functionality from Moodle and semi-automatedvisualization functionality from the WVS. A user will have the ability to write a specification of the researchproject, including hypotheses, assumptions, theoretical framework(s), research questions, context, and so forth,and create any number or types of fields to collect data about a designed intervention. Researchers will then beable to use a drag-and-drop tool to create a multi-layered visualization of the intervention and associate datafields to each part of the visualization. (See Figure 1.) They will also have the ability to annotate each part of thevisualization. As a designed intervention takes place, learner-generated data will be added to the visualization.All information and data will be accessible from the visualization by mouse-overs or clicks on icons. Theoverall purpose of the tool is to make visible in one place the interconnections among elements of theintervention and provide access to data about, or generated from, each element of the intervention.This presentation will describe and show examples of our emerging cyberinfrastructure tool formanaging the processes of DBR, and how it may contribute to codifying standards and moving DBR from acollection of methods to a methodology.ICLS 2014 Proceedings1318© ISLSClick on an icon in set-up orimplementation mode to add anannotationDrag icons and connectorsfrom a palette to buildintervention workflows.Drag a resourceto an icon andthe resource isassociated withthe icon.Figure 1. A drag-and-drop tool for create workflow visualizations.Knitting Designs Across Venues and ContextsRichard LehrerDesign-based research is now acknowledged as a useful and powerful framework for generating and testing“modest” theories of how particular innovations function in particular settings. Yet there is a persistent press toelevate designing to the status of method, with attendant clear guidelines and prescriptions for practice. Thispress for method may have the unintended consequence of exchanging the flexibility and responsivenessessential to the conduct of design-based research for the apparent rigors of method. A more productiveelaboration on design-based research would be to develop tools that make the process and products of designmore transparent and that are useful to the process of design. First steps in this direction have been taken, asindicated by the participants in this symposium. For our part, we have established a practice of constructing“design documents” that describe essential elements of a learning ecology and the ways in which these elementsare orchestrated to increase the probability of learning in particular ways for particular purposes. Designelements include: 1. the nature of the tasks/problems that are either posed to participants or are likely to begenerated by them; 2. the representational and related symbolic systems that participants either appropriate orinvent; 3. the material means of production available; 4. the modes and means of argument that are privileged inthe system, and 5. the activity or participant structures that are likely to influence exchange among participants.Interactions among these elements of design constitute prospective mechanisms of learning, and theseinteractions are intentionally supported in classrooms or other deliberately designed ecologies.Design documents also describe the nature of evidence about and for learning. Reflecting on ourpractice, design documents appear adequate for rendering forms and spaces of learning that are characterized bymodest scope—those that can be traversed within days or perhaps a few weeks of instruction. For moreambitious efforts, such as designing learning progressions or other endeavors of more significant scope, designdocuments are difficult to sustain. One problem is that efforts of larger scope—as in longitudinal researchextending over years, or coordinating design of professional development and its temporal patterns with that ofstudents—require tools that allow one to visualize relations between otherwise distinct design efforts. Moreover,designs for learning that are extended in time require ways of tracking prospective trajectories of change atindividual and collective levels. In my talk I will suggest that the next phase of design-based research needs tobe one focused on instrumentation, and that instrumentation needs to lead so that theory development canfollow.ICLS 2014 Proceedings1319© ISLSUsing the Social Infrastructure Framework to Guide Data Collection andTagging Metadata in Building a Cyber-infrastructure for Design ResearchKaterine BielaczycIn carrying out design research involving technology-based tools, it is critical to extend the design processbeyond the tool itself to consider the elements of the broader classroom learning environment. The socialstructures of a classroom play a key role in such a design. The Social Infrastructure Framework (SIF) specifiesa set of critical design variables concerning classroom social structures (Bielaczyc, 2006). The top-levelvariables include:• Cultural Beliefs• Practices• Socio-Techno Spatial Relations• Interaction with the “Outside World”In turn, each top-level category has sub-variables. For example, Practices includes sub-variables such as theassociated participant structures of students and the coordination of on-tool and off-tool activities.In creating a cyber-infrastructure for Design Research, the SIF can be used to guide both (a) the datacollection activities of distributed researchers working toward common design goals, and (b) the tagging ofmetadata in storing the data collected across multiple projects. Looking at a variety of classroomimplementations through the lens of the SIF would make such design decisions explicit and permit designelements across these settings to be classified into a common form, providing a systematic basis for comparisonsand contrasts.Further, because these data archives would provide a rich representation of various classroom socialstructures, researchers not involved in the original research may be able to study questions of their ownchoosing concerning the social structures in a particular setting. That is, through tagging meta-data acrossmultiple design experiments, they may address new questions beyond those posed by the original researchers.What an educator, anthropologist, cognitive psychologist, or media designer will see when they look at the datafrom a particular design experiment or across a variety of design experiments will be very different. Whendifferent eyes look at a design, it provides a kind of triangulation that we do not often find in reports on designexperiments. Instead of analysis using different sources of data, this kind of triangulation would providedifferent theoretical perspectives on the same data.In addition, such an archive would allow researchers, who are just starting their careers or who arelocated in places with few resources, to carry out their own research with a rich data source. This would givedoctoral students the possibility of carrying out design research with a deep disciplinary focus. Their analysiswould be set against the multi-disciplinary analysis of the original researchers, which would promote learning ofthe broader issues, while maintaining an emphasis on the methodological concerns of their own discipline. Theirtraining would embody disciplinary analysis in a multi-disciplinary context, and hence they might get the best ofboth approaches to training.In the presentation, I will discuss various issues that arise in using the SIF to guide both data collectionand the tagging of metadata. I will draw from various cases, including the corpus of classroom data collected byresearchers involved in distributed design research with the Knowledge Building Communities model(Scardamalia, 2002; Scardamalia & Bereiter, 2006).Learning to do Educational Design Research: A Needs AnalysisCarlos Gonzalez and William A. SandovalIn the writing on educational design research that has blossomed over the last decade, there is little articulationabout how to train new researchers to conduct educational design research. We present a needs analysis forgraduate training in educational design research, derived from a synthesis of available writing on designresearch as a method and our own experiences in doctoral training. The analysis identifies areas wherenewcomers to the learning sciences require training to conduct design research that moves the field forward,while leaving open the questions of how to best provide such training. Obviously, learning to do design researchentails overlaps with graduate training in educational research more generally, including selecting good researchquestions, reviewing relevant literatures, and methods and research design. We see design research ascomplicating this research training in two respects. First, there are the particulars of work that change throughphases of educational design research, from problem analysis, design, and enactment, through retrospectiveanalysis. Each phase carries particular training demands. For example, problem analysis typically requires thesynthesis of literatures, often across a range of disparate fields, to understand the background to some learningproblem and previous efforts to solve that problem. It may further require a local needs analysis in the specificintended context of work. Similar demands, unique to design research, arise in each phase.ICLS 2014 Proceedings1320© ISLSA second respect in which design research requires particular training is related to trajectories ofdesign research. The majority of writing about design research assumes a linear trajectory of increasing scale,akin to clinical trials in medical research and familiar to educational intervention research generally. Yet, ananalysis of design research as it has been carried out over the last couple of decades reveals a diverse family ofapproaches to design research, with their own particular trajectories suited to the pursuit of particular sorts ofquestions. Important and notable design research is best characterized in terms of increasing depth rather thanscale, and this difference in focus carries implications for the research methods one uses across design studies.These families of design research may differ in their emphasis on the joint goals of understanding processes oflearning and teaching and understanding innovation, and such emphasis can change across a trajectory of work.These demands require new design researchers to be well trained in a variety of qualitative and quantitativemethods in ways that are linked to phases and trajectories of design research. We will present the details of ourneeds analysis in terms of training requirements for doctoral training. The analysis will suggest features of acyberinfrastructure that could support such training.ReferencesAnderson, T. & Shattuck, J. (2012). Design-based research: A decade of progress in education research?Educational Researcher, 41(16).Barab, S. A., & Squire, K. (2004). Design-based research: Putting a stake in the ground. Journal of the LearningSciences, 13, 1–14.Bielaczyc, K. (2006). Designing social infrastructure: Critical issues in creating learning environments withtechnology. The Journal of the Learning Sciences, 15(3), 301–329.Bielaczyc, K. (2013). Informing design research: Learning from teachers’ designs of social infrastructure. TheJournal of the Learning Sciences, 22(2), 258-311.Brown, A. L. (1992). Design experiments: Theoretical and methodological challenges in creating complexinterventions in classroom settings. The Journal of the Learning Sciences, 2(2), 141–178.Collins, A., Joseph, D., & Bielaczyc, K. (2004). Design research: Theoretical and methodological issues. TheJournal of the Learning Sciences, 13(1), 15–42.Dede, C. (2004). If design-based research is the answer, what is the question? A commentary on Collins,,Joseph, & Bielaczyc; diSessa & Cobb; & Fishman, Marx, Blumenthal, Krajcik, & Soloway. In JLSSpecial issue on design-based research, Journal of the Learning Sciences, 12(1), 105-114.Derry, S. J., Pea, R. D., Engle, R. A., Erickson, F., Goldman, R., Hall, R.,Koselmann, T., Lemke, J., Sherin, M.& Sherin, B. (2010). Conducting video research in the learning sciences: Guidance on selection,analysis, technology, and ethics. The Journal of the Learning Sciences, 19(1), 3–53.Fadel, C. & Lemke, C. (2006). Technology in schools: What the research says. Culver City, CA: Metiri GroupCommissionedbyCiscoSystems.RetrievedJune3,2013,fromwww.cisco.com/web/strategy/docs/education/TechnologyinSchoolsReport.pdfHackbarth, A.J. (2011). Workflow Visualization: Design, Development and Evaluation of a System to SupportDesign-Based Research and Sharing of Learning Interventions (unpublished Doctoral dissertation).Hackbarth, A. J., Derry, S. J., Eagan, B. R., & Gressick, J.. (June-July, 2010) Adapting workflow technology todesign-based research: Development of a method for organizing the “messiness” of research intechnology-rich online learning environments. Paper presented at the 9th International Conference ofthe Learning Sciences, Chicago, IL.McKenney, S. & Reeves, T. C. (2013). Systematic review of design-based research progress: Is a littleknowledge a dangerous thing? Educational Researcher, 42(97), 97-100.Noeth, R. J. & Volkov, B. B. (2004) Evaluating the effectiveness of technology in our schools. Iowa City, IA:ACTPolicyReport.RetrievedJune3,2013,fromhttp://www.act.org/research/policymakers/pdf/school_tech.pdfPenuel, W. R., Fishman, B. J., Cheng, B. H. & Sabelli, N. (2011). Organizing research and development at theintersection of learning, implementation, and design. Educational Researcher, 40(331).Schacter, J. (1999). The impact of education technology on student achievement: What the most currentresearch has to say. Santa Monica, CA: Milken Exchange for Education Technology, Milken FamilyFoundation.RetrievedDecember21,2010,fromwww.eric.ed.gov/ERICWebPortal/recordDetail?accno=ED430537Scardamalia,	  M.	  (2002).	  Collective	  cognitive	  responsibility	  for	  the	  advancement	  of	  knowledge.	  In	  B.	  Smith	  (Ed.)	  Liberal	  education	  in	  the	  knowledge	  society.	  (pp.	  67-­‐98).	  Chicago:	  Open	  Court.	  Scardamalia, M. & Bereiter, C. (2006). Knowledge building: Theory, pedagogy and technology. In R. K.Sawyer (Ed.), Cambridge handbook of the learning sciences (pp. 97-115). Cambridge University Press.Tufte, E. R. (1997). Visual explanations: Images and quantities, evidence and narrative. Cheshire, CT: GraphicsPress.ICLS 2014 Proceedings1321© ISLSThe Design-Based Research Collective. (2003). Design-based research: An emerging paradigm for educationalinquiry. Educational Researcher, 32(1), 5–8.AcknowledgmentsSome of the work reported in this symposium was supported by NSF Grants DRL 0822189 and IIS 2127027.Any opinions, findings, and conclusions or recommendations expressed in this material are those of theauthor(s) and do not necessarily reflect the views of the National Science Foundation.ICLS 2014 Proceedings1322© ISLS